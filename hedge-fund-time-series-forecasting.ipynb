{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":105581,"databundleVersionId":15271735,"sourceType":"competition"}],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Data & Initial Inspection","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 1 — Load Data & Initial Inspection (REVISI FULL v3, RAM-SAFE + ROBUST)\n# - weight wajib di train, opsional di test\n# - FEAT_COLS = intersection(train,test) - {id, weight, target}\n# Output globals:\n#   df_train, df_test, TARGET_COL, ID_COL, WEIGHT_COL, TIME_COL\n#   BASE_CATS, CAT_COLS, SERIES_KEYS\n#   FEAT_COLS, NUM_COLS, BASE_NUM_COLS, DROP_COLS\n# ============================================================\n\nimport gc\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\npd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.width\", 200)\n\nSEED = 42\nnp.random.seed(SEED)\n\nTRAIN_PATH = Path(\"/kaggle/input/ts-forecasting/train.parquet\")\nTEST_PATH  = Path(\"/kaggle/input/ts-forecasting/test.parquet\")\n\nfor p in [TRAIN_PATH, TEST_PATH]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing file: {p}\")\n\nprint(\"Loading parquet...\")\ndf_train = pd.read_parquet(TRAIN_PATH)\ndf_test  = pd.read_parquet(TEST_PATH)\n\nprint(\"\\n==================== BASIC SHAPES ====================\")\nprint(\"train:\", df_train.shape)\nprint(\"test :\", df_test.shape)\n\n# ----------------------------\n# 0) Robust key column detection\n# ----------------------------\ndef pick_first_existing(cols, candidates):\n    for c in candidates:\n        if c in cols:\n            return c\n    return None\n\n# ID\nID_COL = pick_first_existing(df_train.columns, [\"id\", \"ID\"])\nif ID_COL is None:\n    ID_COL = df_train.columns[0]\n    print(f\"[WARN] id column not found by name; fallback to first col: {ID_COL}\")\n\n# TIME\nTIME_COL = pick_first_existing(df_train.columns, [\"ts_index\", \"time_index\", \"t\", \"time\"])\nif TIME_COL is None:\n    # try heuristic: column containing 'ts' and 'index'\n    heur = [c for c in df_train.columns if (\"ts\" in str(c).lower() and \"index\" in str(c).lower())]\n    TIME_COL = heur[0] if heur else None\nif TIME_COL is None:\n    raise RuntimeError(\"Could not detect TIME_COL (ts_index).\")\n\n# WEIGHT (wajib di train, opsional di test)\ndef detect_weight_col(cols):\n    if \"weight\" in cols:\n        return \"weight\"\n    # common alternatives\n    alts = [\"w\", \"sample_weight\", \"weights\", \"wgt\", \"wt\"]\n    for a in alts:\n        if a in cols:\n            return a\n    # heuristic contains 'weight'\n    heur = [c for c in cols if \"weight\" in str(c).lower()]\n    return heur[0] if heur else None\n\nWEIGHT_COL = detect_weight_col(df_train.columns)\nif WEIGHT_COL is None:\n    print(\"[WARN] weight column not found in TRAIN. Will create WEIGHT_COL=1.0 (not ideal).\")\n    WEIGHT_COL = \"__weight__\"\n    df_train[WEIGHT_COL] = 1.0\n\n# Base cats (as described)\nBASE_CATS = [\"code\", \"sub_code\", \"sub_category\", \"horizon\"]\n\nprint(\"\\n==================== KEY COLS ====================\")\nprint(\"ID_COL     :\", ID_COL)\nprint(\"TIME_COL   :\", TIME_COL)\nprint(\"WEIGHT_COL :\", WEIGHT_COL, \"| present in train:\", WEIGHT_COL in df_train.columns, \"| present in test:\", WEIGHT_COL in df_test.columns)\n\n# ----------------------------\n# 1) Detect TARGET_COL (train-only numeric)\n# ----------------------------\ntrain_only = [c for c in df_train.columns if c not in df_test.columns]\n# remove obvious non-targets\ntrain_only = [c for c in train_only if c not in [ID_COL, WEIGHT_COL, TIME_COL] + BASE_CATS]\n\npreferred_names = [\"target\", \"y\", \"label\", \"value\", \"prediction_target\"]\nTARGET_COL = None\nfor nm in preferred_names:\n    if nm in df_train.columns and nm not in df_test.columns:\n        TARGET_COL = nm\n        break\n\nif TARGET_COL is None:\n    # numeric train-only\n    cand = [c for c in train_only if pd.api.types.is_numeric_dtype(df_train[c])]\n    if len(cand) == 1:\n        TARGET_COL = cand[0]\n    elif len(cand) > 1:\n        vars_ = {c: float(np.nanvar(df_train[c].to_numpy(np.float64))) for c in cand}\n        TARGET_COL = sorted(vars_.items(), key=lambda kv: kv[1], reverse=True)[0][0]\n        print(\"\\n[WARN] Multiple numeric train-only cols found; picked by variance:\", TARGET_COL)\n    else:\n        # last resort: single train-only col\n        if len(train_only) == 1:\n            TARGET_COL = train_only[0]\n            print(\"\\n[WARN] Non-numeric train-only target picked:\", TARGET_COL)\n        else:\n            raise RuntimeError(f\"Could not detect target. train_only candidates: {train_only[:20]}\")\n\nprint(\"TARGET_COL :\", TARGET_COL)\n\n# ----------------------------\n# 2) RAM-safe downcast numeric\n# ----------------------------\ndef downcast_numeric(df: pd.DataFrame) -> pd.DataFrame:\n    out = df.copy()\n    for c in out.columns:\n        if c == ID_COL:\n            continue\n        if pd.api.types.is_float_dtype(out[c]):\n            out[c] = pd.to_numeric(out[c], downcast=\"float\")\n        elif pd.api.types.is_integer_dtype(out[c]):\n            out[c] = pd.to_numeric(out[c], downcast=\"integer\")\n    return out\n\ndf_train = downcast_numeric(df_train)\ndf_test  = downcast_numeric(df_test)\n\n# ----------------------------\n# 3) Feature columns = intersection(train,test) excluding id/weight/target\n# ----------------------------\nDROP_COLS = {ID_COL, TARGET_COL, WEIGHT_COL}\n\ncommon_cols = [c for c in df_train.columns if c in df_test.columns]\nFEAT_COLS = [c for c in common_cols if c not in DROP_COLS]\n\n# Categorical columns = base cats (if in FEAT_COLS) + any object/category (also must be in FEAT_COLS)\nCAT_COLS = [c for c in BASE_CATS if c in FEAT_COLS]\nfor c in FEAT_COLS:\n    if c in CAT_COLS:\n        continue\n    if pd.api.types.is_object_dtype(df_train[c]) or str(df_train[c].dtype).startswith(\"category\"):\n        CAT_COLS.append(c)\n\n# unify categories across train+test for categorical cols\nfor c in CAT_COLS:\n    if c in df_train.columns and c in df_test.columns:\n        both = pd.concat([df_train[c], df_test[c]], axis=0, ignore_index=True)\n        both = both.astype(\"category\")\n        df_train[c] = pd.Categorical(df_train[c], categories=both.cat.categories)\n        df_test[c]  = pd.Categorical(df_test[c],  categories=both.cat.categories)\n\n# Numeric feature cols\nNUM_COLS = [c for c in FEAT_COLS if pd.api.types.is_numeric_dtype(df_train[c])]\n\n# Prefer anonymized base numeric features for later FE expansion\nBASE_NUM_COLS = [c for c in NUM_COLS if str(c).startswith(\"feature_\")]\nif len(BASE_NUM_COLS) < 10:\n    BASE_NUM_COLS = [c for c in NUM_COLS if c != TIME_COL]\n\n# series keys for sequential FE\nSERIES_KEYS = [c for c in [\"code\", \"sub_code\", \"sub_category\", \"horizon\"] if c in FEAT_COLS]\n\n# ----------------------------\n# 4) Quick checks\n# ----------------------------\nprint(\"\\n==================== QUICK CHECKS ====================\")\nprint(\"train id unique:\", int(df_train[ID_COL].nunique()), \"/\", len(df_train))\nprint(\"test  id unique:\", int(df_test[ID_COL].nunique()),  \"/\", len(df_test))\n\ntr_min, tr_max = int(df_train[TIME_COL].min()), int(df_train[TIME_COL].max())\nte_min, te_max = int(df_test[TIME_COL].min()),  int(df_test[TIME_COL].max())\nprint(\"train ts range:\", tr_min, \"->\", tr_max)\nprint(\"test  ts range:\", te_min, \"->\", te_max)\nprint(\"test_after_train_max?:\", te_min >= tr_max)\n\n# Missing rates\nprint(\"\\nMissing rate top 10 (train):\")\nprint(df_train[FEAT_COLS].isna().mean().sort_values(ascending=False).head(10))\nprint(\"\\nMissing rate top 10 (test):\")\nprint(df_test[FEAT_COLS].isna().mean().sort_values(ascending=False).head(10))\n\n# target stats\ny = df_train[TARGET_COL].to_numpy(np.float64)\nprint(\"\\n==================== TARGET STATS ====================\")\nprint(\"finite:\", int(np.isfinite(y).sum()), \"/\", len(y))\nprint(\"mean:\", float(np.nanmean(y)), \"std:\", float(np.nanstd(y)))\nprint(\"p1:\", float(np.nanpercentile(y, 1)), \"p50:\", float(np.nanpercentile(y, 50)), \"p99:\", float(np.nanpercentile(y, 99)))\n\n# weight stats\nw = df_train[WEIGHT_COL].to_numpy(np.float64)\nw = np.where(np.isfinite(w), w, 0.0)\nprint(\"\\n==================== WEIGHT STATS (TRAIN ONLY) ====================\")\nprint(\"min:\", float(np.min(w)), \"p50:\", float(np.percentile(w, 50)), \"p99:\", float(np.percentile(w, 99)), \"max:\", float(np.max(w)))\n\nprint(\"\\n==================== FEATURE SET SUMMARY ====================\")\nprint(\"SERIES_KEYS:\", SERIES_KEYS)\nprint(\"CAT_COLS   :\", CAT_COLS)\nprint(\"NUM_COLS   :\", len(NUM_COLS))\nprint(\"BASE_NUM_COLS(for FE):\", len(BASE_NUM_COLS))\nprint(\"FEAT_COLS  :\", len(FEAT_COLS))\n\nprint(\"\\n==================== HEAD (train) ====================\")\ndisplay(df_train.head(3))\nprint(\"\\n==================== HEAD (test) ====================\")\ndisplay(df_test.head(3))\n\nglobals().update({\n    \"df_train\": df_train,\n    \"df_test\": df_test,\n    \"TARGET_COL\": TARGET_COL,\n    \"ID_COL\": ID_COL,\n    \"WEIGHT_COL\": WEIGHT_COL,\n    \"TIME_COL\": TIME_COL,\n    \"BASE_CATS\": BASE_CATS,\n    \"CAT_COLS\": CAT_COLS,\n    \"SERIES_KEYS\": SERIES_KEYS,\n    \"FEAT_COLS\": FEAT_COLS,\n    \"NUM_COLS\": NUM_COLS,\n    \"BASE_NUM_COLS\": BASE_NUM_COLS,\n    \"DROP_COLS\": DROP_COLS,\n    \"SEED\": SEED,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:58:40.303390Z","iopub.execute_input":"2026-01-13T23:58:40.303699Z","iopub.status.idle":"2026-01-13T23:59:08.928906Z","shell.execute_reply.started":"2026-01-13T23:58:40.303673Z","shell.execute_reply":"2026-01-13T23:59:08.928365Z"}},"outputs":[{"name":"stdout","text":"Loading parquet...\n\n==================== BASIC SHAPES ====================\ntrain: (5337414, 94)\ntest : (1447107, 92)\n\n==================== KEY COLS ====================\nID_COL     : id\nTIME_COL   : ts_index\nWEIGHT_COL : weight | present in train: True | present in test: False\nTARGET_COL : y_target\n\n==================== QUICK CHECKS ====================\ntrain id unique: 5337414 / 5337414\ntest  id unique: 1447107 / 1447107\ntrain ts range: 1 -> 3601\ntest  ts range: 3602 -> 4376\ntest_after_train_max?: True\n\nMissing rate top 10 (train):\nfeature_at    0.124719\nfeature_by    0.110192\nfeature_ay    0.085420\nfeature_cd    0.074964\nfeature_ce    0.051678\nfeature_cf    0.044289\nfeature_al    0.042233\nfeature_aw    0.038444\nfeature_bz    0.028426\nfeature_bi    0.027622\ndtype: float64\n\nMissing rate top 10 (test):\nfeature_y     0.385765\nfeature_x     0.385765\nfeature_w     0.385765\nfeature_z     0.385765\nfeature_at    0.092342\nfeature_by    0.092043\nfeature_ay    0.057988\nfeature_cd    0.057969\nfeature_aw    0.038026\nfeature_bz    0.037755\ndtype: float64\n\n==================== TARGET STATS ====================\nfinite: 5337414 / 5337414\nmean: -0.6659048370404826 std: 32.52763850194126\np1: -82.79721488952636 p50: -0.0005774817545898259 p99: 62.923418731689566\n\n==================== WEIGHT STATS (TRAIN ONLY) ====================\nmin: 0.0 p50: 1699.3843705131449 p99: 303840772.74105984 max: 13912217783333.135\n\n==================== FEATURE SET SUMMARY ====================\nSERIES_KEYS: ['code', 'sub_code', 'sub_category', 'horizon']\nCAT_COLS   : ['code', 'sub_code', 'sub_category', 'horizon']\nNUM_COLS   : 87\nBASE_NUM_COLS(for FE): 86\nFEAT_COLS  : 91\n\n==================== HEAD (train) ====================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                     id      code  sub_code sub_category horizon  ts_index  feature_a  feature_b  feature_c  feature_d  feature_e  feature_f  feature_g  feature_h  feature_i  \\\n0  W2MW3G2L__J0G2B0KU__PZ9S1Z4V__25__89  W2MW3G2L  J0G2B0KU     PZ9S1Z4V      25        89         29  16.364094   7.464023   5.966933   1.622184  10.261360   4.914369   0.000467   0.023686   \n1   W2MW3G2L__J0G2B0KU__PZ9S1Z4V__1__89  W2MW3G2L  J0G2B0KU     PZ9S1Z4V       1        89         53   2.858806   5.050617  15.906651  10.879453   3.072151   4.091032   0.000467   0.023686   \n2   W2MW3G2L__J0G2B0KU__PZ9S1Z4V__3__89  W2MW3G2L  J0G2B0KU     PZ9S1Z4V       3        89         51   9.585452   1.076268   9.004147  16.740490  15.166901  11.427982   0.000467   0.023686   \n\n   feature_j  feature_k  feature_l  feature_m  feature_n  feature_o  feature_p  feature_q  feature_r  feature_s  feature_t  feature_u  feature_v  feature_w  feature_x  feature_y   feature_z  \\\n0   0.006409   0.000187   0.744244   2.001013   -0.01687   0.009892   0.013162   0.021502   0.901966   0.402125   0.038566   0.177947   0.091141 -84.968735  -1.765306  10.109641  145.320404   \n1   0.006409   0.000187   0.744244   2.001013   -0.01687   0.009892   0.013162   0.021502   0.901966   0.402125   0.038566   0.177947   0.091141 -84.968735  -1.765306  10.109641  145.320404   \n2   0.006409   0.000187   0.744244   2.001013   -0.01687   0.009892   0.013162   0.021502   0.901966   0.402125   0.038566   0.177947   0.091141 -84.968735  -1.765306  10.109641  145.320404   \n\n   feature_aa  feature_ab  feature_ac  feature_ad  feature_ae  feature_af  feature_ag  feature_ah  feature_ai  feature_aj  feature_ak  feature_al  feature_am  feature_an  feature_ao  feature_ap  \\\n0     0.08958    0.868698    0.080088    0.101631    0.026555    0.092776       0.004    1.298972    7.321646    3.628258    0.453027   -0.080212    0.192181    0.510727   17.136629    0.267856   \n1     0.08958    0.868698    0.080088    0.101631    0.026555    0.092776       0.004    1.298972    7.321646    3.628258    0.453027    0.001480    0.192181    0.510727   17.136629    0.267856   \n2     0.08958    0.868698    0.080088    0.101631    0.026555    0.092776       0.004    1.298972    7.321646    3.628258    0.453027   -0.045494    0.192181    0.510727   17.136629    0.267856   \n\n   feature_aq  feature_ar  feature_as  feature_at  feature_au  feature_av  feature_aw  feature_ax  feature_ay  feature_az  feature_ba  feature_bb  feature_bc    feature_bd    feature_be  feature_bf  \\\n0    7.745722    4.037853    4.856791         NaN    5.188995   79.423474  244.471191   13.848771         NaN     0.01707    0.709292    21.80395    0.120968  26999.430482  34126.269444  791.709562   \n1    7.745722    4.037853    4.856791         NaN    5.188995   79.423474  244.471191   13.848771         NaN     0.01707    0.709292    21.80395    0.120968  26999.430482  34126.269444  791.709562   \n2    7.745722    4.037853    4.856791         NaN    5.188995   79.423474  244.471191   13.848771         NaN     0.01707    0.709292    21.80395    0.120968  26999.430482  34126.269444  791.709562   \n\n   feature_bg   feature_bh  feature_bi  feature_bj  feature_bk  feature_bl  feature_bm  feature_bn  feature_bo  feature_bp  feature_bq  feature_br  feature_bs  feature_bt  feature_bu  feature_bv  \\\n0     0.15467  9499.742248    1.266071  429.318704  2540.88981    0.008927    1.122459   23.815924     0.54985    0.067941    0.076033     0.02759    -0.47269   -0.202944   -3.769914    0.104535   \n1     0.15467  9499.742248    1.266071  429.318704  2540.88981    0.008927    1.122459   23.815924     0.54985    0.067941    0.076033     0.02759    -0.47269   -0.202944   -3.769914    0.104535   \n2     0.15467  9499.742248    1.266071  429.318704  2540.88981    0.008927    1.122459   23.815924     0.54985    0.067941    0.076033     0.02759    -0.47269   -0.202944   -3.769914    0.104535   \n\n   feature_bw  feature_bx  feature_by  feature_bz  feature_ca  feature_cb  feature_cc  feature_cd  feature_ce  feature_cf  feature_cg  feature_ch  y_target      weight  \n0    3.040304    4.499546         NaN   -0.058543   -0.001686   -0.105328   -0.005045         NaN   -0.133697    2.849819    0.112068           1 -0.551324   40.982572  \n1    3.040304    4.499546         NaN   -0.058543   -0.001686   -0.105328   -0.005045         NaN   -0.133697    2.849819    0.112068           1 -0.315583  150.075406  \n2    3.040304    4.499546         NaN   -0.058543   -0.001686   -0.105328   -0.005045         NaN   -0.133697    2.849819    0.112068           1 -0.362894  115.953552  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>code</th>\n      <th>sub_code</th>\n      <th>sub_category</th>\n      <th>horizon</th>\n      <th>ts_index</th>\n      <th>feature_a</th>\n      <th>feature_b</th>\n      <th>feature_c</th>\n      <th>feature_d</th>\n      <th>feature_e</th>\n      <th>feature_f</th>\n      <th>feature_g</th>\n      <th>feature_h</th>\n      <th>feature_i</th>\n      <th>feature_j</th>\n      <th>feature_k</th>\n      <th>feature_l</th>\n      <th>feature_m</th>\n      <th>feature_n</th>\n      <th>feature_o</th>\n      <th>feature_p</th>\n      <th>feature_q</th>\n      <th>feature_r</th>\n      <th>feature_s</th>\n      <th>feature_t</th>\n      <th>feature_u</th>\n      <th>feature_v</th>\n      <th>feature_w</th>\n      <th>feature_x</th>\n      <th>feature_y</th>\n      <th>feature_z</th>\n      <th>feature_aa</th>\n      <th>feature_ab</th>\n      <th>feature_ac</th>\n      <th>feature_ad</th>\n      <th>feature_ae</th>\n      <th>feature_af</th>\n      <th>feature_ag</th>\n      <th>feature_ah</th>\n      <th>feature_ai</th>\n      <th>feature_aj</th>\n      <th>feature_ak</th>\n      <th>feature_al</th>\n      <th>feature_am</th>\n      <th>feature_an</th>\n      <th>feature_ao</th>\n      <th>feature_ap</th>\n      <th>feature_aq</th>\n      <th>feature_ar</th>\n      <th>feature_as</th>\n      <th>feature_at</th>\n      <th>feature_au</th>\n      <th>feature_av</th>\n      <th>feature_aw</th>\n      <th>feature_ax</th>\n      <th>feature_ay</th>\n      <th>feature_az</th>\n      <th>feature_ba</th>\n      <th>feature_bb</th>\n      <th>feature_bc</th>\n      <th>feature_bd</th>\n      <th>feature_be</th>\n      <th>feature_bf</th>\n      <th>feature_bg</th>\n      <th>feature_bh</th>\n      <th>feature_bi</th>\n      <th>feature_bj</th>\n      <th>feature_bk</th>\n      <th>feature_bl</th>\n      <th>feature_bm</th>\n      <th>feature_bn</th>\n      <th>feature_bo</th>\n      <th>feature_bp</th>\n      <th>feature_bq</th>\n      <th>feature_br</th>\n      <th>feature_bs</th>\n      <th>feature_bt</th>\n      <th>feature_bu</th>\n      <th>feature_bv</th>\n      <th>feature_bw</th>\n      <th>feature_bx</th>\n      <th>feature_by</th>\n      <th>feature_bz</th>\n      <th>feature_ca</th>\n      <th>feature_cb</th>\n      <th>feature_cc</th>\n      <th>feature_cd</th>\n      <th>feature_ce</th>\n      <th>feature_cf</th>\n      <th>feature_cg</th>\n      <th>feature_ch</th>\n      <th>y_target</th>\n      <th>weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>W2MW3G2L__J0G2B0KU__PZ9S1Z4V__25__89</td>\n      <td>W2MW3G2L</td>\n      <td>J0G2B0KU</td>\n      <td>PZ9S1Z4V</td>\n      <td>25</td>\n      <td>89</td>\n      <td>29</td>\n      <td>16.364094</td>\n      <td>7.464023</td>\n      <td>5.966933</td>\n      <td>1.622184</td>\n      <td>10.261360</td>\n      <td>4.914369</td>\n      <td>0.000467</td>\n      <td>0.023686</td>\n      <td>0.006409</td>\n      <td>0.000187</td>\n      <td>0.744244</td>\n      <td>2.001013</td>\n      <td>-0.01687</td>\n      <td>0.009892</td>\n      <td>0.013162</td>\n      <td>0.021502</td>\n      <td>0.901966</td>\n      <td>0.402125</td>\n      <td>0.038566</td>\n      <td>0.177947</td>\n      <td>0.091141</td>\n      <td>-84.968735</td>\n      <td>-1.765306</td>\n      <td>10.109641</td>\n      <td>145.320404</td>\n      <td>0.08958</td>\n      <td>0.868698</td>\n      <td>0.080088</td>\n      <td>0.101631</td>\n      <td>0.026555</td>\n      <td>0.092776</td>\n      <td>0.004</td>\n      <td>1.298972</td>\n      <td>7.321646</td>\n      <td>3.628258</td>\n      <td>0.453027</td>\n      <td>-0.080212</td>\n      <td>0.192181</td>\n      <td>0.510727</td>\n      <td>17.136629</td>\n      <td>0.267856</td>\n      <td>7.745722</td>\n      <td>4.037853</td>\n      <td>4.856791</td>\n      <td>NaN</td>\n      <td>5.188995</td>\n      <td>79.423474</td>\n      <td>244.471191</td>\n      <td>13.848771</td>\n      <td>NaN</td>\n      <td>0.01707</td>\n      <td>0.709292</td>\n      <td>21.80395</td>\n      <td>0.120968</td>\n      <td>26999.430482</td>\n      <td>34126.269444</td>\n      <td>791.709562</td>\n      <td>0.15467</td>\n      <td>9499.742248</td>\n      <td>1.266071</td>\n      <td>429.318704</td>\n      <td>2540.88981</td>\n      <td>0.008927</td>\n      <td>1.122459</td>\n      <td>23.815924</td>\n      <td>0.54985</td>\n      <td>0.067941</td>\n      <td>0.076033</td>\n      <td>0.02759</td>\n      <td>-0.47269</td>\n      <td>-0.202944</td>\n      <td>-3.769914</td>\n      <td>0.104535</td>\n      <td>3.040304</td>\n      <td>4.499546</td>\n      <td>NaN</td>\n      <td>-0.058543</td>\n      <td>-0.001686</td>\n      <td>-0.105328</td>\n      <td>-0.005045</td>\n      <td>NaN</td>\n      <td>-0.133697</td>\n      <td>2.849819</td>\n      <td>0.112068</td>\n      <td>1</td>\n      <td>-0.551324</td>\n      <td>40.982572</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>W2MW3G2L__J0G2B0KU__PZ9S1Z4V__1__89</td>\n      <td>W2MW3G2L</td>\n      <td>J0G2B0KU</td>\n      <td>PZ9S1Z4V</td>\n      <td>1</td>\n      <td>89</td>\n      <td>53</td>\n      <td>2.858806</td>\n      <td>5.050617</td>\n      <td>15.906651</td>\n      <td>10.879453</td>\n      <td>3.072151</td>\n      <td>4.091032</td>\n      <td>0.000467</td>\n      <td>0.023686</td>\n      <td>0.006409</td>\n      <td>0.000187</td>\n      <td>0.744244</td>\n      <td>2.001013</td>\n      <td>-0.01687</td>\n      <td>0.009892</td>\n      <td>0.013162</td>\n      <td>0.021502</td>\n      <td>0.901966</td>\n      <td>0.402125</td>\n      <td>0.038566</td>\n      <td>0.177947</td>\n      <td>0.091141</td>\n      <td>-84.968735</td>\n      <td>-1.765306</td>\n      <td>10.109641</td>\n      <td>145.320404</td>\n      <td>0.08958</td>\n      <td>0.868698</td>\n      <td>0.080088</td>\n      <td>0.101631</td>\n      <td>0.026555</td>\n      <td>0.092776</td>\n      <td>0.004</td>\n      <td>1.298972</td>\n      <td>7.321646</td>\n      <td>3.628258</td>\n      <td>0.453027</td>\n      <td>0.001480</td>\n      <td>0.192181</td>\n      <td>0.510727</td>\n      <td>17.136629</td>\n      <td>0.267856</td>\n      <td>7.745722</td>\n      <td>4.037853</td>\n      <td>4.856791</td>\n      <td>NaN</td>\n      <td>5.188995</td>\n      <td>79.423474</td>\n      <td>244.471191</td>\n      <td>13.848771</td>\n      <td>NaN</td>\n      <td>0.01707</td>\n      <td>0.709292</td>\n      <td>21.80395</td>\n      <td>0.120968</td>\n      <td>26999.430482</td>\n      <td>34126.269444</td>\n      <td>791.709562</td>\n      <td>0.15467</td>\n      <td>9499.742248</td>\n      <td>1.266071</td>\n      <td>429.318704</td>\n      <td>2540.88981</td>\n      <td>0.008927</td>\n      <td>1.122459</td>\n      <td>23.815924</td>\n      <td>0.54985</td>\n      <td>0.067941</td>\n      <td>0.076033</td>\n      <td>0.02759</td>\n      <td>-0.47269</td>\n      <td>-0.202944</td>\n      <td>-3.769914</td>\n      <td>0.104535</td>\n      <td>3.040304</td>\n      <td>4.499546</td>\n      <td>NaN</td>\n      <td>-0.058543</td>\n      <td>-0.001686</td>\n      <td>-0.105328</td>\n      <td>-0.005045</td>\n      <td>NaN</td>\n      <td>-0.133697</td>\n      <td>2.849819</td>\n      <td>0.112068</td>\n      <td>1</td>\n      <td>-0.315583</td>\n      <td>150.075406</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>W2MW3G2L__J0G2B0KU__PZ9S1Z4V__3__89</td>\n      <td>W2MW3G2L</td>\n      <td>J0G2B0KU</td>\n      <td>PZ9S1Z4V</td>\n      <td>3</td>\n      <td>89</td>\n      <td>51</td>\n      <td>9.585452</td>\n      <td>1.076268</td>\n      <td>9.004147</td>\n      <td>16.740490</td>\n      <td>15.166901</td>\n      <td>11.427982</td>\n      <td>0.000467</td>\n      <td>0.023686</td>\n      <td>0.006409</td>\n      <td>0.000187</td>\n      <td>0.744244</td>\n      <td>2.001013</td>\n      <td>-0.01687</td>\n      <td>0.009892</td>\n      <td>0.013162</td>\n      <td>0.021502</td>\n      <td>0.901966</td>\n      <td>0.402125</td>\n      <td>0.038566</td>\n      <td>0.177947</td>\n      <td>0.091141</td>\n      <td>-84.968735</td>\n      <td>-1.765306</td>\n      <td>10.109641</td>\n      <td>145.320404</td>\n      <td>0.08958</td>\n      <td>0.868698</td>\n      <td>0.080088</td>\n      <td>0.101631</td>\n      <td>0.026555</td>\n      <td>0.092776</td>\n      <td>0.004</td>\n      <td>1.298972</td>\n      <td>7.321646</td>\n      <td>3.628258</td>\n      <td>0.453027</td>\n      <td>-0.045494</td>\n      <td>0.192181</td>\n      <td>0.510727</td>\n      <td>17.136629</td>\n      <td>0.267856</td>\n      <td>7.745722</td>\n      <td>4.037853</td>\n      <td>4.856791</td>\n      <td>NaN</td>\n      <td>5.188995</td>\n      <td>79.423474</td>\n      <td>244.471191</td>\n      <td>13.848771</td>\n      <td>NaN</td>\n      <td>0.01707</td>\n      <td>0.709292</td>\n      <td>21.80395</td>\n      <td>0.120968</td>\n      <td>26999.430482</td>\n      <td>34126.269444</td>\n      <td>791.709562</td>\n      <td>0.15467</td>\n      <td>9499.742248</td>\n      <td>1.266071</td>\n      <td>429.318704</td>\n      <td>2540.88981</td>\n      <td>0.008927</td>\n      <td>1.122459</td>\n      <td>23.815924</td>\n      <td>0.54985</td>\n      <td>0.067941</td>\n      <td>0.076033</td>\n      <td>0.02759</td>\n      <td>-0.47269</td>\n      <td>-0.202944</td>\n      <td>-3.769914</td>\n      <td>0.104535</td>\n      <td>3.040304</td>\n      <td>4.499546</td>\n      <td>NaN</td>\n      <td>-0.058543</td>\n      <td>-0.001686</td>\n      <td>-0.105328</td>\n      <td>-0.005045</td>\n      <td>NaN</td>\n      <td>-0.133697</td>\n      <td>2.849819</td>\n      <td>0.112068</td>\n      <td>1</td>\n      <td>-0.362894</td>\n      <td>115.953552</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n==================== HEAD (test) ====================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                       id      code  sub_code sub_category horizon  ts_index  feature_a  feature_b  feature_c  feature_d  feature_e  feature_f  feature_g  feature_h  feature_i  \\\n0   W2MW3G2L__495MGHFJ__PZ9S1Z4V__3__3647  W2MW3G2L  495MGHFJ     PZ9S1Z4V       3      3647         95  10.365266   3.209321   8.109340   9.043471  10.123041  15.722121   0.000243   0.021819   \n1  W2MW3G2L__495MGHFJ__PZ9S1Z4V__10__3647  W2MW3G2L  495MGHFJ     PZ9S1Z4V      10      3647         88   2.571476  15.234848  16.505699   0.230426  10.145378  10.159640   0.000243   0.021819   \n2  W2MW3G2L__495MGHFJ__PZ9S1Z4V__25__3647  W2MW3G2L  495MGHFJ     PZ9S1Z4V      25      3647         71   5.524709   6.931663   8.939537   0.668187  16.578701   3.150691   0.000243   0.021819   \n\n   feature_j  feature_k  feature_l  feature_m  feature_n  feature_o  feature_p  feature_q  feature_r  feature_s  feature_t  feature_u  feature_v  feature_w  feature_x  feature_y   feature_z  \\\n0    0.00142   0.000073   0.572125   1.265875   1.341192   0.005564   0.011987   0.035243   0.833918   1.791284   0.020539   0.218876    0.08066 -50.981239  -4.854592  -8.087713  119.237251   \n1    0.00142   0.000073   0.572125   1.265875   1.341192   0.005564   0.011987   0.035243   0.833918   1.791284   0.020539   0.218876    0.08066 -50.981239  -4.854592  -8.087713  119.237251   \n2    0.00142   0.000073   0.572125   1.265875   1.341192   0.005564   0.011987   0.035243   0.833918   1.791284   0.020539   0.218876    0.08066 -50.981239  -4.854592  -8.087713  119.237251   \n\n   feature_aa  feature_ab  feature_ac  feature_ad  feature_ae  feature_af  feature_ag  feature_ah  feature_ai  feature_aj  feature_ak  feature_al  feature_am  feature_an  feature_ao  feature_ap  \\\n0    0.040442    0.635006    0.105355    0.075415     0.03444     0.09455    0.006728    1.986904    4.411098    3.050746    0.484755    0.020247    0.186578    0.528456   15.395411    0.219483   \n1    0.040442    0.635006    0.105355    0.075415     0.03444     0.09455    0.006728    1.986904    4.411098    3.050746    0.484755    0.052623    0.186578    0.528456   15.395411    0.219483   \n2    0.040442    0.635006    0.105355    0.075415     0.03444     0.09455    0.006728    1.986904    4.411098    3.050746    0.484755    0.041667    0.186578    0.528456   15.395411    0.219483   \n\n   feature_aq  feature_ar  feature_as  feature_at  feature_au   feature_av  feature_aw  feature_ax  feature_ay  feature_az  feature_ba  feature_bb  feature_bc    feature_bd    feature_be  \\\n0     4.83955    2.420423    2.652015         0.0    4.151196  1012.649294  425.853042  197.344987  209.253182    0.016366    0.552138  108.859861    2.369993  66589.814887  34282.221003   \n1     4.83955    2.420423    2.652015         0.0    4.151196  1012.649294  425.853042  197.344987  209.253182    0.016366    0.552138  108.859861    2.369993  66589.814887  34282.221003   \n2     4.83955    2.420423    2.652015         0.0    4.151196  1012.649294  425.853042  197.344987  209.253182    0.016366    0.552138  108.859861    2.369993  66589.814887  34282.221003   \n\n    feature_bf  feature_bg    feature_bh  feature_bi  feature_bj  feature_bk  feature_bl  feature_bm  feature_bn  feature_bo  feature_bp  feature_bq  feature_br  feature_bs  feature_bt  feature_bu  \\\n0  1316.738008     0.04801  11660.961097    0.116372   11.122246  716.158132    0.008559    1.772256   38.452076    0.872948     0.06611    0.078856    0.030888   -0.480743   -0.197747   -3.659776   \n1  1316.738008     0.04801  11660.961097    0.116372   11.122246  716.158132    0.008559    1.772256   38.452076    0.872948     0.06611    0.078856    0.030888   -0.480743   -0.197747   -3.659776   \n2  1316.738008     0.04801  11660.961097    0.116372   11.122246  716.158132    0.008559    1.772256   38.452076    0.872948     0.06611    0.078856    0.030888   -0.480743   -0.197747   -3.659776   \n\n   feature_bv  feature_bw  feature_bx  feature_by  feature_bz  feature_ca  feature_cb  feature_cc  feature_cd  feature_ce  feature_cf  feature_cg  feature_ch  \n0    0.100295    3.131395    4.554258   -0.000832   -0.032241    -0.00083   -0.058961   -0.002774    -0.00148    -0.25646    1.665532    0.071324           2  \n1    0.100295    3.131395    4.554258   -0.000832   -0.032241    -0.00083   -0.058961   -0.002774    -0.00148    -0.25646    1.665532    0.071324           2  \n2    0.100295    3.131395    4.554258   -0.000832   -0.032241    -0.00083   -0.058961   -0.002774    -0.00148    -0.25646    1.665532    0.071324           2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>code</th>\n      <th>sub_code</th>\n      <th>sub_category</th>\n      <th>horizon</th>\n      <th>ts_index</th>\n      <th>feature_a</th>\n      <th>feature_b</th>\n      <th>feature_c</th>\n      <th>feature_d</th>\n      <th>feature_e</th>\n      <th>feature_f</th>\n      <th>feature_g</th>\n      <th>feature_h</th>\n      <th>feature_i</th>\n      <th>feature_j</th>\n      <th>feature_k</th>\n      <th>feature_l</th>\n      <th>feature_m</th>\n      <th>feature_n</th>\n      <th>feature_o</th>\n      <th>feature_p</th>\n      <th>feature_q</th>\n      <th>feature_r</th>\n      <th>feature_s</th>\n      <th>feature_t</th>\n      <th>feature_u</th>\n      <th>feature_v</th>\n      <th>feature_w</th>\n      <th>feature_x</th>\n      <th>feature_y</th>\n      <th>feature_z</th>\n      <th>feature_aa</th>\n      <th>feature_ab</th>\n      <th>feature_ac</th>\n      <th>feature_ad</th>\n      <th>feature_ae</th>\n      <th>feature_af</th>\n      <th>feature_ag</th>\n      <th>feature_ah</th>\n      <th>feature_ai</th>\n      <th>feature_aj</th>\n      <th>feature_ak</th>\n      <th>feature_al</th>\n      <th>feature_am</th>\n      <th>feature_an</th>\n      <th>feature_ao</th>\n      <th>feature_ap</th>\n      <th>feature_aq</th>\n      <th>feature_ar</th>\n      <th>feature_as</th>\n      <th>feature_at</th>\n      <th>feature_au</th>\n      <th>feature_av</th>\n      <th>feature_aw</th>\n      <th>feature_ax</th>\n      <th>feature_ay</th>\n      <th>feature_az</th>\n      <th>feature_ba</th>\n      <th>feature_bb</th>\n      <th>feature_bc</th>\n      <th>feature_bd</th>\n      <th>feature_be</th>\n      <th>feature_bf</th>\n      <th>feature_bg</th>\n      <th>feature_bh</th>\n      <th>feature_bi</th>\n      <th>feature_bj</th>\n      <th>feature_bk</th>\n      <th>feature_bl</th>\n      <th>feature_bm</th>\n      <th>feature_bn</th>\n      <th>feature_bo</th>\n      <th>feature_bp</th>\n      <th>feature_bq</th>\n      <th>feature_br</th>\n      <th>feature_bs</th>\n      <th>feature_bt</th>\n      <th>feature_bu</th>\n      <th>feature_bv</th>\n      <th>feature_bw</th>\n      <th>feature_bx</th>\n      <th>feature_by</th>\n      <th>feature_bz</th>\n      <th>feature_ca</th>\n      <th>feature_cb</th>\n      <th>feature_cc</th>\n      <th>feature_cd</th>\n      <th>feature_ce</th>\n      <th>feature_cf</th>\n      <th>feature_cg</th>\n      <th>feature_ch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>W2MW3G2L__495MGHFJ__PZ9S1Z4V__3__3647</td>\n      <td>W2MW3G2L</td>\n      <td>495MGHFJ</td>\n      <td>PZ9S1Z4V</td>\n      <td>3</td>\n      <td>3647</td>\n      <td>95</td>\n      <td>10.365266</td>\n      <td>3.209321</td>\n      <td>8.109340</td>\n      <td>9.043471</td>\n      <td>10.123041</td>\n      <td>15.722121</td>\n      <td>0.000243</td>\n      <td>0.021819</td>\n      <td>0.00142</td>\n      <td>0.000073</td>\n      <td>0.572125</td>\n      <td>1.265875</td>\n      <td>1.341192</td>\n      <td>0.005564</td>\n      <td>0.011987</td>\n      <td>0.035243</td>\n      <td>0.833918</td>\n      <td>1.791284</td>\n      <td>0.020539</td>\n      <td>0.218876</td>\n      <td>0.08066</td>\n      <td>-50.981239</td>\n      <td>-4.854592</td>\n      <td>-8.087713</td>\n      <td>119.237251</td>\n      <td>0.040442</td>\n      <td>0.635006</td>\n      <td>0.105355</td>\n      <td>0.075415</td>\n      <td>0.03444</td>\n      <td>0.09455</td>\n      <td>0.006728</td>\n      <td>1.986904</td>\n      <td>4.411098</td>\n      <td>3.050746</td>\n      <td>0.484755</td>\n      <td>0.020247</td>\n      <td>0.186578</td>\n      <td>0.528456</td>\n      <td>15.395411</td>\n      <td>0.219483</td>\n      <td>4.83955</td>\n      <td>2.420423</td>\n      <td>2.652015</td>\n      <td>0.0</td>\n      <td>4.151196</td>\n      <td>1012.649294</td>\n      <td>425.853042</td>\n      <td>197.344987</td>\n      <td>209.253182</td>\n      <td>0.016366</td>\n      <td>0.552138</td>\n      <td>108.859861</td>\n      <td>2.369993</td>\n      <td>66589.814887</td>\n      <td>34282.221003</td>\n      <td>1316.738008</td>\n      <td>0.04801</td>\n      <td>11660.961097</td>\n      <td>0.116372</td>\n      <td>11.122246</td>\n      <td>716.158132</td>\n      <td>0.008559</td>\n      <td>1.772256</td>\n      <td>38.452076</td>\n      <td>0.872948</td>\n      <td>0.06611</td>\n      <td>0.078856</td>\n      <td>0.030888</td>\n      <td>-0.480743</td>\n      <td>-0.197747</td>\n      <td>-3.659776</td>\n      <td>0.100295</td>\n      <td>3.131395</td>\n      <td>4.554258</td>\n      <td>-0.000832</td>\n      <td>-0.032241</td>\n      <td>-0.00083</td>\n      <td>-0.058961</td>\n      <td>-0.002774</td>\n      <td>-0.00148</td>\n      <td>-0.25646</td>\n      <td>1.665532</td>\n      <td>0.071324</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>W2MW3G2L__495MGHFJ__PZ9S1Z4V__10__3647</td>\n      <td>W2MW3G2L</td>\n      <td>495MGHFJ</td>\n      <td>PZ9S1Z4V</td>\n      <td>10</td>\n      <td>3647</td>\n      <td>88</td>\n      <td>2.571476</td>\n      <td>15.234848</td>\n      <td>16.505699</td>\n      <td>0.230426</td>\n      <td>10.145378</td>\n      <td>10.159640</td>\n      <td>0.000243</td>\n      <td>0.021819</td>\n      <td>0.00142</td>\n      <td>0.000073</td>\n      <td>0.572125</td>\n      <td>1.265875</td>\n      <td>1.341192</td>\n      <td>0.005564</td>\n      <td>0.011987</td>\n      <td>0.035243</td>\n      <td>0.833918</td>\n      <td>1.791284</td>\n      <td>0.020539</td>\n      <td>0.218876</td>\n      <td>0.08066</td>\n      <td>-50.981239</td>\n      <td>-4.854592</td>\n      <td>-8.087713</td>\n      <td>119.237251</td>\n      <td>0.040442</td>\n      <td>0.635006</td>\n      <td>0.105355</td>\n      <td>0.075415</td>\n      <td>0.03444</td>\n      <td>0.09455</td>\n      <td>0.006728</td>\n      <td>1.986904</td>\n      <td>4.411098</td>\n      <td>3.050746</td>\n      <td>0.484755</td>\n      <td>0.052623</td>\n      <td>0.186578</td>\n      <td>0.528456</td>\n      <td>15.395411</td>\n      <td>0.219483</td>\n      <td>4.83955</td>\n      <td>2.420423</td>\n      <td>2.652015</td>\n      <td>0.0</td>\n      <td>4.151196</td>\n      <td>1012.649294</td>\n      <td>425.853042</td>\n      <td>197.344987</td>\n      <td>209.253182</td>\n      <td>0.016366</td>\n      <td>0.552138</td>\n      <td>108.859861</td>\n      <td>2.369993</td>\n      <td>66589.814887</td>\n      <td>34282.221003</td>\n      <td>1316.738008</td>\n      <td>0.04801</td>\n      <td>11660.961097</td>\n      <td>0.116372</td>\n      <td>11.122246</td>\n      <td>716.158132</td>\n      <td>0.008559</td>\n      <td>1.772256</td>\n      <td>38.452076</td>\n      <td>0.872948</td>\n      <td>0.06611</td>\n      <td>0.078856</td>\n      <td>0.030888</td>\n      <td>-0.480743</td>\n      <td>-0.197747</td>\n      <td>-3.659776</td>\n      <td>0.100295</td>\n      <td>3.131395</td>\n      <td>4.554258</td>\n      <td>-0.000832</td>\n      <td>-0.032241</td>\n      <td>-0.00083</td>\n      <td>-0.058961</td>\n      <td>-0.002774</td>\n      <td>-0.00148</td>\n      <td>-0.25646</td>\n      <td>1.665532</td>\n      <td>0.071324</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>W2MW3G2L__495MGHFJ__PZ9S1Z4V__25__3647</td>\n      <td>W2MW3G2L</td>\n      <td>495MGHFJ</td>\n      <td>PZ9S1Z4V</td>\n      <td>25</td>\n      <td>3647</td>\n      <td>71</td>\n      <td>5.524709</td>\n      <td>6.931663</td>\n      <td>8.939537</td>\n      <td>0.668187</td>\n      <td>16.578701</td>\n      <td>3.150691</td>\n      <td>0.000243</td>\n      <td>0.021819</td>\n      <td>0.00142</td>\n      <td>0.000073</td>\n      <td>0.572125</td>\n      <td>1.265875</td>\n      <td>1.341192</td>\n      <td>0.005564</td>\n      <td>0.011987</td>\n      <td>0.035243</td>\n      <td>0.833918</td>\n      <td>1.791284</td>\n      <td>0.020539</td>\n      <td>0.218876</td>\n      <td>0.08066</td>\n      <td>-50.981239</td>\n      <td>-4.854592</td>\n      <td>-8.087713</td>\n      <td>119.237251</td>\n      <td>0.040442</td>\n      <td>0.635006</td>\n      <td>0.105355</td>\n      <td>0.075415</td>\n      <td>0.03444</td>\n      <td>0.09455</td>\n      <td>0.006728</td>\n      <td>1.986904</td>\n      <td>4.411098</td>\n      <td>3.050746</td>\n      <td>0.484755</td>\n      <td>0.041667</td>\n      <td>0.186578</td>\n      <td>0.528456</td>\n      <td>15.395411</td>\n      <td>0.219483</td>\n      <td>4.83955</td>\n      <td>2.420423</td>\n      <td>2.652015</td>\n      <td>0.0</td>\n      <td>4.151196</td>\n      <td>1012.649294</td>\n      <td>425.853042</td>\n      <td>197.344987</td>\n      <td>209.253182</td>\n      <td>0.016366</td>\n      <td>0.552138</td>\n      <td>108.859861</td>\n      <td>2.369993</td>\n      <td>66589.814887</td>\n      <td>34282.221003</td>\n      <td>1316.738008</td>\n      <td>0.04801</td>\n      <td>11660.961097</td>\n      <td>0.116372</td>\n      <td>11.122246</td>\n      <td>716.158132</td>\n      <td>0.008559</td>\n      <td>1.772256</td>\n      <td>38.452076</td>\n      <td>0.872948</td>\n      <td>0.06611</td>\n      <td>0.078856</td>\n      <td>0.030888</td>\n      <td>-0.480743</td>\n      <td>-0.197747</td>\n      <td>-3.659776</td>\n      <td>0.100295</td>\n      <td>3.131395</td>\n      <td>4.554258</td>\n      <td>-0.000832</td>\n      <td>-0.032241</td>\n      <td>-0.00083</td>\n      <td>-0.058961</td>\n      <td>-0.002774</td>\n      <td>-0.00148</td>\n      <td>-0.25646</td>\n      <td>1.665532</td>\n      <td>0.071324</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Sanity Checks & Leakage Rules Setup","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 2 — Sanity Checks & Leakage Rules Setup (REVISI FULL v2, RANK1-READY)\n# Assumes STAGE 1 already ran and created:\n#   df_train, df_test, TARGET_COL, ID_COL, TIME_COL, WEIGHT_COL, CAT_COLS, FEAT_COLS, SERIES_KEYS\n#\n# Outputs/Globals:\n#   DO_NOT_USE_COLS\n#   FEATURE_COLS_CAT, FEATURE_COLS_NUM, FEATURE_COLS_ALL\n#   TRAIN_MIN_TS, TRAIN_MAX_TS, TEST_MIN_TS, TEST_MAX_TS\n#   HAS_WEIGHT_TRAIN, HAS_WEIGHT_TEST\n#   DROP_TARGET_NAN (bool), N_TARGET_NAN\n#   ALLOW_TIME_FEATURES (bool)\n# ============================================================\n\nimport gc\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require STAGE 1 globals\n# ----------------------------\nneed = [\"df_train\",\"df_test\",\"TARGET_COL\",\"ID_COL\",\"TIME_COL\",\"CAT_COLS\",\"FEAT_COLS\",\"WEIGHT_COL\",\"SERIES_KEYS\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1 dulu.\")\n\nassert isinstance(df_train, pd.DataFrame) and isinstance(df_test, pd.DataFrame)\n\nprint(\"==================== STAGE 2: SANITY (v2) ====================\")\nprint(\"ID_COL:\", ID_COL, \"| TIME_COL:\", TIME_COL, \"| WEIGHT_COL:\", WEIGHT_COL, \"| TARGET_COL:\", TARGET_COL)\n\n# ----------------------------\n# 1) Core column existence\n# ----------------------------\nmust_in_train = [ID_COL, TIME_COL, TARGET_COL]\nmust_in_test  = [ID_COL, TIME_COL]\nfor c in must_in_train:\n    if c not in df_train.columns:\n        raise RuntimeError(f\"Train missing required column: {c}\")\nfor c in must_in_test:\n    if c not in df_test.columns:\n        raise RuntimeError(f\"Test missing required column: {c}\")\n\nHAS_WEIGHT_TRAIN = WEIGHT_COL in df_train.columns\nHAS_WEIGHT_TEST  = WEIGHT_COL in df_test.columns\n\nprint(\"Has weight in train:\", HAS_WEIGHT_TRAIN, \"| in test:\", HAS_WEIGHT_TEST)\nif HAS_WEIGHT_TEST:\n    print(\"[WARN] weight exists in test too; tetap dilarang sebagai fitur.\")\n\n# ----------------------------\n# 2) ID uniqueness checks\n# ----------------------------\nntr = len(df_train)\nnts = len(df_test)\n\nnuniq_tr = df_train[ID_COL].nunique(dropna=False)\nnuniq_ts = df_test[ID_COL].nunique(dropna=False)\n\nif nuniq_tr != ntr:\n    dup = df_train.loc[df_train[ID_COL].duplicated(keep=False), ID_COL].head(10).tolist()\n    raise RuntimeError(f\"Train id not unique: {nuniq_tr}/{ntr}. Example dups: {dup}\")\nif nuniq_ts != nts:\n    dup = df_test.loc[df_test[ID_COL].duplicated(keep=False), ID_COL].head(10).tolist()\n    raise RuntimeError(f\"Test id not unique: {nuniq_ts}/{nts}. Example dups: {dup}\")\n\n# It's fine if ids don't overlap; warn if overlap\nintersect = np.intersect1d(df_train[ID_COL].astype(str).values, df_test[ID_COL].astype(str).values)\nif len(intersect) > 0:\n    print(f\"[WARN] Train/Test share {len(intersect)} ids. Example:\", intersect[:5])\n\nprint(\"ID uniqueness: OK\")\n\n# ----------------------------\n# 3) Time ordering checks\n# ----------------------------\nif not pd.api.types.is_numeric_dtype(df_train[TIME_COL]):\n    raise RuntimeError(f\"{TIME_COL} in train is not numeric.\")\nif not pd.api.types.is_numeric_dtype(df_test[TIME_COL]):\n    raise RuntimeError(f\"{TIME_COL} in test is not numeric.\")\n\nTRAIN_MIN_TS = int(np.nanmin(df_train[TIME_COL].values))\nTRAIN_MAX_TS = int(np.nanmax(df_train[TIME_COL].values))\nTEST_MIN_TS  = int(np.nanmin(df_test[TIME_COL].values))\nTEST_MAX_TS  = int(np.nanmax(df_test[TIME_COL].values))\n\nprint(\"Train ts range:\", TRAIN_MIN_TS, \"->\", TRAIN_MAX_TS)\nprint(\"Test  ts range:\", TEST_MIN_TS,  \"->\", TEST_MAX_TS)\n\nif TEST_MIN_TS <= TRAIN_MAX_TS:\n    print(\"[WARN] Test min ts_index <= Train max ts_index. Bisa overlap. Pastikan fitur time-series di-shift agar anti look-forward.\")\nelse:\n    print(\"Time ordering: OK (test after train).\")\n\n# time feature policy: we won't use raw ts_index as feature by default, but we WILL engineer age/bin in Stage 5\nALLOW_TIME_FEATURES = True\n\n# ----------------------------\n# 4) Leakage rules + feature lists (SAFE for inference)\n# ----------------------------\n# IMPORTANT: FEAT_COLS from Stage 1 sudah intersection(train,test) minus id/target/weight\n# Jadi gunakan FEAT_COLS sebagai universe fitur untuk avoid \"missing in test\"\nDO_NOT_USE_COLS = set([ID_COL, TARGET_COL, WEIGHT_COL, \"fold\"])\n\n# Categoricals: only those that are in FEAT_COLS\nFEATURE_COLS_CAT = [c for c in CAT_COLS if (c in FEAT_COLS) and (c not in DO_NOT_USE_COLS)]\n\n# Numerics: only numeric columns that are in FEAT_COLS\nFEATURE_COLS_NUM = [c for c in FEAT_COLS\n                    if (c not in DO_NOT_USE_COLS)\n                    and pd.api.types.is_numeric_dtype(df_train[c])]\n\n# Exclude raw ts_index from numeric features (we'll engineer time features later)\nFEATURE_COLS_NUM = [c for c in FEATURE_COLS_NUM if c != TIME_COL]\n\nFEATURE_COLS_ALL = FEATURE_COLS_CAT + FEATURE_COLS_NUM\n\nprint(\"\\n==================== FEATURE LISTS ====================\")\nprint(\"SERIES_KEYS:\", SERIES_KEYS)\nprint(\"Categorical features:\", FEATURE_COLS_CAT)\nprint(\"Numeric features (excluding ts_index):\", len(FEATURE_COLS_NUM))\nprint(\"Total features:\", len(FEATURE_COLS_ALL))\n\n# ----------------------------\n# 5) Integrity checks\n# ----------------------------\n# Ensure all features exist in test\nmissing_in_test = [c for c in FEATURE_COLS_ALL if c not in df_test.columns]\nif missing_in_test:\n    raise RuntimeError(f\"Some feature columns missing in test (shouldn't happen if FEAT_COLS is intersection): {missing_in_test[:10]}\")\n\n# Target NaNs\nN_TARGET_NAN = int(df_train[TARGET_COL].isna().sum())\nDROP_TARGET_NAN = (N_TARGET_NAN > 0)\nprint(\"\\nTarget NaN count:\", N_TARGET_NAN, \"| drop later:\", DROP_TARGET_NAN)\n\n# Weight sanity (train)\nif HAS_WEIGHT_TRAIN:\n    w = df_train[WEIGHT_COL].to_numpy(np.float64)\n    w = np.where(np.isfinite(w), w, 0.0)\n    w_neg = float(np.mean(w < 0))\n    w_zero = float(np.mean(w == 0))\n    print(\"\\n==================== WEIGHT SANITY (TRAIN) ====================\")\n    print(\"min:\", float(np.min(w)), \"p50:\", float(np.percentile(w, 50)), \"p99:\", float(np.percentile(w, 99)), \"max:\", float(np.max(w)))\n    print(\"negative rate:\", w_neg, \"| zero rate:\", w_zero)\n    if w_neg > 0:\n        print(\"[WARN] Negative weights exist. We'll clip to >=0 later.\")\n\n# ----------------------------\n# 6) Extra diagnostics (helps rank-1 tuning)\n# ----------------------------\n# Cardinality of key categorical cols (drift hint)\nprint(\"\\n==================== CATEGORY CARDINALITY ====================\")\nfor c in FEATURE_COLS_CAT:\n    tr_u = int(df_train[c].nunique(dropna=False))\n    te_u = int(df_test[c].nunique(dropna=False))\n    print(f\"{c:15s} | train uniq={tr_u:6d} | test uniq={te_u:6d}\")\n\n# quick ts density check\nprint(\"\\n==================== TS DENSITY (unique ts_index) ====================\")\nprint(\"train unique ts:\", int(df_train[TIME_COL].nunique()), \"| test unique ts:\", int(df_test[TIME_COL].nunique()))\n\nprint(\"\\n==================== LEAKAGE RULES (REMINDER) ====================\")\nprint(\"- Jangan gunakan WEIGHT sebagai fitur (hanya sample_weight).\")\nprint(\"- FE time-series wajib computed per group dan SHIFT(1) (no future).\")\nprint(\"- Encoder/imputer/scaler fit hanya di TRAIN-fold, apply ke valid/test.\")\nprint(\"- Hindari compute statistik menggunakan data ts_index > t untuk prediksi t.\")\n\nglobals().update({\n    \"DO_NOT_USE_COLS\": DO_NOT_USE_COLS,\n    \"FEATURE_COLS_CAT\": FEATURE_COLS_CAT,\n    \"FEATURE_COLS_NUM\": FEATURE_COLS_NUM,\n    \"FEATURE_COLS_ALL\": FEATURE_COLS_ALL,\n    \"TRAIN_MIN_TS\": TRAIN_MIN_TS,\n    \"TRAIN_MAX_TS\": TRAIN_MAX_TS,\n    \"TEST_MIN_TS\": TEST_MIN_TS,\n    \"TEST_MAX_TS\": TEST_MAX_TS,\n    \"HAS_WEIGHT_TRAIN\": HAS_WEIGHT_TRAIN,\n    \"HAS_WEIGHT_TEST\": HAS_WEIGHT_TEST,\n    \"DROP_TARGET_NAN\": DROP_TARGET_NAN,\n    \"N_TARGET_NAN\": N_TARGET_NAN,\n    \"ALLOW_TIME_FEATURES\": ALLOW_TIME_FEATURES,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T00:02:24.440599Z","iopub.execute_input":"2026-01-14T00:02:24.440948Z","iopub.status.idle":"2026-01-14T00:02:38.722883Z","shell.execute_reply.started":"2026-01-14T00:02:24.440921Z","shell.execute_reply":"2026-01-14T00:02:38.722290Z"}},"outputs":[{"name":"stdout","text":"==================== STAGE 2: SANITY (v2) ====================\nID_COL: id | TIME_COL: ts_index | WEIGHT_COL: weight | TARGET_COL: y_target\nHas weight in train: True | in test: False\nID uniqueness: OK\nTrain ts range: 1 -> 3601\nTest  ts range: 3602 -> 4376\nTime ordering: OK (test after train).\n\n==================== FEATURE LISTS ====================\nSERIES_KEYS: ['code', 'sub_code', 'sub_category', 'horizon']\nCategorical features: ['code', 'sub_code', 'sub_category', 'horizon']\nNumeric features (excluding ts_index): 86\nTotal features: 90\n\nTarget NaN count: 0 | drop later: False\n\n==================== WEIGHT SANITY (TRAIN) ====================\nmin: 0.0 p50: 1699.3843705131449 p99: 303840772.74105984 max: 13912217783333.135\nnegative rate: 0.0 | zero rate: 0.0009332234673945098\n\n==================== CATEGORY CARDINALITY ====================\ncode            | train uniq=    23 | test uniq=    23\nsub_code        | train uniq=   180 | test uniq=    47\nsub_category    | train uniq=     5 | test uniq=     5\nhorizon         | train uniq=     4 | test uniq=     4\n\n==================== TS DENSITY (unique ts_index) ====================\ntrain unique ts: 3601 | test unique ts: 775\n\n==================== LEAKAGE RULES (REMINDER) ====================\n- Jangan gunakan WEIGHT sebagai fitur (hanya sample_weight).\n- FE time-series wajib computed per group dan SHIFT(1) (no future).\n- Encoder/imputer/scaler fit hanya di TRAIN-fold, apply ke valid/test.\n- Hindari compute statistik menggunakan data ts_index > t untuk prediksi t.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Implement Official Metric","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 3 — Implement Official Metric (REVISI FULL v2, ROBUST)\n# Assumes STAGE 1–2 already ran and created:\n#   df_train, TARGET_COL, ID_COL, TIME_COL, WEIGHT_COL\n# Outputs/Globals:\n#   weighted_rmse_score, score_df, score_arrays\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require minimal globals\n# ----------------------------\nneed = [\"df_train\", \"TARGET_COL\", \"ID_COL\", \"TIME_COL\", \"WEIGHT_COL\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1–2 dulu.\")\n\nHAS_W = WEIGHT_COL in df_train.columns\n\n# ----------------------------\n# 1) Official metric (host)\n# ----------------------------\ndef _clip01(x: float) -> float:\n    return float(np.minimum(np.maximum(x, 0.0), 1.0))\n\ndef weighted_rmse_score(y_target, y_pred, w) -> float:\n    \"\"\"\n    Competition metric:\n      denom = sum(w * y^2)\n      ratio = sum(w * (y - yhat)^2) / denom\n      score = sqrt( 1 - clip01(ratio) )\n    \"\"\"\n    y_target = np.asarray(y_target, dtype=np.float64)\n    y_pred   = np.asarray(y_pred, dtype=np.float64)\n    w        = np.asarray(w,        dtype=np.float64)\n\n    if y_target.shape != y_pred.shape or y_target.shape != w.shape:\n        raise ValueError(f\"Shape mismatch: y={y_target.shape}, yhat={y_pred.shape}, w={w.shape}\")\n\n    # robust weights: finite & non-negative\n    w = np.where(np.isfinite(w), w, 0.0)\n    w = np.maximum(w, 0.0)\n\n    # robust targets/preds: finite only (mask non-finite)\n    m = np.isfinite(y_target) & np.isfinite(y_pred)\n    if not np.all(m):\n        y_target = y_target[m]\n        y_pred   = y_pred[m]\n        w        = w[m]\n\n    denom = np.sum(w * (y_target ** 2))\n    if not np.isfinite(denom) or denom <= 0:\n        return 0.0\n\n    ratio = np.sum(w * ((y_target - y_pred) ** 2)) / denom\n    clipped = _clip01(ratio)\n    val = 1.0 - clipped\n    return float(np.sqrt(max(val, 0.0)))\n\n# ----------------------------\n# 2) Convenience wrappers\n# ----------------------------\ndef score_arrays(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray | None = None) -> float:\n    y_true = np.asarray(y_true, dtype=np.float64)\n    y_pred = np.asarray(y_pred, dtype=np.float64)\n    if w is None:\n        w = np.ones_like(y_true, dtype=np.float64)\n    else:\n        w = np.asarray(w, dtype=np.float64)\n    return weighted_rmse_score(y_true, y_pred, w)\n\ndef score_df(df: pd.DataFrame, y_col: str, pred_col: str, w_col: str | None = None) -> float:\n    y = df[y_col].to_numpy(np.float64)\n    p = df[pred_col].to_numpy(np.float64)\n    if (w_col is None) or (w_col not in df.columns):\n        w = np.ones(len(df), dtype=np.float64)\n    else:\n        w = df[w_col].to_numpy(np.float64)\n    return weighted_rmse_score(y, p, w)\n\n# ----------------------------\n# 3) Sanity checks / baselines\n# ----------------------------\nprint(\"==================== STAGE 3: OFFICIAL METRIC (v2) ====================\")\n\ny = df_train[TARGET_COL].to_numpy(np.float64)\n\nif HAS_W:\n    w = df_train[WEIGHT_COL].to_numpy(np.float64)\nelse:\n    w = np.ones_like(y, dtype=np.float64)\n\n# clean w\nw = np.where(np.isfinite(w), w, 0.0)\nw = np.maximum(w, 0.0)\n\n# For baselines, mask finite y\nm = np.isfinite(y)\ny_m = y[m]\nw_m = w[m]\nif len(y_m) == 0:\n    raise RuntimeError(\"All targets are NaN/Inf after masking.\")\n\n# Baseline A: predict 0\npred0 = np.zeros_like(y_m, dtype=np.float64)\ns0 = weighted_rmse_score(y_m, pred0, w_m)\n\n# Baseline B: predict weighted mean (best constant for weighted MSE)\nw_sum = float(np.sum(w_m))\nc = float(np.sum(w_m * y_m) / (w_sum + 1e-18))\npredc = np.full_like(y_m, c, dtype=np.float64)\nsc = weighted_rmse_score(y_m, predc, w_m)\n\n# Baseline C: predict weighted median (more robust than mean)\n# compute weighted median (simple sort)\norder = np.argsort(y_m)\nys = y_m[order]\nws = w_m[order]\ncum = np.cumsum(ws)\nwm = ys[np.searchsorted(cum, 0.5 * cum[-1])] if cum[-1] > 0 else float(np.median(y_m))\npredwm = np.full_like(y_m, float(wm), dtype=np.float64)\nswm = weighted_rmse_score(y_m, predwm, w_m)\n\nprint(f\"Using WEIGHT_COL: {WEIGHT_COL} | present: {HAS_W}\")\nprint(f\"Baseline (predict 0)                score = {s0:.6f}\")\nprint(f\"Baseline (predict weighted-mean {c:.6f}) score = {sc:.6f}\")\nprint(f\"Baseline (predict weighted-med  {float(wm):.6f}) score = {swm:.6f}\")\n\ndenom = float(np.sum(w_m * (y_m ** 2)))\nsse0  = float(np.sum(w_m * ((y_m - 0.0) ** 2)))\nratio0 = sse0 / denom if denom > 0 else np.nan\nprint(\"\\nDiagnostics:\")\nprint(f\"denom sum(w*y^2) = {denom:.6e}\")\nprint(f\"ratio(predict0)  = {ratio0:.6f} (score -> ~0 if ratio~1)\")\n\nprint(\"\\nGlobals exported: weighted_rmse_score, score_arrays, score_df\")\n\nglobals().update({\n    \"weighted_rmse_score\": weighted_rmse_score,\n    \"score_arrays\": score_arrays,\n    \"score_df\": score_df,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T00:05:24.430672Z","iopub.execute_input":"2026-01-14T00:05:24.431493Z","iopub.status.idle":"2026-01-14T00:05:25.293854Z","shell.execute_reply.started":"2026-01-14T00:05:24.431461Z","shell.execute_reply":"2026-01-14T00:05:25.293165Z"}},"outputs":[{"name":"stdout","text":"==================== STAGE 3: OFFICIAL METRIC (v2) ====================\nUsing WEIGHT_COL: weight | present: True\nBaseline (predict 0)                score = 0.000000\nBaseline (predict weighted-mean -0.000024) score = 0.011117\nBaseline (predict weighted-med  0.000009) score = 0.000000\n\nDiagnostics:\ndenom sum(w*y^2) = 4.082630e+08\nratio(predict0)  = 1.000000 (score -> ~0 if ratio~1)\n\nGlobals exported: weighted_rmse_score, score_arrays, score_df\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Time-based Validation Split","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 4 — Time-based Validation Split (REVISI FULL v2, PURGED + TAIL)\n# Assumes STAGE 1–3 already ran and created:\n#   df_train, ID_COL, TIME_COL\n#\n# This stage:\n# - Builds purged time CV on unique ts_index (tail-focused)\n# - Adds df_train[\"fold\"] where fold>=0 means validation membership, -1 means never validated\n#\n# Outputs/Globals:\n#   df_train (with 'fold'), df_folds\n#   fold_boundaries, fold_info, FOLD_CFG\n#   CUT_TS, TAIL_TS\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport gc\n\n# ----------------------------\n# 0) Require\n# ----------------------------\nneed = [\"df_train\", \"ID_COL\", \"TIME_COL\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1–3 dulu.\")\n\n# ----------------------------\n# 1) Config (rank1-oriented defaults)\n# ----------------------------\nN_FOLDS = 5\n\n# Use tail portion for validation placement (mimic test regime)\nCV_START_FRAC = 0.70   # validate on last 30% ts points (lebih informatif dari 25%)\n\n# Purge gap to avoid subtle leakage through smoothing/rolling\nGAP = 2                # 1-2 biasanya bagus; 0 boleh, tapi lebih risk\n\n# Minimal validation rows per fold (adaptive, warn-only)\nMIN_VALID_ROWS = 50_000\n\n# ----------------------------\n# 2) Prepare unique ts timeline (robust if ts_index has gaps)\n# ----------------------------\nts_unique = np.sort(df_train[TIME_COL].dropna().unique().astype(np.int64))\nif len(ts_unique) < (N_FOLDS + 5):\n    raise RuntimeError(f\"Unique ts_index terlalu sedikit ({len(ts_unique)}) untuk N_FOLDS={N_FOLDS}.\")\n\ncut_idx = int(len(ts_unique) * CV_START_FRAC)\ncut_idx = min(max(cut_idx, 0), len(ts_unique) - 1)\nCUT_TS = int(ts_unique[cut_idx])\n\nTAIL_TS = ts_unique[ts_unique >= CUT_TS]\nif len(TAIL_TS) < N_FOLDS:\n    raise RuntimeError(f\"TAIL_TS terlalu kecil ({len(TAIL_TS)}) untuk N_FOLDS={N_FOLDS}. Turunkan N_FOLDS atau CV_START_FRAC.\")\n\n# split tail ts into contiguous segments (by order, not by value)\nsegments = np.array_split(TAIL_TS, N_FOLDS)\n\nfold_boundaries = []\nfold_info = {}\nfor k, seg in enumerate(segments):\n    seg = np.array(seg, dtype=np.int64)\n    if len(seg) == 0:\n        continue\n    vmin = int(seg.min())\n    vmax = int(seg.max())\n    train_max = int(vmin - GAP)  # purged\n\n    fold_boundaries.append({\n        \"fold\": int(k),\n        \"val_min_ts\": vmin,\n        \"val_max_ts\": vmax,\n        \"train_max_ts\": train_max,\n        \"gap\": int(GAP),\n        \"n_val_ts\": int(len(seg)),\n    })\n    fold_info[int(k)] = (vmin, vmax, train_max)\n\nfold_ids = sorted(fold_info.keys())\n\nFOLD_CFG = dict(\n    N_FOLDS=int(N_FOLDS),\n    CV_START_FRAC=float(CV_START_FRAC),\n    GAP=int(GAP),\n    MIN_VALID_ROWS=int(MIN_VALID_ROWS),\n    CUT_TS=int(CUT_TS),\n)\n\nprint(\"==================== STAGE 4: PURGED TIME CV ====================\")\nprint(\"Unique ts:\", len(ts_unique), \"| ts_min:\", int(ts_unique.min()), \"| ts_max:\", int(ts_unique.max()))\nprint(\"CV_START_FRAC:\", CV_START_FRAC, \"| CUT_TS:\", CUT_TS, \"| tail unique ts:\", len(TAIL_TS))\nprint(\"N_FOLDS:\", N_FOLDS, \"| GAP:\", GAP)\nprint(\"\\nFold boundaries:\")\nfor b in fold_boundaries:\n    print(f\"  fold {b['fold']}: train <= {b['train_max_ts']} | val [{b['val_min_ts']}, {b['val_max_ts']}] | n_val_ts={b['n_val_ts']}\")\n\n# ----------------------------\n# 3) Assign folds by ts_index membership in validation segments\n# ----------------------------\nts_arr = df_train[TIME_COL].to_numpy(np.int64)\nfold_arr = np.full(len(df_train), -1, dtype=np.int16)\n\n# mapping ts -> fold (fast)\nts_to_fold = {}\nfor b in fold_boundaries:\n    k = int(b[\"fold\"])\n    vmin, vmax = int(b[\"val_min_ts\"]), int(b[\"val_max_ts\"])\n    # map all ts in range that actually exist in tail segments\n    # safest: explicit mapping from segment list\n    seg = segments[k]  # segment order matches fold\n    for t in seg:\n        ts_to_fold[int(t)] = k\n\n# assign\nfor i, t in enumerate(ts_arr):\n    fold_arr[i] = ts_to_fold.get(int(t), -1)\n\ndf_train[\"fold\"] = fold_arr\ndf_folds = df_train[[ID_COL, \"fold\"]].copy()\n\n# ----------------------------\n# 4) Diagnostics\n# ----------------------------\nvc = df_train[\"fold\"].value_counts(dropna=False).sort_index()\nprint(\"\\nFold row counts (fold=-1 means never validated):\")\nprint(vc.to_dict())\n\nfor b in fold_boundaries:\n    k = int(b[\"fold\"])\n    n_valid = int((df_train[\"fold\"] == k).sum())\n    if n_valid < MIN_VALID_ROWS:\n        print(f\"[WARN] fold {k} valid rows kecil: {n_valid} < {MIN_VALID_ROWS} (tetap lanjut)\")\n\n# strict validity check\nviol = []\nfor b in fold_boundaries:\n    if not (int(b[\"train_max_ts\"]) < int(b[\"val_min_ts\"])):\n        viol.append(int(b[\"fold\"]))\nif viol:\n    raise RuntimeError(f\"Invalid split: folds where train_max_ts >= val_min_ts: {viol}\")\n\nprint(\"\\nGlobals exported: df_train['fold'], df_folds, fold_boundaries, fold_info, FOLD_CFG, CUT_TS, TAIL_TS\")\n\nglobals().update({\n    \"df_train\": df_train,\n    \"df_folds\": df_folds,\n    \"fold_boundaries\": fold_boundaries,\n    \"fold_info\": fold_info,\n    \"FOLD_CFG\": FOLD_CFG,\n    \"CUT_TS\": CUT_TS,\n    \"TAIL_TS\": TAIL_TS,\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T00:06:57.706461Z","iopub.execute_input":"2026-01-14T00:06:57.706786Z","iopub.status.idle":"2026-01-14T00:07:00.140095Z","shell.execute_reply.started":"2026-01-14T00:06:57.706745Z","shell.execute_reply":"2026-01-14T00:07:00.139359Z"}},"outputs":[{"name":"stdout","text":"==================== STAGE 4: PURGED TIME CV ====================\nUnique ts: 3601 | ts_min: 1 | ts_max: 3601\nCV_START_FRAC: 0.7 | CUT_TS: 2521 | tail unique ts: 1081\nN_FOLDS: 5 | GAP: 2\n\nFold boundaries:\n  fold 0: train <= 2519 | val [2521, 2737] | n_val_ts=217\n  fold 1: train <= 2736 | val [2738, 2953] | n_val_ts=216\n  fold 2: train <= 2952 | val [2954, 3169] | n_val_ts=216\n  fold 3: train <= 3168 | val [3170, 3385] | n_val_ts=216\n  fold 4: train <= 3384 | val [3386, 3601] | n_val_ts=216\n\nFold row counts (fold=-1 means never validated):\n{-1: 3493999, 0: 379658, 1: 374318, 2: 361889, 3: 361991, 4: 365559}\n\nGlobals exported: df_train['fold'], df_folds, fold_boundaries, fold_info, FOLD_CFG, CUT_TS, TAIL_TS\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Feature Preparation & Weighting Strategy","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 5 — Feature Engineering (REVISI FULL v5, RAM-SAFE + TS/LAG + CS subset)\n# REQUIRE (from STAGE 1–4):\n#   df_train, df_test, TARGET_COL, ID_COL, TIME_COL, WEIGHT_COL, SERIES_KEYS\n#   df_train[\"fold\"], fold_boundaries (optional, for tail focus)\n#\n# OUTPUT globals:\n#   df_train, df_test (added engineered features)\n#   TOPK, CS_FEATS\n#   FEATURE_COLS_CAT_ALL, FEATURE_COLS_NUM_ALL, FEATURE_COLS_ALL\n#   CAT_FEATURE_IDXS_ALL\n#   make_sample_weight(), fit_median_imputer(), apply_median_imputer()\n# ============================================================\n\nimport gc\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require\n# ----------------------------\nneed = [\"df_train\",\"df_test\",\"TARGET_COL\",\"ID_COL\",\"TIME_COL\",\"WEIGHT_COL\",\"SERIES_KEYS\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1–4 dulu.\")\nif \"fold\" not in df_train.columns:\n    raise RuntimeError(\"Missing df_train['fold']. Jalankan STAGE 4 dulu.\")\n\n# ----------------------------\n# 1) Config (rank1-oriented tapi RAM-safe)\n# ----------------------------\nTOPK_NUM_FOR_FE   = 20          # naikkan ke 30 kalau RAM kuat\nCS_TOPK           = 8           # hanya subset untuk CS (biar aman)\nLAGS              = (1, 2, 3)    # kalau berat: (1,2)\nADD_DIFF1         = True\nADD_TIME_GAP      = True\nADD_AGE           = True\nADD_TS_BIN        = True\nTS_BIN_SIZE       = 20\n\n# Cross-sectional within SAME ts (no future): zscore + rank pct\nADD_CS_Z          = True\nADD_CS_RANK       = True        # kalau lambat: False\nCS_BUCKET_CAND    = [TIME_COL, \"horizon\"]\n\n# TOPK selection sampling\nMAX_ROWS_FOR_CORR = 180_000\nTAU_SELECT        = 450.0       # recency emphasis for selecting TOPK\nRANDOM_STATE      = 42\n\n# Weighting config (used later in make_sample_weight)\nRECENCY_TAU_DEFAULT = 450.0\nCLIP_W_Q_DEFAULT    = 0.9995\n\nTRAIN_MAX_TS = int(df_train[TIME_COL].max())\n\n# ----------------------------\n# 2) Pick numeric features to expand (TOPK) using weighted abs-corr on tail-ish data\n# ----------------------------\ndef _weighted_abs_corr(x: np.ndarray, y: np.ndarray, w: np.ndarray, eps: float = 1e-12) -> float:\n    m_w = np.sum(w) + eps\n    wx = np.sum(w * x) / m_w\n    wy = np.sum(w * y) / m_w\n    xc = x - wx\n    yc = y - wy\n    cov = np.sum(w * xc * yc) / m_w\n    vx = np.sum(w * xc * xc) / m_w\n    vy = np.sum(w * yc * yc) / m_w\n    return float(abs(cov / (np.sqrt(vx * vy) + eps)))\n\nDROP_ALWAYS = {ID_COL, TARGET_COL, WEIGHT_COL, \"fold\"}\n\n# numeric candidates restricted to common numeric features (prefer feature_*)\nnum_candidates = [c for c in df_train.columns\n                  if (c not in DROP_ALWAYS)\n                  and pd.api.types.is_numeric_dtype(df_train[c])]\n\nfeat_candidates = [c for c in num_candidates if str(c).startswith(\"feature_\")]\nif len(feat_candidates) >= 10:\n    num_candidates = feat_candidates\n\n# focus correlation selection on validation-tail (fold>=0) if available\ndf_sel = df_train[df_train[\"fold\"] >= 0].copy()\nif len(df_sel) < 50_000:\n    df_sel = df_train.copy()\n\nuse_cols = list(dict.fromkeys([*num_candidates, TARGET_COL, WEIGHT_COL, TIME_COL]))\ndf_sel = df_sel[use_cols]\n\n# sample with recency\nif len(df_sel) > MAX_ROWS_FOR_CORR:\n    t = df_sel[TIME_COL].to_numpy(np.float64)\n    p = np.exp(-(TRAIN_MAX_TS - t) / float(TAU_SELECT))\n    p = p / (p.sum() + 1e-12)\n    take = np.random.RandomState(RANDOM_STATE).choice(len(df_sel), size=MAX_ROWS_FOR_CORR, replace=False, p=p)\n    df_sel = df_sel.iloc[take].copy()\n\ny = df_sel[TARGET_COL].to_numpy(np.float64)\nw = df_sel[WEIGHT_COL].to_numpy(np.float64)\nw = np.where(np.isfinite(w), w, 0.0)\nw = np.maximum(w, 0.0)\n# recency multiply for TOPK selection\nt = df_sel[TIME_COL].to_numpy(np.float64)\nw = w * np.exp(-(TRAIN_MAX_TS - t) / float(TAU_SELECT))\nif w.sum() <= 0:\n    w = np.ones_like(y, dtype=np.float64)\n\ncorr_scores = []\nfor c in num_candidates:\n    x = df_sel[c].to_numpy(np.float64)\n    med = np.nanmedian(x)\n    x = np.where(np.isfinite(x), x, med)\n    corr_scores.append((_weighted_abs_corr(x, y, w), c))\n\ncorr_scores.sort(reverse=True, key=lambda kv: kv[0])\nTOPK = [c for _, c in corr_scores[:min(TOPK_NUM_FOR_FE, len(corr_scores))]]\nCS_FEATS = TOPK[:min(CS_TOPK, len(TOPK))]\n\nprint(\"TOPK selected:\", TOPK[:12], \"...\" if len(TOPK) > 12 else \"\")\nprint(\"CS_FEATS subset:\", CS_FEATS)\n\ndel df_sel, y, w, t\ngc.collect()\n\n# ----------------------------\n# 3) Build engineered features on MINIMAL concatenation (RAM-safe)\n# ----------------------------\ntr_n = len(df_train)\nte_n = len(df_test)\n\nbucket = [c for c in CS_BUCKET_CAND if c in df_train.columns and c in df_test.columns]\n# columns needed for FE\nmin_cols = list(dict.fromkeys([TIME_COL, *SERIES_KEYS, *bucket, *TOPK]))\nmin_cols = [c for c in min_cols if c in df_train.columns and c in df_test.columns]\n\nmin_train = df_train[min_cols].copy()\nmin_test  = df_test[min_cols].copy()\n\nmin_train[\"__is_train\"] = 1\nmin_test[\"__is_train\"]  = 0\nmin_train[\"__rid\"] = np.arange(tr_n, dtype=np.int64)\nmin_test[\"__rid\"]  = np.arange(te_n, dtype=np.int64)\n\nmin_all = pd.concat([min_train, min_test], axis=0, ignore_index=True)\ndel min_train, min_test\ngc.collect()\n\n# Optional time bin\nif ADD_TS_BIN:\n    min_all[\"fe_ts_bin\"] = (min_all[TIME_COL].astype(np.int32) // int(TS_BIN_SIZE)).astype(np.int32)\n\n# Sort for sequential FE (stable)\nsort_cols = [c for c in SERIES_KEYS if c in min_all.columns] + [TIME_COL, \"__is_train\", \"__rid\"]\nmin_all = min_all.sort_values(sort_cols, kind=\"mergesort\").reset_index(drop=True)\n\ng = min_all.groupby([c for c in SERIES_KEYS if c in min_all.columns], sort=False)\n\n# Time gap (dt)\nif ADD_TIME_GAP:\n    dt = g[TIME_COL].diff()\n    min_all[\"fe_dt\"] = dt.fillna(0).astype(np.float32)\n    min_all[\"fe_dt_clip\"] = np.clip(min_all[\"fe_dt\"].to_numpy(np.float32), 0.0, 50.0).astype(np.float32)\n\n# Age\nif ADD_AGE:\n    min_all[\"fe_age\"] = (TRAIN_MAX_TS - min_all[TIME_COL].to_numpy(np.int32)).astype(np.int32)\n\n# Lags + diff1 with safety: only valid if current_ts > ts_shift\ncur_ts = min_all[TIME_COL].to_numpy(np.int64)\n\nfor f in TOPK:\n    for L in LAGS:\n        ts_shift = g[TIME_COL].shift(L).to_numpy(np.float64)\n        val_shift = g[f].shift(L).astype(np.float32).to_numpy()\n        ok = (cur_ts.astype(np.float64) > ts_shift)  # strict earlier time\n        out = np.where(ok, val_shift, np.nan).astype(np.float32)\n        min_all[f\"fe_{f}_lag{L}\"] = out\n    if ADD_DIFF1:\n        cur = min_all[f].astype(np.float32).to_numpy()\n        lag1 = min_all[f\"fe_{f}_lag1\"].to_numpy(np.float32)\n        min_all[f\"fe_{f}_diff1\"] = (cur - lag1).astype(np.float32)\n\n# small missing-count feature (cheap & often helps)\nmin_all[\"fe_nan_cnt_topk\"] = np.zeros(len(min_all), dtype=np.float32)\nif len(TOPK) > 0:\n    arr = min_all[TOPK].to_numpy()\n    min_all[\"fe_nan_cnt_topk\"] = np.isnan(arr).sum(axis=1).astype(np.float32)\n    del arr\n    gc.collect()\n\n# Cross-sectional stats within same ts bucket (safe)\nif ADD_CS_Z and len(bucket) > 0 and len(CS_FEATS) > 0:\n    gb = min_all.groupby(bucket, sort=False)\n    for f in CS_FEATS:\n        mu = gb[f].transform(\"mean\").astype(np.float32)\n        sd = gb[f].transform(\"std\").astype(np.float32)\n        min_all[f\"fe_{f}_z_{'_'.join(bucket)}\"] = ((min_all[f].astype(np.float32) - mu) / (sd + 1e-6)).astype(np.float32)\n\nif ADD_CS_RANK and len(bucket) > 0 and len(CS_FEATS) > 0:\n    gb = min_all.groupby(bucket, sort=False)\n    for f in CS_FEATS:\n        min_all[f\"fe_{f}_r_{'_'.join(bucket)}\"] = gb[f].rank(pct=True).astype(np.float32)\n\n# Split back by __rid WITHOUT reordering original df_train/df_test\neng_cols = [c for c in min_all.columns if c.startswith(\"fe_\")]\ntr_eng = min_all[min_all[\"__is_train\"] == 1][[\"__rid\"] + eng_cols].sort_values(\"__rid\")\nte_eng = min_all[min_all[\"__is_train\"] == 0][[\"__rid\"] + eng_cols].sort_values(\"__rid\")\n\nfor c in eng_cols:\n    df_train[c] = tr_eng[c].to_numpy()\n    df_test[c]  = te_eng[c].to_numpy()\n\ndel min_all, tr_eng, te_eng\ngc.collect()\n\n# ----------------------------\n# 4) Finalize feature lists for Stage 6/7\n# ----------------------------\n# categorical (base cats) + fe_ts_bin as categorical if enabled\nFEATURE_COLS_CAT_ALL = [c for c in [\"code\",\"sub_code\",\"sub_category\",\"horizon\"] if c in df_train.columns]\nif ADD_TS_BIN and \"fe_ts_bin\" in df_train.columns:\n    FEATURE_COLS_CAT_ALL = FEATURE_COLS_CAT_ALL + [\"fe_ts_bin\"]\n\n# do-not-use\nDO_NOT_USE = {ID_COL, TARGET_COL, WEIGHT_COL, \"fold\"}\n\n# numeric = all numeric excluding do-not-use & excluding categorical\nFEATURE_COLS_NUM_ALL = [c for c in df_train.columns\n                        if (c not in DO_NOT_USE)\n                        and (c not in FEATURE_COLS_CAT_ALL)\n                        and pd.api.types.is_numeric_dtype(df_train[c])]\n\n# (optional) do NOT include raw ts_index; we use engineered time instead\nFEATURE_COLS_NUM_ALL = [c for c in FEATURE_COLS_NUM_ALL if c != TIME_COL]\n\nFEATURE_COLS_ALL = FEATURE_COLS_CAT_ALL + FEATURE_COLS_NUM_ALL\nCAT_FEATURE_IDXS_ALL = list(range(len(FEATURE_COLS_CAT_ALL)))\n\n# ----------------------------\n# 5) Weight strategy\n# ----------------------------\ndef make_sample_weight(df: pd.DataFrame,\n                       use_recency: bool = True,\n                       tau: float = RECENCY_TAU_DEFAULT,\n                       clip_w_quantile: float | None = CLIP_W_Q_DEFAULT,\n                       eps: float = 1e-12) -> np.ndarray:\n    w = df[WEIGHT_COL].to_numpy(np.float64)\n    w = np.where(np.isfinite(w), w, 0.0)\n    w = np.maximum(w, 0.0)\n\n    if clip_w_quantile is not None:\n        q = float(np.nanquantile(w, float(clip_w_quantile)))\n        if np.isfinite(q) and q > 0:\n            w = np.minimum(w, q)\n\n    if use_recency:\n        t = df[TIME_COL].to_numpy(np.float64)\n        rec = np.exp(-(TRAIN_MAX_TS - t) / float(tau))\n        w = w * rec\n\n    if float(w.sum()) <= eps:\n        w = np.ones(len(df), dtype=np.float64)\n    return w\n\n# ----------------------------\n# 6) Median imputer (optional)\n# ----------------------------\ndef fit_median_imputer(df_fit: pd.DataFrame, num_cols: list[str]) -> dict:\n    med = df_fit[num_cols].median(numeric_only=True)\n    return {c: float(med[c]) if c in med.index and np.isfinite(med[c]) else 0.0 for c in num_cols}\n\ndef apply_median_imputer(df_apply: pd.DataFrame, medians: dict, num_cols: list[str]) -> pd.DataFrame:\n    out = df_apply.copy()\n    for c in num_cols:\n        if c in out.columns:\n            out[c] = out[c].fillna(medians.get(c, 0.0))\n    return out\n\nprint(\"\\n==================== STAGE 5 SUMMARY (v5) ====================\")\nprint(\"Train/Test shapes:\", df_train.shape, df_test.shape)\nprint(\"SERIES_KEYS:\", SERIES_KEYS)\nprint(\"TOPK:\", len(TOPK), \"| CS_FEATS:\", len(CS_FEATS), \"| bucket:\", bucket)\nprint(\"CAT:\", FEATURE_COLS_CAT_ALL)\nprint(\"NUM features:\", len(FEATURE_COLS_NUM_ALL), \"| TOTAL:\", len(FEATURE_COLS_ALL))\nprint(\"Example engineered:\", [c for c in df_train.columns if c.startswith(\"fe_\")][:25])\n\nglobals().update({\n    \"df_train\": df_train,\n    \"df_test\": df_test,\n    \"TOPK\": TOPK,\n    \"CS_FEATS\": CS_FEATS,\n    \"FEATURE_COLS_CAT_ALL\": FEATURE_COLS_CAT_ALL,\n    \"FEATURE_COLS_NUM_ALL\": FEATURE_COLS_NUM_ALL,\n    \"FEATURE_COLS_ALL\": FEATURE_COLS_ALL,\n    \"CAT_FEATURE_IDXS_ALL\": CAT_FEATURE_IDXS_ALL,\n    \"make_sample_weight\": make_sample_weight,\n    \"fit_median_imputer\": fit_median_imputer,\n    \"apply_median_imputer\": apply_median_imputer,\n    \"TRAIN_MAX_TS\": TRAIN_MAX_TS,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T00:11:18.001292Z","iopub.execute_input":"2026-01-14T00:11:18.001926Z","iopub.status.idle":"2026-01-14T00:12:06.192859Z","shell.execute_reply.started":"2026-01-14T00:11:18.001895Z","shell.execute_reply":"2026-01-14T00:12:06.192127Z"}},"outputs":[{"name":"stdout","text":"TOPK selected: ['feature_u', 'feature_bn', 'feature_ca', 'feature_cd', 'feature_v', 'feature_am', 'feature_cb', 'feature_ap', 'feature_bo', 'feature_an', 'feature_bm', 'feature_cc'] ...\nCS_FEATS subset: ['feature_u', 'feature_bn', 'feature_ca', 'feature_cd', 'feature_v', 'feature_am', 'feature_cb', 'feature_ap']\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/479892695.py:156: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  g = min_all.groupby([c for c in SERIES_KEYS if c in min_all.columns], sort=False)\n/tmp/ipykernel_55/479892695.py:193: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  gb = min_all.groupby(bucket, sort=False)\n/tmp/ipykernel_55/479892695.py:200: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  gb = min_all.groupby(bucket, sort=False)\n/tmp/ipykernel_55/479892695.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  min_all[f\"fe_{f}_r_{'_'.join(bucket)}\"] = gb[f].rank(pct=True).astype(np.float32)\n/tmp/ipykernel_55/479892695.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  min_all[f\"fe_{f}_r_{'_'.join(bucket)}\"] = gb[f].rank(pct=True).astype(np.float32)\n/tmp/ipykernel_55/479892695.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  min_all[f\"fe_{f}_r_{'_'.join(bucket)}\"] = gb[f].rank(pct=True).astype(np.float32)\n/tmp/ipykernel_55/479892695.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  min_all[f\"fe_{f}_r_{'_'.join(bucket)}\"] = gb[f].rank(pct=True).astype(np.float32)\n/tmp/ipykernel_55/479892695.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  min_all[f\"fe_{f}_r_{'_'.join(bucket)}\"] = gb[f].rank(pct=True).astype(np.float32)\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_train[c] = tr_eng[c].to_numpy()\n/tmp/ipykernel_55/479892695.py:211: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_test[c]  = te_eng[c].to_numpy()\n","output_type":"stream"},{"name":"stdout","text":"\n==================== STAGE 5 SUMMARY (v5) ====================\nTrain/Test shapes: (5337414, 196) (1447107, 193)\nSERIES_KEYS: ['code', 'sub_code', 'sub_category', 'horizon']\nTOPK: 20 | CS_FEATS: 8 | bucket: ['ts_index', 'horizon']\nCAT: ['code', 'sub_code', 'sub_category', 'horizon', 'fe_ts_bin']\nNUM features: 186 | TOTAL: 191\nExample engineered: ['fe_ts_bin', 'fe_dt', 'fe_dt_clip', 'fe_age', 'fe_feature_u_lag1', 'fe_feature_u_lag2', 'fe_feature_u_lag3', 'fe_feature_u_diff1', 'fe_feature_bn_lag1', 'fe_feature_bn_lag2', 'fe_feature_bn_lag3', 'fe_feature_bn_diff1', 'fe_feature_ca_lag1', 'fe_feature_ca_lag2', 'fe_feature_ca_lag3', 'fe_feature_ca_diff1', 'fe_feature_cd_lag1', 'fe_feature_cd_lag2', 'fe_feature_cd_lag3', 'fe_feature_cd_diff1', 'fe_feature_v_lag1', 'fe_feature_v_lag2', 'fe_feature_v_lag3', 'fe_feature_v_diff1', 'fe_feature_am_lag1']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Model Training, OOF Evaluation, and Model Selection","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 6 — Model Training, OOF Evaluation, and Model Selection (REVISI FULL v5, PURGED + RANK1-READY + RAM-SAFE)\n# - FIX leakage: training fold-k hanya pakai ts_index <= train_max_ts(fold-k) (purged) + horizon mask\n# - OOF predictions built per row (per horizon if MODE=\"per_horizon\")\n# - Select top models by OOF, then per-horizon nonneg blend + alpha calibration\n# - Saves cfg + registry (no heavy model saving; Stage 7 akan final-fit)\n#\n# REQUIRE:\n#   df_train, TARGET_COL, TIME_COL, ID_COL, WEIGHT_COL\n#   FEATURE_COLS_ALL, FEATURE_COLS_CAT_ALL\n#   fold_boundaries, weighted_rmse_score, make_sample_weight\n#   df_train[\"fold\"]\n#\n# OUTPUT globals:\n#   oof_pred_blend, blend_weights_by_h, alpha_by_h, model_cfg_used\n#   top_model_names, oof_store\n# ============================================================\n\nimport gc, json, time, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nneed = [\"df_train\",\"TARGET_COL\",\"TIME_COL\",\"ID_COL\",\"WEIGHT_COL\",\"FEATURE_COLS_ALL\",\"FEATURE_COLS_CAT_ALL\",\n        \"fold_boundaries\",\"weighted_rmse_score\",\"make_sample_weight\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing global: {k}. Jalankan stage sebelumnya.\")\nif \"fold\" not in df_train.columns:\n    raise RuntimeError(\"Missing df_train['fold']. Jalankan STAGE 4 dulu.\")\n\n# ----------------------------\n# 0) Config (moderate -> aman RAM; bisa dinaikkan setelah stabil)\n# ----------------------------\nMODE = \"per_horizon\"  # \"per_horizon\" recommended\n\n# sampling\nTRAIN_SAMPLE_CAP = 450_000\nSAMPLE_WEIGHTED  = True\n\n# sample weights (official * recency); untuk fold-k recency anchor di train_max_ts fold-k\nUSE_RECENCY = True\nTAU = 450.0\nCLIP_W_Q = 0.9995\n\n# model families (mulai tidak terlalu banyak)\nUSE_CATBOOST = True\nUSE_LGBM = True\n\n# CatBoost (2 configs x 2 seeds) -> 4 model per fold/horizon\nCB_SEEDS = [42, 62]\nCB_PARAM_LIST = [\n    dict(iterations=5000, learning_rate=0.03, depth=10, l2_leaf_reg=8.0,  random_strength=1.1, rsm=0.9,\n         min_data_in_leaf=80, bootstrap_type=\"Bernoulli\", subsample=0.8),\n    dict(iterations=3500, learning_rate=0.04, depth=8,  l2_leaf_reg=6.0,  random_strength=1.0, rsm=0.9,\n         min_data_in_leaf=120, bootstrap_type=\"Bernoulli\", subsample=0.85),\n]\nEARLY_STOPPING_ROUNDS = 250\n\n# LightGBM (2 seeds) -> 2 model per fold/horizon\nLGB_SEEDS = [41, 51]\nLGB_PARAMS = dict(\n    objective=\"regression\",\n    metric=\"rmse\",\n    learning_rate=0.03,\n    num_leaves=192,\n    min_data_in_leaf=180,\n    feature_fraction=0.8,\n    bagging_fraction=0.8,\n    bagging_freq=1,\n    lambda_l2=8.0,\n    max_bin=255,\n    verbose=-1,\n)\n\n# target transform (asinh stabil untuk heavy-tail & bisa negatif)\nUSE_TARGET_ASINH = True\nASINH_CLIP_PRED = 8.0  # clamp pred in transformed space (stability)\n\n# Blend selection\nTOP_MODELS_KEEP = 6   # pilih top-N model (across all names) untuk blending\nNONNEG_BLEND = True\n\nOUT_DIR = Path(\"/kaggle/working/tsf_stage6_models_v5\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# 1) Feature columns for model (per_horizon exclude horizon from inputs)\n# ----------------------------\nif MODE == \"per_horizon\" and \"horizon\" in df_train.columns and (\"horizon\" in FEATURE_COLS_CAT_ALL):\n    CAT_COLS_MODEL = [c for c in FEATURE_COLS_CAT_ALL if c != \"horizon\"]\nelse:\n    CAT_COLS_MODEL = list(FEATURE_COLS_CAT_ALL)\n\n# ensure categorical dtypes for cat cols (once, in-place)\nfor c in CAT_COLS_MODEL:\n    if c in df_train.columns:\n        if not (pd.api.types.is_categorical_dtype(df_train[c]) or pd.api.types.is_object_dtype(df_train[c])):\n            df_train[c] = df_train[c].astype(\"category\")\n\nNUM_COLS_MODEL = [c for c in FEATURE_COLS_ALL if c not in FEATURE_COLS_CAT_ALL]\nFEATURE_COLS_MODEL = CAT_COLS_MODEL + NUM_COLS_MODEL\nCAT_FEATURE_IDXS = [i for i, c in enumerate(FEATURE_COLS_MODEL) if c in CAT_COLS_MODEL]\n\nprint(\"MODE:\", MODE)\nprint(\"Total features:\", len(FEATURE_COLS_MODEL), \"| cat idx:\", len(CAT_FEATURE_IDXS))\n\n# ----------------------------\n# 2) Fold boundaries map (PURGED)\n# ----------------------------\nfold_map = {int(b[\"fold\"]): b for b in fold_boundaries}\nfold_ids = sorted(fold_map.keys())\n\n# horizons list\nif MODE == \"per_horizon\" and \"horizon\" in df_train.columns:\n    horizons = sorted(df_train[\"horizon\"].astype(str).unique().tolist())\nelse:\n    horizons = [\"__all__\"]\n\nprint(\"fold_ids:\", fold_ids)\nprint(\"horizons:\", horizons[:10], \"...\" if len(horizons) > 10 else \"\")\n\n# ----------------------------\n# 3) Helpers\n# ----------------------------\ndef _new_oof(n: int) -> np.ndarray:\n    return np.full(n, np.nan, dtype=np.float32)\n\ndef _clean_weight(w: np.ndarray) -> np.ndarray:\n    w = np.asarray(w, dtype=np.float64)\n    w = np.where(np.isfinite(w), w, 0.0)\n    w = np.maximum(w, 0.0)\n    return w\n\ndef _sample_df(df: pd.DataFrame, cap: int, p: np.ndarray, seed: int) -> pd.DataFrame:\n    if cap is None or cap <= 0 or len(df) <= cap:\n        return df\n    rs = np.random.RandomState(seed)\n    p = np.asarray(p, dtype=np.float64)\n    if (not np.isfinite(p).all()) or p.sum() <= 0:\n        idx = rs.choice(len(df), size=cap, replace=False)\n        return df.iloc[idx]\n    p = p / (p.sum() + 1e-12)\n    idx = rs.choice(len(df), size=cap, replace=False, p=p)\n    return df.iloc[idx]\n\ndef _fold_sample_weight(df: pd.DataFrame, anchor_ts: int, use_recency: bool, tau: float, clip_w_q: float | None) -> np.ndarray:\n    # official weight\n    w = df[WEIGHT_COL].to_numpy(np.float64) if (WEIGHT_COL in df.columns) else np.ones(len(df), dtype=np.float64)\n    w = _clean_weight(w)\n\n    if clip_w_q is not None:\n        q = float(np.nanquantile(w, float(clip_w_q)))\n        if np.isfinite(q) and q > 0:\n            w = np.minimum(w, q)\n\n    if use_recency:\n        t = df[TIME_COL].to_numpy(np.float64)\n        rec = np.exp(-(float(anchor_ts) - t) / float(tau))\n        rec = np.where(np.isfinite(rec), rec, 0.0)\n        w = w * rec\n\n    if w.sum() <= 1e-12:\n        w = np.ones(len(df), dtype=np.float64)\n    return w\n\ndef fit_alpha(y: np.ndarray, pred: np.ndarray, w: np.ndarray, clip=(0.0, 3.0)) -> float:\n    y = np.asarray(y, np.float64)\n    pred = np.asarray(pred, np.float64)\n    w = _clean_weight(w)\n    m = np.isfinite(y) & np.isfinite(pred)\n    if not np.any(m):\n        return 1.0\n    y, pred, w = y[m], pred[m], w[m]\n    num = float(np.sum(w * y * pred))\n    den = float(np.sum(w * pred * pred) + 1e-12)\n    a = num / den\n    return float(np.clip(a, clip[0], clip[1]))\n\ndef fit_blend_weights(pred_mat: np.ndarray, y: np.ndarray, w: np.ndarray, nonneg=True) -> np.ndarray:\n    y = np.asarray(y, np.float64)\n    P = np.asarray(pred_mat, np.float64)\n    w = _clean_weight(w)\n\n    m = np.isfinite(y) & np.isfinite(P).all(axis=1)\n    if not np.any(m):\n        b = np.ones(P.shape[1], dtype=np.float64) / P.shape[1]\n        return b\n\n    y = y[m]\n    P = P[m]\n    w = w[m]\n\n    sw = np.sqrt(np.maximum(w, 0.0))\n    Pw = P * sw[:, None]\n    yw = y * sw\n\n    b, *_ = np.linalg.lstsq(Pw, yw, rcond=None)\n    b = b.astype(np.float64)\n\n    if nonneg:\n        b = np.clip(b, 0.0, None)\n\n    s = b.sum()\n    if not np.isfinite(s) or s <= 0:\n        b = np.ones(P.shape[1], dtype=np.float64) / P.shape[1]\n    else:\n        b = b / s\n    return b\n\n# ----------------------------\n# 4) Imports for models\n# ----------------------------\nfrom catboost import CatBoostRegressor, Pool\n\ntry:\n    import lightgbm as lgb\n    _HAS_LGB = True\nexcept Exception:\n    _HAS_LGB = False\n    USE_LGBM = False\n    print(\"[WARN] lightgbm not available; skipping LGBM.\")\n\n# ----------------------------\n# 5) Train loop (PURGED per fold)\n# ----------------------------\ndf_train = df_train.copy()\ndf_train[\"fold\"] = df_train[\"fold\"].astype(int)\n\nN = len(df_train)\noof_store = {}          # name -> oof pred vector (len N)\nmodel_registry = []     # list of dicts with fold/horizon score\n\nt0 = time.time()\n\nfor h_key in horizons:\n    if h_key == \"__all__\":\n        mask_h = np.ones(N, dtype=bool)\n    else:\n        mask_h = (df_train[\"horizon\"].astype(str).to_numpy() == str(h_key))\n\n    n_h = int(mask_h.sum())\n    if n_h == 0:\n        continue\n\n    print(\"\\n\" + \"=\"*90)\n    print(\"HORIZON:\", h_key, \"| rows:\", n_h)\n\n    for k in fold_ids:\n        b = fold_map[int(k)]\n        train_max_ts = int(b[\"train_max_ts\"])\n        vmin = int(b[\"val_min_ts\"])\n        vmax = int(b[\"val_max_ts\"])\n\n        # VALID strictly inside [vmin,vmax]\n        va_mask = mask_h & (df_train[TIME_COL].to_numpy(np.int64) >= vmin) & (df_train[TIME_COL].to_numpy(np.int64) <= vmax)\n        # TRAIN strictly <= train_max_ts (purged)\n        tr_mask = mask_h & (df_train[TIME_COL].to_numpy(np.int64) <= train_max_ts)\n\n        n_va = int(va_mask.sum())\n        n_tr = int(tr_mask.sum())\n        if n_va == 0 or n_tr == 0:\n            continue\n\n        df_tr = df_train.loc[tr_mask]\n        df_va = df_train.loc[va_mask]\n\n        # weights for sampling/training anchored at train_max_ts\n        w_tr_full = _fold_sample_weight(df_tr, anchor_ts=train_max_ts, use_recency=USE_RECENCY, tau=TAU, clip_w_q=CLIP_W_Q)\n\n        # optional sampling (RAM/time)\n        if TRAIN_SAMPLE_CAP is not None and TRAIN_SAMPLE_CAP > 0 and len(df_tr) > TRAIN_SAMPLE_CAP:\n            df_tr = _sample_df(df_tr, cap=TRAIN_SAMPLE_CAP, p=w_tr_full, seed=10000 + 31*k + (0 if h_key==\"__all__\" else (hash(h_key) % 1000)))\n            w_tr_full = _fold_sample_weight(df_tr, anchor_ts=train_max_ts, use_recency=USE_RECENCY, tau=TAU, clip_w_q=CLIP_W_Q)\n\n        # build matrices\n        X_tr = df_tr[FEATURE_COLS_MODEL]\n        X_va = df_va[FEATURE_COLS_MODEL]\n        y_tr_raw = df_tr[TARGET_COL].to_numpy(np.float64)\n        y_va_raw = df_va[TARGET_COL].to_numpy(np.float64)\n\n        # eval weights should be official (NO recency) to match metric\n        w_va_eval = df_va[WEIGHT_COL].to_numpy(np.float64) if (WEIGHT_COL in df_va.columns) else np.ones(len(df_va), np.float64)\n        w_va_eval = _clean_weight(w_va_eval)\n\n        # target transform (fit scale only on TRAIN fold subset)\n        if USE_TARGET_ASINH:\n            s = float(np.nanmedian(np.abs(y_tr_raw)))\n            if not np.isfinite(s) or s <= 1e-6:\n                s = 1.0\n            y_tr = np.arcsinh(y_tr_raw / s).astype(np.float32)\n            y_va = np.arcsinh(y_va_raw / s).astype(np.float32)\n            inv = lambda z: (np.sinh(np.clip(z, -ASINH_CLIP_PRED, ASINH_CLIP_PRED)) * s).astype(np.float64)\n        else:\n            y_tr = y_tr_raw.astype(np.float32)\n            y_va = y_va_raw.astype(np.float32)\n            inv = lambda z: np.asarray(z, np.float64)\n\n        w_tr = _clean_weight(w_tr_full)\n\n        print(f\"  fold {k}: train(ts<= {train_max_ts})={len(df_tr):,} | valid[{vmin},{vmax}]={len(df_va):,}\")\n\n        # --- CatBoost\n        if USE_CATBOOST:\n            train_pool = Pool(X_tr, label=y_tr, weight=w_tr, cat_features=CAT_FEATURE_IDXS)\n            valid_pool = Pool(X_va, label=y_va, cat_features=CAT_FEATURE_IDXS)\n\n            for pi, pbase in enumerate(CB_PARAM_LIST):\n                for sd in CB_SEEDS:\n                    name = f\"cb_p{pi}_s{sd}\"\n                    if name not in oof_store:\n                        oof_store[name] = _new_oof(N)\n\n                    params = dict(\n                        loss_function=\"RMSE\",\n                        eval_metric=\"RMSE\",\n                        task_type=\"CPU\",\n                        thread_count=-1,\n                        random_seed=int(sd + 1000*k),\n                        allow_writing_files=False,\n                        od_type=\"Iter\",\n                        **pbase\n                    )\n\n                    model = CatBoostRegressor(**params)\n                    model.fit(\n                        train_pool,\n                        eval_set=valid_pool,\n                        use_best_model=True,\n                        verbose=False,\n                        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                    )\n\n                    pred_va_t = model.predict(valid_pool).astype(np.float64)\n                    pred_va = inv(pred_va_t)\n\n                    # store OOF\n                    oof_store[name][va_mask] = pred_va.astype(np.float32)\n\n                    sc = weighted_rmse_score(y_va_raw.astype(np.float64), pred_va.astype(np.float64), w_va_eval.astype(np.float64))\n                    model_registry.append(dict(family=\"catboost\", name=name, horizon=h_key, fold=int(k), score=float(sc)))\n\n        # --- LightGBM\n        if USE_LGBM and _HAS_LGB:\n            X_tr_l = X_tr.copy()\n            X_va_l = X_va.copy()\n            for c in CAT_COLS_MODEL:\n                if c in X_tr_l.columns:\n                    X_tr_l[c] = X_tr_l[c].astype(\"category\")\n                    X_va_l[c] = X_va_l[c].astype(\"category\")\n\n            # LightGBM trains on transformed y if enabled\n            y_tr_l = y_tr.astype(np.float32)\n            y_va_l = y_va.astype(np.float32)\n\n            dtrain = lgb.Dataset(X_tr_l, label=y_tr_l, weight=w_tr, categorical_feature=CAT_COLS_MODEL, free_raw_data=True)\n            dvalid = lgb.Dataset(X_va_l, label=y_va_l, weight=None, categorical_feature=CAT_COLS_MODEL, free_raw_data=True)\n\n            for sd in LGB_SEEDS:\n                name = f\"lgb_s{sd}\"\n                if name not in oof_store:\n                    oof_store[name] = _new_oof(N)\n\n                params = dict(LGB_PARAMS)\n                params[\"seed\"] = int(sd + 1000*k)\n\n                model = lgb.train(\n                    params,\n                    dtrain,\n                    num_boost_round=12000,\n                    valid_sets=[dvalid],\n                    valid_names=[\"valid\"],\n                    callbacks=[lgb.early_stopping(350, verbose=False), lgb.log_evaluation(0)],\n                )\n\n                pred_va_t = model.predict(X_va_l, num_iteration=model.best_iteration).astype(np.float64)\n                pred_va = inv(pred_va_t)\n\n                oof_store[name][va_mask] = pred_va.astype(np.float32)\n\n                sc = weighted_rmse_score(y_va_raw.astype(np.float64), pred_va.astype(np.float64), w_va_eval.astype(np.float64))\n                model_registry.append(dict(family=\"lgbm\", name=name, horizon=h_key, fold=int(k), score=float(sc)))\n\n        gc.collect()\n\nprint(\"\\nTraining done in\", round(time.time() - t0, 1), \"sec\")\nprint(\"OOF variants:\", len(oof_store))\n\n# ----------------------------\n# 6) Choose top models by global OOF (across all horizons)\n# ----------------------------\ny_all = df_train[TARGET_COL].to_numpy(np.float64)\nw_all = df_train[WEIGHT_COL].to_numpy(np.float64) if (WEIGHT_COL in df_train.columns) else np.ones(N, np.float64)\nw_all = _clean_weight(w_all)\n\noof_scores_global = []\nfor name, pred in oof_store.items():\n    p = pred.astype(np.float64)\n    m = np.isfinite(p) & np.isfinite(y_all)\n    if not np.any(m):\n        continue\n    sc = weighted_rmse_score(y_all[m], p[m], w_all[m])\n    oof_scores_global.append((float(sc), name))\n\noof_scores_global.sort(reverse=True, key=lambda x: x[0])\ntop_model_names = [n for _, n in oof_scores_global[:min(TOP_MODELS_KEEP, len(oof_scores_global))]]\n\nprint(\"\\nTop models kept for blending:\")\nfor s, n in oof_scores_global[:min(10, len(oof_scores_global))]:\n    tag = \" <KEEP>\" if n in top_model_names else \"\"\n    print(f\"  {n:10s}  score={s:.6f}{tag}\")\n\nif len(top_model_names) == 0:\n    raise RuntimeError(\"No valid OOF models found (all NaN).\")\n\n# ----------------------------\n# 7) Per-horizon blend + alpha\n# ----------------------------\nblend_weights_by_h = {}\nalpha_by_h = {}\noof_pred_blend = np.full(N, np.nan, dtype=np.float32)\n\n# pre-stack top preds for speed\nP_top = np.stack([oof_store[n] for n in top_model_names], axis=1).astype(np.float64)  # (N, M)\n\nfor h_key in horizons:\n    if h_key == \"__all__\":\n        idx = np.ones(N, dtype=bool)\n        hk = \"__all__\"\n    else:\n        idx = (df_train[\"horizon\"].astype(str).to_numpy() == str(h_key))\n        hk = str(h_key)\n\n    y = y_all[idx]\n    w = w_all[idx]\n    P = P_top[idx]\n\n    b = fit_blend_weights(P, y, w, nonneg=NONNEG_BLEND)\n    blend_weights_by_h[hk] = {top_model_names[i]: float(b[i]) for i in range(len(top_model_names))}\n\n    pred_bl = (P @ b).astype(np.float64)\n\n    a = fit_alpha(y, pred_bl, w, clip=(0.0, 3.0))\n    alpha_by_h[hk] = float(a)\n\n    oof_pred_blend[idx] = (pred_bl * a).astype(np.float32)\n\n# global CV\nm_ok = np.isfinite(oof_pred_blend) & np.isfinite(y_all)\ncv_score = weighted_rmse_score(y_all[m_ok], oof_pred_blend[m_ok].astype(np.float64), w_all[m_ok])\n\nprint(\"\\nCV score (OOF blend):\", float(cv_score))\nprint(\"alpha_by_h:\", alpha_by_h)\n\n# ----------------------------\n# 8) Save cfg\n# ----------------------------\nmodel_cfg_used = dict(\n    MODE=MODE,\n    USE_RECENCY=USE_RECENCY, TAU=float(TAU), CLIP_W_Q=float(CLIP_W_Q),\n    TRAIN_SAMPLE_CAP=int(TRAIN_SAMPLE_CAP), SAMPLE_WEIGHTED=bool(SAMPLE_WEIGHTED),\n    USE_CATBOOST=bool(USE_CATBOOST), USE_LGBM=bool(USE_LGBM and _HAS_LGB),\n    CB_SEEDS=CB_SEEDS, LGB_SEEDS=LGB_SEEDS,\n    CB_PARAM_LIST=CB_PARAM_LIST, LGB_PARAMS=LGB_PARAMS,\n    USE_TARGET_ASINH=bool(USE_TARGET_ASINH), ASINH_CLIP_PRED=float(ASINH_CLIP_PRED),\n    FEATURE_COLS_MODEL=FEATURE_COLS_MODEL,\n    CAT_COLS_MODEL=CAT_COLS_MODEL,\n    CAT_FEATURE_IDXS=CAT_FEATURE_IDXS,\n    fold_boundaries=fold_boundaries,\n    top_model_names=top_model_names,\n    blend_weights_by_h=blend_weights_by_h,\n    alpha_by_h=alpha_by_h,\n    cv_score=float(cv_score),\n)\n\n(OUT_DIR / \"stage6_cfg.json\").write_text(json.dumps(model_cfg_used, indent=2))\n(OUT_DIR / \"stage6_model_registry.json\").write_text(json.dumps(model_registry, indent=2))\n(OUT_DIR / \"stage6_oof_scores_global.json\").write_text(json.dumps(oof_scores_global[:200], indent=2))\nprint(\"Saved:\", str(OUT_DIR / \"stage6_cfg.json\"))\n\n# export globals\nglobals().update({\n    \"oof_pred_blend\": oof_pred_blend,\n    \"blend_weights_by_h\": blend_weights_by_h,\n    \"alpha_by_h\": alpha_by_h,\n    \"model_cfg_used\": model_cfg_used,\n    \"FEATURE_COLS_MODEL\": FEATURE_COLS_MODEL,\n    \"CAT_FEATURE_IDXS\": CAT_FEATURE_IDXS,\n    \"CAT_COLS_MODEL\": CAT_COLS_MODEL,\n    \"top_model_names\": top_model_names,\n    \"oof_store\": oof_store,\n    \"OUT_DIR_STAGE6\": str(OUT_DIR),\n})\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T00:15:51.533524Z","iopub.execute_input":"2026-01-14T00:15:51.533875Z"}},"outputs":[{"name":"stdout","text":"MODE: per_horizon\nTotal features: 190 | cat idx: 4\nfold_ids: [0, 1, 2, 3, 4]\nhorizons: ['1', '10', '25', '3'] \n\n==========================================================================================\nHORIZON: 1 | rows: 1394653\n  fold 0: train(ts<= 2519)=450,000 | valid[2521,2737]=99,142\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Final Fit, Test Inference, and Submission Packaging","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 7 — Final Fit, Test Inference, and Submission Packaging (REVISI FULL v5)\n# - CONSISTENT with STAGE 6 v5:\n#     * Purged-CV used asinh target transform -> apply same transform in final fit\n#     * Blend uses TOP model names (top_model_names) not family averages\n# - RAM-safe:\n#     * trains models ONE-BY-ONE, predicts, discards model (no big stacks)\n#     * uses float32 matrices where possible\n# - Per-horizon final fit (recommended), with robust fallback if horizon missing in test\n# - Saves lightweight artifacts (cfg + selected model list + submission)\n#\n# REQUIRE:\n#   df_train, df_test, TARGET_COL, ID_COL, TIME_COL, WEIGHT_COL\n#   FEATURE_COLS_MODEL, CAT_FEATURE_IDXS, CAT_COLS_MODEL\n#   make_sample_weight OR (better) _fold_sample_weight logic in CFG\n#   model_cfg_used (from Stage 6 v5)\n#\n# OUTPUT:\n#   /kaggle/working/submission.csv\n#   /kaggle/working/tsf_stage7_bundle_v5/bundle.json\n# ============================================================\n\nimport gc, json, time, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\nneed = [\"df_train\",\"df_test\",\"TARGET_COL\",\"ID_COL\",\"TIME_COL\",\"WEIGHT_COL\",\n        \"FEATURE_COLS_MODEL\",\"CAT_FEATURE_IDXS\",\"CAT_COLS_MODEL\",\"model_cfg_used\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing global: {k}. Jalankan stage sebelumnya.\")\n\nfrom catboost import CatBoostRegressor, Pool\ntry:\n    import lightgbm as lgb\n    _HAS_LGB = True\nexcept Exception:\n    _HAS_LGB = False\n\nCFG = model_cfg_used\nMODE = CFG.get(\"MODE\", \"per_horizon\")\n\n# blend maps produced by Stage 6 v5\nblend_weights_by_h = CFG[\"blend_weights_by_h\"]\nalpha_by_h = CFG[\"alpha_by_h\"]\ntop_model_names = CFG.get(\"top_model_names\", None)\nif not top_model_names:\n    raise RuntimeError(\"CFG missing top_model_names. Pastikan pakai STAGE 6 v5 (yang menyimpan top_model_names).\")\n\nUSE_CATBOOST = bool(CFG.get(\"USE_CATBOOST\", True))\nUSE_LGBM = bool(CFG.get(\"USE_LGBM\", False)) and _HAS_LGB\n\nCB_SEEDS = CFG.get(\"CB_SEEDS\", [42, 62])\nLGB_SEEDS = CFG.get(\"LGB_SEEDS\", [41, 51])\nCB_PARAM_LIST = CFG.get(\"CB_PARAM_LIST\", [])\nLGB_PARAMS = CFG.get(\"LGB_PARAMS\", {})\n\nUSE_RECENCY = bool(CFG.get(\"USE_RECENCY\", True))\nTAU = float(CFG.get(\"TAU\", 450.0))\nCLIP_W_Q = float(CFG.get(\"CLIP_W_Q\", 0.9995))\n\nUSE_TARGET_ASINH = bool(CFG.get(\"USE_TARGET_ASINH\", True))\nASINH_CLIP_PRED = float(CFG.get(\"ASINH_CLIP_PRED\", 8.0))\n\n# training anchor for recency: use TRAIN_MAX_TS (final training sees all past)\nTRAIN_MAX_TS = int(df_train[TIME_COL].max())\n\nOUT_DIR = Path(\"/kaggle/working/tsf_stage7_bundle_v5\")\nMODEL_DIR = OUT_DIR / \"final_models\"\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef _clean_weight(w: np.ndarray) -> np.ndarray:\n    w = np.asarray(w, dtype=np.float64)\n    w = np.where(np.isfinite(w), w, 0.0)\n    w = np.maximum(w, 0.0)\n    if w.sum() <= 1e-12:\n        w = np.ones_like(w, dtype=np.float64)\n    return w\n\ndef _make_final_weight(df: pd.DataFrame) -> np.ndarray:\n    # final-fit weight = official weight * recency (optional)\n    w = df[WEIGHT_COL].to_numpy(np.float64) if (WEIGHT_COL in df.columns) else np.ones(len(df), np.float64)\n    w = _clean_weight(w)\n\n    # clip huge weights (stability)\n    if CLIP_W_Q is not None:\n        q = float(np.nanquantile(w, float(CLIP_W_Q)))\n        if np.isfinite(q) and q > 0:\n            w = np.minimum(w, q)\n\n    if USE_RECENCY:\n        t = df[TIME_COL].to_numpy(np.float64)\n        rec = np.exp(-(float(TRAIN_MAX_TS) - t) / float(TAU))\n        rec = np.where(np.isfinite(rec), rec, 0.0)\n        w = w * rec\n\n    return _clean_weight(w)\n\ndef _asinh_fit_scale(y: np.ndarray) -> float:\n    s = float(np.nanmedian(np.abs(y)))\n    if not np.isfinite(s) or s <= 1e-6:\n        s = 1.0\n    return s\n\ndef _asinh_transform(y: np.ndarray, s: float) -> np.ndarray:\n    return np.arcsinh(y / s).astype(np.float32)\n\ndef _asinh_inverse(z: np.ndarray, s: float) -> np.ndarray:\n    z = np.asarray(z, np.float64)\n    z = np.clip(z, -ASINH_CLIP_PRED, ASINH_CLIP_PRED)\n    return (np.sinh(z) * s).astype(np.float64)\n\n# Parse which exact model instances to train from top_model_names\n# Expected names: \"cb_p{pi}_s{sd}\" and \"lgb_s{sd}\"\ndef _parse_top_models(top_names):\n    cb_list = []\n    lgb_list = []\n    for nm in top_names:\n        if nm.startswith(\"cb_\"):\n            # cb_p{pi}_s{sd}\n            try:\n                parts = nm.split(\"_\")\n                pi = int(parts[1].replace(\"p\",\"\"))\n                sd = int(parts[2].replace(\"s\",\"\"))\n                cb_list.append((pi, sd, nm))\n            except Exception:\n                pass\n        elif nm.startswith(\"lgb_\"):\n            try:\n                sd = int(nm.split(\"s\")[-1])\n                lgb_list.append((sd, nm))\n            except Exception:\n                pass\n    return cb_list, lgb_list\n\nCB_KEEP, LGB_KEEP = _parse_top_models(top_model_names)\nif USE_CATBOOST and (len(CB_KEEP) == 0):\n    print(\"[WARN] No CatBoost model in top_model_names (blend). CatBoost inference may be skipped.\")\nif USE_LGBM and (len(LGB_KEEP) == 0):\n    print(\"[WARN] No LGBM model in top_model_names (blend). LGBM inference may be skipped.\")\n\n# horizons\nif MODE == \"per_horizon\" and (\"horizon\" in df_train.columns):\n    horizons = sorted(df_train[\"horizon\"].astype(str).unique().tolist())\nelse:\n    horizons = [\"__all__\"]\n\n# Pre-init predictions\ntest_pred = np.zeros(len(df_test), dtype=np.float64)\n\n# ----------------------------\n# Main loop\n# ----------------------------\nt0 = time.time()\nfor h in horizons:\n    if h == \"__all__\":\n        tr_idx = np.ones(len(df_train), dtype=bool)\n        te_idx = np.ones(len(df_test), dtype=bool)\n        h_key = \"__all__\"\n    else:\n        tr_idx = (df_train[\"horizon\"].astype(str).to_numpy() == str(h))\n        if \"horizon\" in df_test.columns:\n            te_idx = (df_test[\"horizon\"].astype(str).to_numpy() == str(h))\n        else:\n            te_idx = np.ones(len(df_test), dtype=bool)\n        h_key = str(h)\n\n    ntr = int(tr_idx.sum())\n    nte = int(te_idx.sum())\n    if ntr == 0 or nte == 0:\n        continue\n\n    # prepare data (keep as float32 where possible)\n    X_tr = df_train.loc[tr_idx, FEATURE_COLS_MODEL]\n    y_tr_raw = df_train.loc[tr_idx, TARGET_COL].to_numpy(np.float64)\n    w_tr = _make_final_weight(df_train.loc[tr_idx])\n\n    X_te = df_test.loc[te_idx, FEATURE_COLS_MODEL]\n\n    # target transform consistent with Stage 6 v5\n    if USE_TARGET_ASINH:\n        s = _asinh_fit_scale(y_tr_raw)\n        y_tr = _asinh_transform(y_tr_raw, s)\n        inv = lambda z: _asinh_inverse(z, s)\n    else:\n        y_tr = y_tr_raw.astype(np.float32)\n        inv = lambda z: np.asarray(z, np.float64)\n\n    # Which models to use for this horizon blend?\n    # weights are stored by horizon key; if missing -> fallback uniform over available comps\n    wts = blend_weights_by_h.get(h_key, None)\n    if wts is None:\n        wts = blend_weights_by_h.get(\"__all__\", None)\n\n    # We'll accumulate prediction by iterating each kept model (RAM-safe).\n    pred_acc = np.zeros(nte, dtype=np.float64)\n    used_any = False\n\n    # ---------- CatBoost models ----------\n    if USE_CATBOOST and len(CB_PARAM_LIST) and len(CB_KEEP):\n        pool_tr = Pool(X_tr, label=y_tr, weight=w_tr, cat_features=CAT_FEATURE_IDXS)\n        pool_te = Pool(X_te, cat_features=CAT_FEATURE_IDXS)\n\n        for (pi, sd, name) in CB_KEEP:\n            # skip if weight for this model is 0 (for this horizon)\n            if wts is not None and float(wts.get(name, 0.0)) <= 0:\n                continue\n\n            if pi < 0 or pi >= len(CB_PARAM_LIST):\n                continue\n\n            pbase = CB_PARAM_LIST[pi]\n            params = dict(\n                loss_function=\"RMSE\",\n                eval_metric=\"RMSE\",\n                task_type=\"CPU\",\n                thread_count=-1,\n                random_seed=int(sd + 777),\n                allow_writing_files=False,\n                **pbase\n            )\n            model = CatBoostRegressor(**params)\n            model.fit(pool_tr, verbose=False)\n\n            pred_t = model.predict(pool_te).astype(np.float64)  # transformed space if asinh enabled\n            pred = inv(pred_t)\n\n            wt = float(wts.get(name, 0.0)) if wts is not None else 1.0\n            pred_acc += wt * pred\n            used_any = True\n\n            # optional save\n            model.save_model(str(MODEL_DIR / f\"cb_h{h_key}_{name}.cbm\"))\n            del model, pred_t, pred\n            gc.collect()\n\n    # ---------- LightGBM models ----------\n    if USE_LGBM and len(LGB_KEEP):\n        X_tr_l = X_tr.copy()\n        X_te_l = X_te.copy()\n        for c in CAT_COLS_MODEL:\n            if c in X_tr_l.columns:\n                X_tr_l[c] = X_tr_l[c].astype(\"category\")\n                X_te_l[c] = X_te_l[c].astype(\"category\")\n\n        dtrain = lgb.Dataset(X_tr_l, label=y_tr.astype(np.float32), weight=w_tr,\n                             categorical_feature=CAT_COLS_MODEL, free_raw_data=True)\n\n        num_boost = int(CFG.get(\"LGB_NUM_BOOST\", 6000))\n        for (sd, name) in LGB_KEEP:\n            if wts is not None and float(wts.get(name, 0.0)) <= 0:\n                continue\n\n            params = dict(LGB_PARAMS)\n            params[\"seed\"] = int(sd + 777)\n\n            model = lgb.train(params, dtrain, num_boost_round=num_boost)\n            pred_t = model.predict(X_te_l).astype(np.float64)\n            pred = inv(pred_t)\n\n            wt = float(wts.get(name, 0.0)) if wts is not None else 1.0\n            pred_acc += wt * pred\n            used_any = True\n\n            model.save_model(str(MODEL_DIR / f\"lgb_h{h_key}_{name}.txt\"))\n            del model, pred_t, pred\n            gc.collect()\n\n        del X_tr_l, X_te_l, dtrain\n        gc.collect()\n\n    # fallback if wts missing or all 0\n    if not used_any:\n        # uniform average over available kept models (rare)\n        print(f\"[WARN] No model used for horizon {h_key}. Fallback to mean=0.\")\n        pred_acc = np.zeros(nte, dtype=np.float64)\n\n    # apply alpha calibration (per horizon, fallback to __all__)\n    a = alpha_by_h.get(h_key, alpha_by_h.get(\"__all__\", 1.0))\n    pred_acc *= float(a)\n\n    # assign\n    test_pred[te_idx] = pred_acc\n\n    print(f\"H={h_key} | n_tr={ntr} n_te={nte} | used_models={used_any} | alpha={a}\")\n\n    del X_tr, X_te, y_tr_raw, y_tr, w_tr, pred_acc\n    gc.collect()\n\nprint(\"Final inference done in\", round(time.time()-t0, 1), \"sec\")\n\n# ----------------------------\n# Submission\n# ----------------------------\nSUB_PATH = Path(\"/kaggle/working/submission.csv\")\nsub = pd.DataFrame({ID_COL: df_test[ID_COL].astype(str).values, \"prediction\": test_pred.astype(np.float64)})\nsub.to_csv(SUB_PATH, index=False)\nprint(\"Saved submission:\", str(SUB_PATH), \"| shape:\", sub.shape)\n\n# ----------------------------\n# Bundle for reproducibility\n# ----------------------------\nbundle = dict(\n    created_utc=float(time.time()),\n    cfg=CFG,\n    top_model_names=top_model_names,\n    features=FEATURE_COLS_MODEL,\n    cat_cols=CAT_COLS_MODEL,\n    cat_feature_idxs=CAT_FEATURE_IDXS,\n    id_col=ID_COL, time_col=TIME_COL, weight_col=WEIGHT_COL, target_col=TARGET_COL,\n    train_shape=list(df_train.shape),\n    test_shape=list(df_test.shape),\n)\n(OUT_DIR / \"bundle.json\").write_text(json.dumps(bundle, indent=2))\nprint(\"Saved bundle:\", str(OUT_DIR / \"bundle.json\"))\n\nglobals().update({\n    \"test_pred\": test_pred,\n    \"SUB_PATH\": str(SUB_PATH),\n    \"BUNDLE_DIR\": str(OUT_DIR),\n})\n\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}