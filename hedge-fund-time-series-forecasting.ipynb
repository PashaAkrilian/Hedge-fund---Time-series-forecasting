{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3e0f09",
   "metadata": {
    "papermill": {
     "duration": 0.005447,
     "end_time": "2026-01-13T16:34:38.539068",
     "exception": false,
     "start_time": "2026-01-13T16:34:38.533621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data & Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7538e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T16:34:38.549253Z",
     "iopub.status.busy": "2026-01-13T16:34:38.548878Z",
     "iopub.status.idle": "2026-01-13T16:35:02.777683Z",
     "shell.execute_reply": "2026-01-13T16:35:02.776640Z"
    },
    "papermill": {
     "duration": 24.237015,
     "end_time": "2026-01-13T16:35:02.779942",
     "exception": false,
     "start_time": "2026-01-13T16:34:38.542927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parquet...\n",
      "\n",
      "==================== BASIC SHAPES ====================\n",
      "train: (5337414, 94)\n",
      "test : (1447107, 92)\n",
      "\n",
      "==================== COLUMNS ====================\n",
      "train cols: 94\n",
      "test  cols: 92\n",
      "\n",
      "==================== KEY COLS ====================\n",
      "ID_COL     : id | exists: True\n",
      "TIME_COL   : ts_index | exists: True\n",
      "WEIGHT_COL : weight | exists: False\n",
      "TARGET_COL : y_target | exists in train: True | exists in test: False\n",
      "\n",
      "==================== QUICK CHECKS ====================\n",
      "train id unique: 5337414 / 5337414\n",
      "test  id unique: 1447107 / 1447107\n",
      "train ts_index range: 1 -> 3601\n",
      "test  ts_index range: 3602 -> 4376\n",
      "\n",
      "train horizon value counts (top):\n",
      "horizon\n",
      "1     1394653\n",
      "3     1385816\n",
      "10    1337236\n",
      "25    1219709\n",
      "Name: count, dtype: int64\n",
      "\n",
      "test horizon value counts (top):\n",
      "horizon\n",
      "1     379617\n",
      "3     376558\n",
      "10    362057\n",
      "25    328875\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================== MISSING VALUES (TOP) ====================\n",
      "train missing rate top 15:\n",
      "feature_at    0.124719\n",
      "feature_by    0.110192\n",
      "feature_ay    0.085420\n",
      "feature_cd    0.074964\n",
      "feature_ce    0.051678\n",
      "feature_cf    0.044289\n",
      "feature_al    0.042233\n",
      "feature_aw    0.038444\n",
      "feature_bz    0.028426\n",
      "feature_bi    0.027622\n",
      "feature_k     0.011059\n",
      "feature_i     0.011059\n",
      "feature_h     0.010954\n",
      "feature_j     0.010954\n",
      "feature_cg    0.007428\n",
      "dtype: float64\n",
      "\n",
      "test missing rate top 15:\n",
      "feature_z     0.385765\n",
      "feature_y     0.385765\n",
      "feature_x     0.385765\n",
      "feature_w     0.385765\n",
      "feature_at    0.092342\n",
      "feature_by    0.092043\n",
      "feature_ay    0.057988\n",
      "feature_cd    0.057969\n",
      "feature_aw    0.038026\n",
      "feature_bz    0.037755\n",
      "feature_cf    0.035255\n",
      "feature_ce    0.035255\n",
      "feature_bi    0.033526\n",
      "feature_al    0.032591\n",
      "feature_av    0.001190\n",
      "dtype: float64\n",
      "\n",
      "==================== TARGET STATS ====================\n",
      "count: 5337414  /  5337414\n",
      "mean : -0.665904836792942\n",
      "std  : 32.52763850220829\n",
      "min  : -2201.8815779044935\n",
      "p1   : -82.79721584066921\n",
      "p50  : -0.0005774817524177132\n",
      "p99  : 62.923418606328426\n",
      "max  : 2314.4111524982895\n",
      "\n",
      "==================== WEIGHT STATS (NOT A FEATURE) ====================\n",
      "mean : 16427879.645291774\n",
      "min  : 0.0\n",
      "p50  : 1699.3843705131449\n",
      "p99  : 303840772.74105984\n",
      "max  : 13912217783333.135\n",
      "\n",
      "==================== FEATURE SET SUMMARY ====================\n",
      "CAT_COLS : ['code', 'sub_code', 'sub_category', 'horizon']\n",
      "NUM_COLS : 88 (numeric features excluding weight/target/id)\n",
      "FEAT_COLS: 91 (all usable columns excluding target and weight; id excluded)\n",
      "\n",
      "==================== HEAD (train) ====================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>code</th>\n",
       "      <th>sub_code</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>horizon</th>\n",
       "      <th>ts_index</th>\n",
       "      <th>feature_a</th>\n",
       "      <th>feature_b</th>\n",
       "      <th>feature_c</th>\n",
       "      <th>feature_d</th>\n",
       "      <th>feature_e</th>\n",
       "      <th>feature_f</th>\n",
       "      <th>feature_g</th>\n",
       "      <th>feature_h</th>\n",
       "      <th>feature_i</th>\n",
       "      <th>feature_j</th>\n",
       "      <th>feature_k</th>\n",
       "      <th>feature_l</th>\n",
       "      <th>feature_m</th>\n",
       "      <th>feature_n</th>\n",
       "      <th>feature_o</th>\n",
       "      <th>feature_p</th>\n",
       "      <th>feature_q</th>\n",
       "      <th>feature_r</th>\n",
       "      <th>feature_s</th>\n",
       "      <th>feature_t</th>\n",
       "      <th>feature_u</th>\n",
       "      <th>feature_v</th>\n",
       "      <th>feature_w</th>\n",
       "      <th>feature_x</th>\n",
       "      <th>feature_y</th>\n",
       "      <th>feature_z</th>\n",
       "      <th>feature_aa</th>\n",
       "      <th>feature_ab</th>\n",
       "      <th>feature_ac</th>\n",
       "      <th>feature_ad</th>\n",
       "      <th>feature_ae</th>\n",
       "      <th>feature_af</th>\n",
       "      <th>feature_ag</th>\n",
       "      <th>feature_ah</th>\n",
       "      <th>feature_ai</th>\n",
       "      <th>feature_aj</th>\n",
       "      <th>feature_ak</th>\n",
       "      <th>feature_al</th>\n",
       "      <th>feature_am</th>\n",
       "      <th>feature_an</th>\n",
       "      <th>feature_ao</th>\n",
       "      <th>feature_ap</th>\n",
       "      <th>feature_aq</th>\n",
       "      <th>feature_ar</th>\n",
       "      <th>feature_as</th>\n",
       "      <th>feature_at</th>\n",
       "      <th>feature_au</th>\n",
       "      <th>feature_av</th>\n",
       "      <th>feature_aw</th>\n",
       "      <th>feature_ax</th>\n",
       "      <th>feature_ay</th>\n",
       "      <th>feature_az</th>\n",
       "      <th>feature_ba</th>\n",
       "      <th>feature_bb</th>\n",
       "      <th>feature_bc</th>\n",
       "      <th>feature_bd</th>\n",
       "      <th>feature_be</th>\n",
       "      <th>feature_bf</th>\n",
       "      <th>feature_bg</th>\n",
       "      <th>feature_bh</th>\n",
       "      <th>feature_bi</th>\n",
       "      <th>feature_bj</th>\n",
       "      <th>feature_bk</th>\n",
       "      <th>feature_bl</th>\n",
       "      <th>feature_bm</th>\n",
       "      <th>feature_bn</th>\n",
       "      <th>feature_bo</th>\n",
       "      <th>feature_bp</th>\n",
       "      <th>feature_bq</th>\n",
       "      <th>feature_br</th>\n",
       "      <th>feature_bs</th>\n",
       "      <th>feature_bt</th>\n",
       "      <th>feature_bu</th>\n",
       "      <th>feature_bv</th>\n",
       "      <th>feature_bw</th>\n",
       "      <th>feature_bx</th>\n",
       "      <th>feature_by</th>\n",
       "      <th>feature_bz</th>\n",
       "      <th>feature_ca</th>\n",
       "      <th>feature_cb</th>\n",
       "      <th>feature_cc</th>\n",
       "      <th>feature_cd</th>\n",
       "      <th>feature_ce</th>\n",
       "      <th>feature_cf</th>\n",
       "      <th>feature_cg</th>\n",
       "      <th>feature_ch</th>\n",
       "      <th>y_target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W2MW3G2L__J0G2B0KU__PZ9S1Z4V__25__89</td>\n",
       "      <td>W2MW3G2L</td>\n",
       "      <td>J0G2B0KU</td>\n",
       "      <td>PZ9S1Z4V</td>\n",
       "      <td>25</td>\n",
       "      <td>89</td>\n",
       "      <td>29</td>\n",
       "      <td>16.364093</td>\n",
       "      <td>7.464023</td>\n",
       "      <td>5.966933</td>\n",
       "      <td>1.622184</td>\n",
       "      <td>10.261360</td>\n",
       "      <td>4.914369</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.023686</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.744244</td>\n",
       "      <td>2.001013</td>\n",
       "      <td>-0.01687</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>0.021502</td>\n",
       "      <td>0.901966</td>\n",
       "      <td>0.402125</td>\n",
       "      <td>0.038566</td>\n",
       "      <td>0.177947</td>\n",
       "      <td>0.091141</td>\n",
       "      <td>-84.968733</td>\n",
       "      <td>-1.765306</td>\n",
       "      <td>10.109641</td>\n",
       "      <td>145.320404</td>\n",
       "      <td>0.08958</td>\n",
       "      <td>0.868698</td>\n",
       "      <td>0.080088</td>\n",
       "      <td>0.101631</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.092776</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.298973</td>\n",
       "      <td>7.321646</td>\n",
       "      <td>3.628258</td>\n",
       "      <td>0.453027</td>\n",
       "      <td>-0.080212</td>\n",
       "      <td>0.192181</td>\n",
       "      <td>0.510727</td>\n",
       "      <td>17.136629</td>\n",
       "      <td>0.267856</td>\n",
       "      <td>7.745722</td>\n",
       "      <td>4.037853</td>\n",
       "      <td>4.85679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.188995</td>\n",
       "      <td>79.423474</td>\n",
       "      <td>244.471191</td>\n",
       "      <td>13.848771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01707</td>\n",
       "      <td>0.709292</td>\n",
       "      <td>21.80395</td>\n",
       "      <td>0.120968</td>\n",
       "      <td>26999.430482</td>\n",
       "      <td>34126.269444</td>\n",
       "      <td>791.709562</td>\n",
       "      <td>0.15467</td>\n",
       "      <td>9499.742248</td>\n",
       "      <td>1.266071</td>\n",
       "      <td>429.318704</td>\n",
       "      <td>2540.88981</td>\n",
       "      <td>0.008927</td>\n",
       "      <td>1.122459</td>\n",
       "      <td>23.815924</td>\n",
       "      <td>0.54985</td>\n",
       "      <td>0.067941</td>\n",
       "      <td>0.076033</td>\n",
       "      <td>0.02759</td>\n",
       "      <td>-0.47269</td>\n",
       "      <td>-0.202944</td>\n",
       "      <td>-3.769914</td>\n",
       "      <td>0.104535</td>\n",
       "      <td>3.040304</td>\n",
       "      <td>4.499546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.058543</td>\n",
       "      <td>-0.001686</td>\n",
       "      <td>-0.105328</td>\n",
       "      <td>-0.005045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.133697</td>\n",
       "      <td>2.849819</td>\n",
       "      <td>0.112068</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.551324</td>\n",
       "      <td>40.982572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W2MW3G2L__J0G2B0KU__PZ9S1Z4V__1__89</td>\n",
       "      <td>W2MW3G2L</td>\n",
       "      <td>J0G2B0KU</td>\n",
       "      <td>PZ9S1Z4V</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>53</td>\n",
       "      <td>2.858806</td>\n",
       "      <td>5.050617</td>\n",
       "      <td>15.906651</td>\n",
       "      <td>10.879453</td>\n",
       "      <td>3.072151</td>\n",
       "      <td>4.091032</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.023686</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.744244</td>\n",
       "      <td>2.001013</td>\n",
       "      <td>-0.01687</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>0.021502</td>\n",
       "      <td>0.901966</td>\n",
       "      <td>0.402125</td>\n",
       "      <td>0.038566</td>\n",
       "      <td>0.177947</td>\n",
       "      <td>0.091141</td>\n",
       "      <td>-84.968733</td>\n",
       "      <td>-1.765306</td>\n",
       "      <td>10.109641</td>\n",
       "      <td>145.320404</td>\n",
       "      <td>0.08958</td>\n",
       "      <td>0.868698</td>\n",
       "      <td>0.080088</td>\n",
       "      <td>0.101631</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.092776</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.298973</td>\n",
       "      <td>7.321646</td>\n",
       "      <td>3.628258</td>\n",
       "      <td>0.453027</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.192181</td>\n",
       "      <td>0.510727</td>\n",
       "      <td>17.136629</td>\n",
       "      <td>0.267856</td>\n",
       "      <td>7.745722</td>\n",
       "      <td>4.037853</td>\n",
       "      <td>4.85679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.188995</td>\n",
       "      <td>79.423474</td>\n",
       "      <td>244.471191</td>\n",
       "      <td>13.848771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01707</td>\n",
       "      <td>0.709292</td>\n",
       "      <td>21.80395</td>\n",
       "      <td>0.120968</td>\n",
       "      <td>26999.430482</td>\n",
       "      <td>34126.269444</td>\n",
       "      <td>791.709562</td>\n",
       "      <td>0.15467</td>\n",
       "      <td>9499.742248</td>\n",
       "      <td>1.266071</td>\n",
       "      <td>429.318704</td>\n",
       "      <td>2540.88981</td>\n",
       "      <td>0.008927</td>\n",
       "      <td>1.122459</td>\n",
       "      <td>23.815924</td>\n",
       "      <td>0.54985</td>\n",
       "      <td>0.067941</td>\n",
       "      <td>0.076033</td>\n",
       "      <td>0.02759</td>\n",
       "      <td>-0.47269</td>\n",
       "      <td>-0.202944</td>\n",
       "      <td>-3.769914</td>\n",
       "      <td>0.104535</td>\n",
       "      <td>3.040304</td>\n",
       "      <td>4.499546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.058543</td>\n",
       "      <td>-0.001686</td>\n",
       "      <td>-0.105328</td>\n",
       "      <td>-0.005045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.133697</td>\n",
       "      <td>2.849819</td>\n",
       "      <td>0.112068</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.315583</td>\n",
       "      <td>150.075406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W2MW3G2L__J0G2B0KU__PZ9S1Z4V__3__89</td>\n",
       "      <td>W2MW3G2L</td>\n",
       "      <td>J0G2B0KU</td>\n",
       "      <td>PZ9S1Z4V</td>\n",
       "      <td>3</td>\n",
       "      <td>89</td>\n",
       "      <td>51</td>\n",
       "      <td>9.585452</td>\n",
       "      <td>1.076268</td>\n",
       "      <td>9.004147</td>\n",
       "      <td>16.740490</td>\n",
       "      <td>15.166901</td>\n",
       "      <td>11.427983</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.023686</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.744244</td>\n",
       "      <td>2.001013</td>\n",
       "      <td>-0.01687</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>0.021502</td>\n",
       "      <td>0.901966</td>\n",
       "      <td>0.402125</td>\n",
       "      <td>0.038566</td>\n",
       "      <td>0.177947</td>\n",
       "      <td>0.091141</td>\n",
       "      <td>-84.968733</td>\n",
       "      <td>-1.765306</td>\n",
       "      <td>10.109641</td>\n",
       "      <td>145.320404</td>\n",
       "      <td>0.08958</td>\n",
       "      <td>0.868698</td>\n",
       "      <td>0.080088</td>\n",
       "      <td>0.101631</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.092776</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.298973</td>\n",
       "      <td>7.321646</td>\n",
       "      <td>3.628258</td>\n",
       "      <td>0.453027</td>\n",
       "      <td>-0.045494</td>\n",
       "      <td>0.192181</td>\n",
       "      <td>0.510727</td>\n",
       "      <td>17.136629</td>\n",
       "      <td>0.267856</td>\n",
       "      <td>7.745722</td>\n",
       "      <td>4.037853</td>\n",
       "      <td>4.85679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.188995</td>\n",
       "      <td>79.423474</td>\n",
       "      <td>244.471191</td>\n",
       "      <td>13.848771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01707</td>\n",
       "      <td>0.709292</td>\n",
       "      <td>21.80395</td>\n",
       "      <td>0.120968</td>\n",
       "      <td>26999.430482</td>\n",
       "      <td>34126.269444</td>\n",
       "      <td>791.709562</td>\n",
       "      <td>0.15467</td>\n",
       "      <td>9499.742248</td>\n",
       "      <td>1.266071</td>\n",
       "      <td>429.318704</td>\n",
       "      <td>2540.88981</td>\n",
       "      <td>0.008927</td>\n",
       "      <td>1.122459</td>\n",
       "      <td>23.815924</td>\n",
       "      <td>0.54985</td>\n",
       "      <td>0.067941</td>\n",
       "      <td>0.076033</td>\n",
       "      <td>0.02759</td>\n",
       "      <td>-0.47269</td>\n",
       "      <td>-0.202944</td>\n",
       "      <td>-3.769914</td>\n",
       "      <td>0.104535</td>\n",
       "      <td>3.040304</td>\n",
       "      <td>4.499546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.058543</td>\n",
       "      <td>-0.001686</td>\n",
       "      <td>-0.105328</td>\n",
       "      <td>-0.005045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.133697</td>\n",
       "      <td>2.849819</td>\n",
       "      <td>0.112068</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.362894</td>\n",
       "      <td>115.953552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id      code  sub_code sub_category  horizon  ts_index  feature_a  feature_b  feature_c  feature_d  feature_e  feature_f  feature_g  feature_h  feature_i  \\\n",
       "0  W2MW3G2L__J0G2B0KU__PZ9S1Z4V__25__89  W2MW3G2L  J0G2B0KU     PZ9S1Z4V       25        89         29  16.364093   7.464023   5.966933   1.622184  10.261360   4.914369   0.000467   0.023686   \n",
       "1   W2MW3G2L__J0G2B0KU__PZ9S1Z4V__1__89  W2MW3G2L  J0G2B0KU     PZ9S1Z4V        1        89         53   2.858806   5.050617  15.906651  10.879453   3.072151   4.091032   0.000467   0.023686   \n",
       "2   W2MW3G2L__J0G2B0KU__PZ9S1Z4V__3__89  W2MW3G2L  J0G2B0KU     PZ9S1Z4V        3        89         51   9.585452   1.076268   9.004147  16.740490  15.166901  11.427983   0.000467   0.023686   \n",
       "\n",
       "   feature_j  feature_k  feature_l  feature_m  feature_n  feature_o  feature_p  feature_q  feature_r  feature_s  feature_t  feature_u  feature_v  feature_w  feature_x  feature_y   feature_z  \\\n",
       "0   0.006409   0.000187   0.744244   2.001013   -0.01687   0.009892   0.013162   0.021502   0.901966   0.402125   0.038566   0.177947   0.091141 -84.968733  -1.765306  10.109641  145.320404   \n",
       "1   0.006409   0.000187   0.744244   2.001013   -0.01687   0.009892   0.013162   0.021502   0.901966   0.402125   0.038566   0.177947   0.091141 -84.968733  -1.765306  10.109641  145.320404   \n",
       "2   0.006409   0.000187   0.744244   2.001013   -0.01687   0.009892   0.013162   0.021502   0.901966   0.402125   0.038566   0.177947   0.091141 -84.968733  -1.765306  10.109641  145.320404   \n",
       "\n",
       "   feature_aa  feature_ab  feature_ac  feature_ad  feature_ae  feature_af  feature_ag  feature_ah  feature_ai  feature_aj  feature_ak  feature_al  feature_am  feature_an  feature_ao  feature_ap  \\\n",
       "0     0.08958    0.868698    0.080088    0.101631    0.026555    0.092776       0.004    1.298973    7.321646    3.628258    0.453027   -0.080212    0.192181    0.510727   17.136629    0.267856   \n",
       "1     0.08958    0.868698    0.080088    0.101631    0.026555    0.092776       0.004    1.298973    7.321646    3.628258    0.453027    0.001480    0.192181    0.510727   17.136629    0.267856   \n",
       "2     0.08958    0.868698    0.080088    0.101631    0.026555    0.092776       0.004    1.298973    7.321646    3.628258    0.453027   -0.045494    0.192181    0.510727   17.136629    0.267856   \n",
       "\n",
       "   feature_aq  feature_ar  feature_as  feature_at  feature_au  feature_av  feature_aw  feature_ax  feature_ay  feature_az  feature_ba  feature_bb  feature_bc    feature_bd    feature_be  feature_bf  \\\n",
       "0    7.745722    4.037853     4.85679         NaN    5.188995   79.423474  244.471191   13.848771         NaN     0.01707    0.709292    21.80395    0.120968  26999.430482  34126.269444  791.709562   \n",
       "1    7.745722    4.037853     4.85679         NaN    5.188995   79.423474  244.471191   13.848771         NaN     0.01707    0.709292    21.80395    0.120968  26999.430482  34126.269444  791.709562   \n",
       "2    7.745722    4.037853     4.85679         NaN    5.188995   79.423474  244.471191   13.848771         NaN     0.01707    0.709292    21.80395    0.120968  26999.430482  34126.269444  791.709562   \n",
       "\n",
       "   feature_bg   feature_bh  feature_bi  feature_bj  feature_bk  feature_bl  feature_bm  feature_bn  feature_bo  feature_bp  feature_bq  feature_br  feature_bs  feature_bt  feature_bu  feature_bv  \\\n",
       "0     0.15467  9499.742248    1.266071  429.318704  2540.88981    0.008927    1.122459   23.815924     0.54985    0.067941    0.076033     0.02759    -0.47269   -0.202944   -3.769914    0.104535   \n",
       "1     0.15467  9499.742248    1.266071  429.318704  2540.88981    0.008927    1.122459   23.815924     0.54985    0.067941    0.076033     0.02759    -0.47269   -0.202944   -3.769914    0.104535   \n",
       "2     0.15467  9499.742248    1.266071  429.318704  2540.88981    0.008927    1.122459   23.815924     0.54985    0.067941    0.076033     0.02759    -0.47269   -0.202944   -3.769914    0.104535   \n",
       "\n",
       "   feature_bw  feature_bx  feature_by  feature_bz  feature_ca  feature_cb  feature_cc  feature_cd  feature_ce  feature_cf  feature_cg  feature_ch  y_target      weight  \n",
       "0    3.040304    4.499546         NaN   -0.058543   -0.001686   -0.105328   -0.005045         NaN   -0.133697    2.849819    0.112068           1 -0.551324   40.982572  \n",
       "1    3.040304    4.499546         NaN   -0.058543   -0.001686   -0.105328   -0.005045         NaN   -0.133697    2.849819    0.112068           1 -0.315583  150.075406  \n",
       "2    3.040304    4.499546         NaN   -0.058543   -0.001686   -0.105328   -0.005045         NaN   -0.133697    2.849819    0.112068           1 -0.362894  115.953552  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== HEAD (test) ====================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>code</th>\n",
       "      <th>sub_code</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>horizon</th>\n",
       "      <th>ts_index</th>\n",
       "      <th>feature_a</th>\n",
       "      <th>feature_b</th>\n",
       "      <th>feature_c</th>\n",
       "      <th>feature_d</th>\n",
       "      <th>feature_e</th>\n",
       "      <th>feature_f</th>\n",
       "      <th>feature_g</th>\n",
       "      <th>feature_h</th>\n",
       "      <th>feature_i</th>\n",
       "      <th>feature_j</th>\n",
       "      <th>feature_k</th>\n",
       "      <th>feature_l</th>\n",
       "      <th>feature_m</th>\n",
       "      <th>feature_n</th>\n",
       "      <th>feature_o</th>\n",
       "      <th>feature_p</th>\n",
       "      <th>feature_q</th>\n",
       "      <th>feature_r</th>\n",
       "      <th>feature_s</th>\n",
       "      <th>feature_t</th>\n",
       "      <th>feature_u</th>\n",
       "      <th>feature_v</th>\n",
       "      <th>feature_w</th>\n",
       "      <th>feature_x</th>\n",
       "      <th>feature_y</th>\n",
       "      <th>feature_z</th>\n",
       "      <th>feature_aa</th>\n",
       "      <th>feature_ab</th>\n",
       "      <th>feature_ac</th>\n",
       "      <th>feature_ad</th>\n",
       "      <th>feature_ae</th>\n",
       "      <th>feature_af</th>\n",
       "      <th>feature_ag</th>\n",
       "      <th>feature_ah</th>\n",
       "      <th>feature_ai</th>\n",
       "      <th>feature_aj</th>\n",
       "      <th>feature_ak</th>\n",
       "      <th>feature_al</th>\n",
       "      <th>feature_am</th>\n",
       "      <th>feature_an</th>\n",
       "      <th>feature_ao</th>\n",
       "      <th>feature_ap</th>\n",
       "      <th>feature_aq</th>\n",
       "      <th>feature_ar</th>\n",
       "      <th>feature_as</th>\n",
       "      <th>feature_at</th>\n",
       "      <th>feature_au</th>\n",
       "      <th>feature_av</th>\n",
       "      <th>feature_aw</th>\n",
       "      <th>feature_ax</th>\n",
       "      <th>feature_ay</th>\n",
       "      <th>feature_az</th>\n",
       "      <th>feature_ba</th>\n",
       "      <th>feature_bb</th>\n",
       "      <th>feature_bc</th>\n",
       "      <th>feature_bd</th>\n",
       "      <th>feature_be</th>\n",
       "      <th>feature_bf</th>\n",
       "      <th>feature_bg</th>\n",
       "      <th>feature_bh</th>\n",
       "      <th>feature_bi</th>\n",
       "      <th>feature_bj</th>\n",
       "      <th>feature_bk</th>\n",
       "      <th>feature_bl</th>\n",
       "      <th>feature_bm</th>\n",
       "      <th>feature_bn</th>\n",
       "      <th>feature_bo</th>\n",
       "      <th>feature_bp</th>\n",
       "      <th>feature_bq</th>\n",
       "      <th>feature_br</th>\n",
       "      <th>feature_bs</th>\n",
       "      <th>feature_bt</th>\n",
       "      <th>feature_bu</th>\n",
       "      <th>feature_bv</th>\n",
       "      <th>feature_bw</th>\n",
       "      <th>feature_bx</th>\n",
       "      <th>feature_by</th>\n",
       "      <th>feature_bz</th>\n",
       "      <th>feature_ca</th>\n",
       "      <th>feature_cb</th>\n",
       "      <th>feature_cc</th>\n",
       "      <th>feature_cd</th>\n",
       "      <th>feature_ce</th>\n",
       "      <th>feature_cf</th>\n",
       "      <th>feature_cg</th>\n",
       "      <th>feature_ch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W2MW3G2L__495MGHFJ__PZ9S1Z4V__3__3647</td>\n",
       "      <td>W2MW3G2L</td>\n",
       "      <td>495MGHFJ</td>\n",
       "      <td>PZ9S1Z4V</td>\n",
       "      <td>3</td>\n",
       "      <td>3647</td>\n",
       "      <td>95</td>\n",
       "      <td>10.365266</td>\n",
       "      <td>3.209321</td>\n",
       "      <td>8.109339</td>\n",
       "      <td>9.043471</td>\n",
       "      <td>10.123041</td>\n",
       "      <td>15.722121</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.021819</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.572125</td>\n",
       "      <td>1.265875</td>\n",
       "      <td>1.341192</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>0.035243</td>\n",
       "      <td>0.833918</td>\n",
       "      <td>1.791284</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.218876</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>-50.98124</td>\n",
       "      <td>-4.854592</td>\n",
       "      <td>-8.087713</td>\n",
       "      <td>119.237254</td>\n",
       "      <td>0.040442</td>\n",
       "      <td>0.635006</td>\n",
       "      <td>0.105355</td>\n",
       "      <td>0.075415</td>\n",
       "      <td>0.03444</td>\n",
       "      <td>0.09455</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>1.986904</td>\n",
       "      <td>4.411098</td>\n",
       "      <td>3.050746</td>\n",
       "      <td>0.484755</td>\n",
       "      <td>0.020247</td>\n",
       "      <td>0.186578</td>\n",
       "      <td>0.528456</td>\n",
       "      <td>15.395411</td>\n",
       "      <td>0.219483</td>\n",
       "      <td>4.83955</td>\n",
       "      <td>2.420422</td>\n",
       "      <td>2.652015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.151196</td>\n",
       "      <td>1012.649294</td>\n",
       "      <td>425.853042</td>\n",
       "      <td>197.344987</td>\n",
       "      <td>209.253182</td>\n",
       "      <td>0.016366</td>\n",
       "      <td>0.552138</td>\n",
       "      <td>108.859861</td>\n",
       "      <td>2.369993</td>\n",
       "      <td>66589.814887</td>\n",
       "      <td>34282.221003</td>\n",
       "      <td>1316.738008</td>\n",
       "      <td>0.04801</td>\n",
       "      <td>11660.961097</td>\n",
       "      <td>0.116372</td>\n",
       "      <td>11.122246</td>\n",
       "      <td>716.158132</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>1.772256</td>\n",
       "      <td>38.452077</td>\n",
       "      <td>0.872948</td>\n",
       "      <td>0.06611</td>\n",
       "      <td>0.078856</td>\n",
       "      <td>0.030888</td>\n",
       "      <td>-0.480743</td>\n",
       "      <td>-0.197747</td>\n",
       "      <td>-3.659776</td>\n",
       "      <td>0.100295</td>\n",
       "      <td>3.131395</td>\n",
       "      <td>4.554259</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>-0.032241</td>\n",
       "      <td>-0.00083</td>\n",
       "      <td>-0.058961</td>\n",
       "      <td>-0.002774</td>\n",
       "      <td>-0.00148</td>\n",
       "      <td>-0.25646</td>\n",
       "      <td>1.665532</td>\n",
       "      <td>0.071324</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W2MW3G2L__495MGHFJ__PZ9S1Z4V__10__3647</td>\n",
       "      <td>W2MW3G2L</td>\n",
       "      <td>495MGHFJ</td>\n",
       "      <td>PZ9S1Z4V</td>\n",
       "      <td>10</td>\n",
       "      <td>3647</td>\n",
       "      <td>88</td>\n",
       "      <td>2.571477</td>\n",
       "      <td>15.234848</td>\n",
       "      <td>16.505699</td>\n",
       "      <td>0.230426</td>\n",
       "      <td>10.145378</td>\n",
       "      <td>10.159641</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.021819</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.572125</td>\n",
       "      <td>1.265875</td>\n",
       "      <td>1.341192</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>0.035243</td>\n",
       "      <td>0.833918</td>\n",
       "      <td>1.791284</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.218876</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>-50.98124</td>\n",
       "      <td>-4.854592</td>\n",
       "      <td>-8.087713</td>\n",
       "      <td>119.237254</td>\n",
       "      <td>0.040442</td>\n",
       "      <td>0.635006</td>\n",
       "      <td>0.105355</td>\n",
       "      <td>0.075415</td>\n",
       "      <td>0.03444</td>\n",
       "      <td>0.09455</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>1.986904</td>\n",
       "      <td>4.411098</td>\n",
       "      <td>3.050746</td>\n",
       "      <td>0.484755</td>\n",
       "      <td>0.052623</td>\n",
       "      <td>0.186578</td>\n",
       "      <td>0.528456</td>\n",
       "      <td>15.395411</td>\n",
       "      <td>0.219483</td>\n",
       "      <td>4.83955</td>\n",
       "      <td>2.420422</td>\n",
       "      <td>2.652015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.151196</td>\n",
       "      <td>1012.649294</td>\n",
       "      <td>425.853042</td>\n",
       "      <td>197.344987</td>\n",
       "      <td>209.253182</td>\n",
       "      <td>0.016366</td>\n",
       "      <td>0.552138</td>\n",
       "      <td>108.859861</td>\n",
       "      <td>2.369993</td>\n",
       "      <td>66589.814887</td>\n",
       "      <td>34282.221003</td>\n",
       "      <td>1316.738008</td>\n",
       "      <td>0.04801</td>\n",
       "      <td>11660.961097</td>\n",
       "      <td>0.116372</td>\n",
       "      <td>11.122246</td>\n",
       "      <td>716.158132</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>1.772256</td>\n",
       "      <td>38.452077</td>\n",
       "      <td>0.872948</td>\n",
       "      <td>0.06611</td>\n",
       "      <td>0.078856</td>\n",
       "      <td>0.030888</td>\n",
       "      <td>-0.480743</td>\n",
       "      <td>-0.197747</td>\n",
       "      <td>-3.659776</td>\n",
       "      <td>0.100295</td>\n",
       "      <td>3.131395</td>\n",
       "      <td>4.554259</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>-0.032241</td>\n",
       "      <td>-0.00083</td>\n",
       "      <td>-0.058961</td>\n",
       "      <td>-0.002774</td>\n",
       "      <td>-0.00148</td>\n",
       "      <td>-0.25646</td>\n",
       "      <td>1.665532</td>\n",
       "      <td>0.071324</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W2MW3G2L__495MGHFJ__PZ9S1Z4V__25__3647</td>\n",
       "      <td>W2MW3G2L</td>\n",
       "      <td>495MGHFJ</td>\n",
       "      <td>PZ9S1Z4V</td>\n",
       "      <td>25</td>\n",
       "      <td>3647</td>\n",
       "      <td>71</td>\n",
       "      <td>5.524709</td>\n",
       "      <td>6.931663</td>\n",
       "      <td>8.939537</td>\n",
       "      <td>0.668187</td>\n",
       "      <td>16.578701</td>\n",
       "      <td>3.150690</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.021819</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.572125</td>\n",
       "      <td>1.265875</td>\n",
       "      <td>1.341192</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>0.035243</td>\n",
       "      <td>0.833918</td>\n",
       "      <td>1.791284</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.218876</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>-50.98124</td>\n",
       "      <td>-4.854592</td>\n",
       "      <td>-8.087713</td>\n",
       "      <td>119.237254</td>\n",
       "      <td>0.040442</td>\n",
       "      <td>0.635006</td>\n",
       "      <td>0.105355</td>\n",
       "      <td>0.075415</td>\n",
       "      <td>0.03444</td>\n",
       "      <td>0.09455</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>1.986904</td>\n",
       "      <td>4.411098</td>\n",
       "      <td>3.050746</td>\n",
       "      <td>0.484755</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.186578</td>\n",
       "      <td>0.528456</td>\n",
       "      <td>15.395411</td>\n",
       "      <td>0.219483</td>\n",
       "      <td>4.83955</td>\n",
       "      <td>2.420422</td>\n",
       "      <td>2.652015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.151196</td>\n",
       "      <td>1012.649294</td>\n",
       "      <td>425.853042</td>\n",
       "      <td>197.344987</td>\n",
       "      <td>209.253182</td>\n",
       "      <td>0.016366</td>\n",
       "      <td>0.552138</td>\n",
       "      <td>108.859861</td>\n",
       "      <td>2.369993</td>\n",
       "      <td>66589.814887</td>\n",
       "      <td>34282.221003</td>\n",
       "      <td>1316.738008</td>\n",
       "      <td>0.04801</td>\n",
       "      <td>11660.961097</td>\n",
       "      <td>0.116372</td>\n",
       "      <td>11.122246</td>\n",
       "      <td>716.158132</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>1.772256</td>\n",
       "      <td>38.452077</td>\n",
       "      <td>0.872948</td>\n",
       "      <td>0.06611</td>\n",
       "      <td>0.078856</td>\n",
       "      <td>0.030888</td>\n",
       "      <td>-0.480743</td>\n",
       "      <td>-0.197747</td>\n",
       "      <td>-3.659776</td>\n",
       "      <td>0.100295</td>\n",
       "      <td>3.131395</td>\n",
       "      <td>4.554259</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>-0.032241</td>\n",
       "      <td>-0.00083</td>\n",
       "      <td>-0.058961</td>\n",
       "      <td>-0.002774</td>\n",
       "      <td>-0.00148</td>\n",
       "      <td>-0.25646</td>\n",
       "      <td>1.665532</td>\n",
       "      <td>0.071324</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id      code  sub_code sub_category  horizon  ts_index  feature_a  feature_b  feature_c  feature_d  feature_e  feature_f  feature_g  feature_h  feature_i  \\\n",
       "0   W2MW3G2L__495MGHFJ__PZ9S1Z4V__3__3647  W2MW3G2L  495MGHFJ     PZ9S1Z4V        3      3647         95  10.365266   3.209321   8.109339   9.043471  10.123041  15.722121   0.000243   0.021819   \n",
       "1  W2MW3G2L__495MGHFJ__PZ9S1Z4V__10__3647  W2MW3G2L  495MGHFJ     PZ9S1Z4V       10      3647         88   2.571477  15.234848  16.505699   0.230426  10.145378  10.159641   0.000243   0.021819   \n",
       "2  W2MW3G2L__495MGHFJ__PZ9S1Z4V__25__3647  W2MW3G2L  495MGHFJ     PZ9S1Z4V       25      3647         71   5.524709   6.931663   8.939537   0.668187  16.578701   3.150690   0.000243   0.021819   \n",
       "\n",
       "   feature_j  feature_k  feature_l  feature_m  feature_n  feature_o  feature_p  feature_q  feature_r  feature_s  feature_t  feature_u  feature_v  feature_w  feature_x  feature_y   feature_z  \\\n",
       "0    0.00142   0.000073   0.572125   1.265875   1.341192   0.005564   0.011987   0.035243   0.833918   1.791284   0.020539   0.218876    0.08066  -50.98124  -4.854592  -8.087713  119.237254   \n",
       "1    0.00142   0.000073   0.572125   1.265875   1.341192   0.005564   0.011987   0.035243   0.833918   1.791284   0.020539   0.218876    0.08066  -50.98124  -4.854592  -8.087713  119.237254   \n",
       "2    0.00142   0.000073   0.572125   1.265875   1.341192   0.005564   0.011987   0.035243   0.833918   1.791284   0.020539   0.218876    0.08066  -50.98124  -4.854592  -8.087713  119.237254   \n",
       "\n",
       "   feature_aa  feature_ab  feature_ac  feature_ad  feature_ae  feature_af  feature_ag  feature_ah  feature_ai  feature_aj  feature_ak  feature_al  feature_am  feature_an  feature_ao  feature_ap  \\\n",
       "0    0.040442    0.635006    0.105355    0.075415     0.03444     0.09455    0.006728    1.986904    4.411098    3.050746    0.484755    0.020247    0.186578    0.528456   15.395411    0.219483   \n",
       "1    0.040442    0.635006    0.105355    0.075415     0.03444     0.09455    0.006728    1.986904    4.411098    3.050746    0.484755    0.052623    0.186578    0.528456   15.395411    0.219483   \n",
       "2    0.040442    0.635006    0.105355    0.075415     0.03444     0.09455    0.006728    1.986904    4.411098    3.050746    0.484755    0.041667    0.186578    0.528456   15.395411    0.219483   \n",
       "\n",
       "   feature_aq  feature_ar  feature_as  feature_at  feature_au   feature_av  feature_aw  feature_ax  feature_ay  feature_az  feature_ba  feature_bb  feature_bc    feature_bd    feature_be  \\\n",
       "0     4.83955    2.420422    2.652015         0.0    4.151196  1012.649294  425.853042  197.344987  209.253182    0.016366    0.552138  108.859861    2.369993  66589.814887  34282.221003   \n",
       "1     4.83955    2.420422    2.652015         0.0    4.151196  1012.649294  425.853042  197.344987  209.253182    0.016366    0.552138  108.859861    2.369993  66589.814887  34282.221003   \n",
       "2     4.83955    2.420422    2.652015         0.0    4.151196  1012.649294  425.853042  197.344987  209.253182    0.016366    0.552138  108.859861    2.369993  66589.814887  34282.221003   \n",
       "\n",
       "    feature_bf  feature_bg    feature_bh  feature_bi  feature_bj  feature_bk  feature_bl  feature_bm  feature_bn  feature_bo  feature_bp  feature_bq  feature_br  feature_bs  feature_bt  feature_bu  \\\n",
       "0  1316.738008     0.04801  11660.961097    0.116372   11.122246  716.158132    0.008559    1.772256   38.452077    0.872948     0.06611    0.078856    0.030888   -0.480743   -0.197747   -3.659776   \n",
       "1  1316.738008     0.04801  11660.961097    0.116372   11.122246  716.158132    0.008559    1.772256   38.452077    0.872948     0.06611    0.078856    0.030888   -0.480743   -0.197747   -3.659776   \n",
       "2  1316.738008     0.04801  11660.961097    0.116372   11.122246  716.158132    0.008559    1.772256   38.452077    0.872948     0.06611    0.078856    0.030888   -0.480743   -0.197747   -3.659776   \n",
       "\n",
       "   feature_bv  feature_bw  feature_bx  feature_by  feature_bz  feature_ca  feature_cb  feature_cc  feature_cd  feature_ce  feature_cf  feature_cg  feature_ch  \n",
       "0    0.100295    3.131395    4.554259   -0.000832   -0.032241    -0.00083   -0.058961   -0.002774    -0.00148    -0.25646    1.665532    0.071324           2  \n",
       "1    0.100295    3.131395    4.554259   -0.000832   -0.032241    -0.00083   -0.058961   -0.002774    -0.00148    -0.25646    1.665532    0.071324           2  \n",
       "2    0.100295    3.131395    4.554259   -0.000832   -0.032241    -0.00083   -0.058961   -0.002774    -0.00148    -0.25646    1.665532    0.071324           2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 â€” Load Data & Initial Inspection (ONE CELL, Kaggle)\n",
    "# Paths (given):\n",
    "#   /kaggle/input/ts-forecasting/train.parquet\n",
    "#   /kaggle/input/ts-forecasting/test.parquet\n",
    "# Output globals:\n",
    "#   df_train, df_test, TARGET_COL, ID_COL, WEIGHT_COL, TIME_COL, CAT_COLS, FEAT_COLS, NUM_COLS\n",
    "# ============================================================\n",
    "\n",
    "import os, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "TRAIN_PATH = Path(\"/kaggle/input/ts-forecasting/train.parquet\")\n",
    "TEST_PATH  = Path(\"/kaggle/input/ts-forecasting/test.parquet\")\n",
    "\n",
    "for p in [TRAIN_PATH, TEST_PATH]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {p}\")\n",
    "\n",
    "print(\"Loading parquet...\")\n",
    "df_train = pd.read_parquet(TRAIN_PATH)\n",
    "df_test  = pd.read_parquet(TEST_PATH)\n",
    "\n",
    "print(\"\\n==================== BASIC SHAPES ====================\")\n",
    "print(\"train:\", df_train.shape)\n",
    "print(\"test :\", df_test.shape)\n",
    "\n",
    "print(\"\\n==================== COLUMNS ====================\")\n",
    "print(\"train cols:\", len(df_train.columns))\n",
    "print(\"test  cols:\", len(df_test.columns))\n",
    "\n",
    "# ---- Standard column names (from competition description)\n",
    "ID_COL     = \"id\"\n",
    "WEIGHT_COL = \"weight\"\n",
    "TIME_COL   = \"ts_index\"\n",
    "BASE_CATS  = [\"code\", \"sub_code\", \"sub_category\", \"horizon\"]\n",
    "\n",
    "# ---- Detect target column (must exist in train, not in test)\n",
    "train_only_cols = [c for c in df_train.columns if c not in df_test.columns]\n",
    "# remove any obviously non-target extras if present\n",
    "train_only_cols = [c for c in train_only_cols if c not in [ID_COL, WEIGHT_COL, TIME_COL] + BASE_CATS]\n",
    "\n",
    "# Prefer common target names if present\n",
    "preferred_names = [\"target\", \"y\", \"label\", \"value\", \"prediction_target\"]\n",
    "TARGET_COL = None\n",
    "for name in preferred_names:\n",
    "    if name in df_train.columns and name not in df_test.columns:\n",
    "        TARGET_COL = name\n",
    "        break\n",
    "\n",
    "if TARGET_COL is None:\n",
    "    # If exactly one train-only col remains -> pick it\n",
    "    if len(train_only_cols) == 1:\n",
    "        TARGET_COL = train_only_cols[0]\n",
    "    else:\n",
    "        # Fallback: pick numeric column(s) absent in test\n",
    "        cand = []\n",
    "        for c in [c for c in df_train.columns if c not in df_test.columns]:\n",
    "            if pd.api.types.is_numeric_dtype(df_train[c]):\n",
    "                cand.append(c)\n",
    "        # remove known non-target just in case\n",
    "        cand = [c for c in cand if c not in [WEIGHT_COL, TIME_COL]]\n",
    "        if len(cand) == 1:\n",
    "            TARGET_COL = cand[0]\n",
    "        elif len(cand) > 1:\n",
    "            # pick the one with highest variance (usually the true target)\n",
    "            vars_ = {c: float(np.nanvar(df_train[c].to_numpy(dtype=np.float64))) for c in cand}\n",
    "            TARGET_COL = sorted(vars_.items(), key=lambda kv: kv[1], reverse=True)[0][0]\n",
    "            print(\"\\n[WARN] Multiple numeric train-only cols found; picked by variance:\", TARGET_COL)\n",
    "        else:\n",
    "            # last resort: if any train-only cols exist, pick the first\n",
    "            if len([c for c in df_train.columns if c not in df_test.columns]) > 0:\n",
    "                TARGET_COL = [c for c in df_train.columns if c not in df_test.columns][0]\n",
    "                print(\"\\n[WARN] Could not confidently detect target; picked first train-only col:\", TARGET_COL)\n",
    "            else:\n",
    "                raise RuntimeError(\"Could not detect target column (no train-only columns).\")\n",
    "\n",
    "print(\"\\n==================== KEY COLS ====================\")\n",
    "print(\"ID_COL     :\", ID_COL, \"| exists:\", ID_COL in df_train.columns and ID_COL in df_test.columns)\n",
    "print(\"TIME_COL   :\", TIME_COL, \"| exists:\", TIME_COL in df_train.columns and TIME_COL in df_test.columns)\n",
    "print(\"WEIGHT_COL :\", WEIGHT_COL, \"| exists:\", WEIGHT_COL in df_train.columns and WEIGHT_COL in df_test.columns)\n",
    "print(\"TARGET_COL :\", TARGET_COL, \"| exists in train:\", TARGET_COL in df_train.columns, \"| exists in test:\", TARGET_COL in df_test.columns)\n",
    "\n",
    "# ---- Determine categorical columns that exist\n",
    "CAT_COLS = [c for c in BASE_CATS if c in df_train.columns]\n",
    "# Also include any object/category cols (excluding id)\n",
    "for c in df_train.columns:\n",
    "    if c == ID_COL or c == TARGET_COL:\n",
    "        continue\n",
    "    if pd.api.types.is_object_dtype(df_train[c]) or str(df_train[c].dtype).startswith(\"category\"):\n",
    "        if c not in CAT_COLS:\n",
    "            CAT_COLS.append(c)\n",
    "\n",
    "# ---- Determine feature columns (exclude id, target, weight; keep everything else)\n",
    "EXCLUDE = set([ID_COL, TARGET_COL, WEIGHT_COL])\n",
    "FEAT_COLS = [c for c in df_train.columns if c not in EXCLUDE]\n",
    "\n",
    "# ---- Determine numeric feature columns\n",
    "NUM_COLS = [c for c in FEAT_COLS if pd.api.types.is_numeric_dtype(df_train[c]) and c != WEIGHT_COL]\n",
    "\n",
    "print(\"\\n==================== QUICK CHECKS ====================\")\n",
    "# id uniqueness\n",
    "if ID_COL in df_train.columns:\n",
    "    print(\"train id unique:\", df_train[ID_COL].nunique(), \"/\", len(df_train))\n",
    "if ID_COL in df_test.columns:\n",
    "    print(\"test  id unique:\", df_test[ID_COL].nunique(), \"/\", len(df_test))\n",
    "\n",
    "# ts_index ranges\n",
    "if TIME_COL in df_train.columns and TIME_COL in df_test.columns:\n",
    "    print(\"train ts_index range:\", int(df_train[TIME_COL].min()), \"->\", int(df_train[TIME_COL].max()))\n",
    "    print(\"test  ts_index range:\", int(df_test[TIME_COL].min()),  \"->\", int(df_test[TIME_COL].max()))\n",
    "\n",
    "# horizon distribution (small peek)\n",
    "if \"horizon\" in df_train.columns:\n",
    "    print(\"\\ntrain horizon value counts (top):\")\n",
    "    print(df_train[\"horizon\"].value_counts(dropna=False).head(10))\n",
    "if \"horizon\" in df_test.columns:\n",
    "    print(\"\\ntest horizon value counts (top):\")\n",
    "    print(df_test[\"horizon\"].value_counts(dropna=False).head(10))\n",
    "\n",
    "# missingness summary (top 15 columns)\n",
    "print(\"\\n==================== MISSING VALUES (TOP) ====================\")\n",
    "miss_train = df_train.isna().mean().sort_values(ascending=False)\n",
    "miss_test  = df_test.isna().mean().sort_values(ascending=False)\n",
    "print(\"train missing rate top 15:\")\n",
    "print(miss_train.head(15))\n",
    "print(\"\\ntest missing rate top 15:\")\n",
    "print(miss_test.head(15))\n",
    "\n",
    "# target stats\n",
    "if TARGET_COL in df_train.columns and pd.api.types.is_numeric_dtype(df_train[TARGET_COL]):\n",
    "    y = df_train[TARGET_COL].to_numpy(dtype=np.float64)\n",
    "    print(\"\\n==================== TARGET STATS ====================\")\n",
    "    print(\"count:\", np.isfinite(y).sum(), \" / \", len(y))\n",
    "    print(\"mean :\", float(np.nanmean(y)))\n",
    "    print(\"std  :\", float(np.nanstd(y)))\n",
    "    print(\"min  :\", float(np.nanmin(y)))\n",
    "    print(\"p1   :\", float(np.nanpercentile(y, 1)))\n",
    "    print(\"p50  :\", float(np.nanpercentile(y, 50)))\n",
    "    print(\"p99  :\", float(np.nanpercentile(y, 99)))\n",
    "    print(\"max  :\", float(np.nanmax(y)))\n",
    "\n",
    "# weight stats (reminder: do NOT use as feature)\n",
    "if WEIGHT_COL in df_train.columns and pd.api.types.is_numeric_dtype(df_train[WEIGHT_COL]):\n",
    "    w = df_train[WEIGHT_COL].to_numpy(dtype=np.float64)\n",
    "    print(\"\\n==================== WEIGHT STATS (NOT A FEATURE) ====================\")\n",
    "    print(\"mean :\", float(np.nanmean(w)))\n",
    "    print(\"min  :\", float(np.nanmin(w)))\n",
    "    print(\"p50  :\", float(np.nanpercentile(w, 50)))\n",
    "    print(\"p99  :\", float(np.nanpercentile(w, 99)))\n",
    "    print(\"max  :\", float(np.nanmax(w)))\n",
    "\n",
    "print(\"\\n==================== FEATURE SET SUMMARY ====================\")\n",
    "print(\"CAT_COLS :\", CAT_COLS)\n",
    "print(\"NUM_COLS :\", len(NUM_COLS), \"(numeric features excluding weight/target/id)\")\n",
    "print(\"FEAT_COLS:\", len(FEAT_COLS), \"(all usable columns excluding target and weight; id excluded)\")\n",
    "\n",
    "print(\"\\n==================== HEAD (train) ====================\")\n",
    "display(df_train.head(3))\n",
    "print(\"\\n==================== HEAD (test) ====================\")\n",
    "display(df_test.head(3))\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2073e",
   "metadata": {
    "papermill": {
     "duration": 0.004733,
     "end_time": "2026-01-13T16:35:02.789843",
     "exception": false,
     "start_time": "2026-01-13T16:35:02.785110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sanity Checks & Leakage Rules Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be801ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T16:35:02.801575Z",
     "iopub.status.busy": "2026-01-13T16:35:02.801098Z",
     "iopub.status.idle": "2026-01-13T16:35:19.706218Z",
     "shell.execute_reply": "2026-01-13T16:35:19.705425Z"
    },
    "papermill": {
     "duration": 16.913544,
     "end_time": "2026-01-13T16:35:19.708099",
     "exception": false,
     "start_time": "2026-01-13T16:35:02.794555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== STAGE 2: SANITY ====================\n",
      "Has weight in train: True | in test: False\n",
      "Target col: y_target\n",
      "ID uniqueness: OK\n",
      "Train ts_index range: 1 -> 3601\n",
      "Test  ts_index range: 3602 -> 4376\n",
      "Time ordering (train -> test): OK (test starts after train).\n",
      "\n",
      "==================== FEATURE LISTS ====================\n",
      "Categorical features: ['code', 'sub_code', 'sub_category', 'horizon']\n",
      "Numeric features (excluding ts_index): 87\n",
      "Total features: 91\n",
      "\n",
      "Target NaN rate: 0.0\n",
      "\n",
      "==================== WEIGHT SANITY (TRAIN) ====================\n",
      "NaN rate: 0.0 | negative rate: 0.0 | zero rate: 0.0009332234673945098\n",
      "\n",
      "==================== LEAKAGE RULES (REMINDER) ====================\n",
      "- Do NOT use 'weight' as a feature (only as sample_weight).\n",
      "- Any preprocessing (imputer/encoder/scaler) must be fit on TRAIN-FOLD only.\n",
      "- Any time-based features (rolling/expanding) must be computed with shift(1) per group.\n",
      "- Do NOT compute statistics using future rows (ts_index > t) for predicting time t.\n",
      "- Avoid fitting encoders on train+test combined.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 â€” Sanity Checks & Leakage Rules Setup (ONE CELL, Kaggle)\n",
    "# Assumes STAGE 1 already ran and created:\n",
    "#   df_train, df_test, TARGET_COL, ID_COL, TIME_COL, CAT_COLS, FEAT_COLS\n",
    "# This stage:\n",
    "# - Validates schema and uniqueness\n",
    "# - Confirms time ordering (train < test)\n",
    "# - Sets leakage-safe column lists\n",
    "# - Defines \"DO NOT USE\" columns and lightweight guards\n",
    "# Outputs/Globals:\n",
    "#   DO_NOT_USE_COLS, FEATURE_COLS_NUM, FEATURE_COLS_CAT, FEATURE_COLS_ALL\n",
    "#   TRAIN_MAX_TS, TEST_MIN_TS, TEST_MAX_TS\n",
    "# ============================================================\n",
    "\n",
    "import gc, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require STAGE 1 globals\n",
    "# ----------------------------\n",
    "need = [\"df_train\",\"df_test\",\"TARGET_COL\",\"ID_COL\",\"TIME_COL\",\"CAT_COLS\",\"FEAT_COLS\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1 dulu.\")\n",
    "\n",
    "assert isinstance(df_train, pd.DataFrame) and isinstance(df_test, pd.DataFrame)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Core column existence\n",
    "# ----------------------------\n",
    "must_in_train = [ID_COL, TIME_COL, TARGET_COL]\n",
    "must_in_test  = [ID_COL, TIME_COL]\n",
    "for c in must_in_train:\n",
    "    if c not in df_train.columns:\n",
    "        raise RuntimeError(f\"Train missing required column: {c}\")\n",
    "for c in must_in_test:\n",
    "    if c not in df_test.columns:\n",
    "        raise RuntimeError(f\"Test missing required column: {c}\")\n",
    "\n",
    "# Optional: weight may exist only in train\n",
    "WEIGHT_COL = \"weight\"\n",
    "has_weight_train = WEIGHT_COL in df_train.columns\n",
    "has_weight_test  = WEIGHT_COL in df_test.columns\n",
    "\n",
    "print(\"==================== STAGE 2: SANITY ====================\")\n",
    "print(\"Has weight in train:\", has_weight_train, \"| in test:\", has_weight_test)\n",
    "if has_weight_test:\n",
    "    print(\"[WARN] weight also exists in test. We'll still exclude it as a feature.\")\n",
    "print(\"Target col:\", TARGET_COL)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) ID uniqueness checks\n",
    "# ----------------------------\n",
    "ntr = len(df_train)\n",
    "nts = len(df_test)\n",
    "\n",
    "nuniq_tr = df_train[ID_COL].nunique(dropna=False)\n",
    "nuniq_ts = df_test[ID_COL].nunique(dropna=False)\n",
    "\n",
    "if nuniq_tr != ntr:\n",
    "    dup = df_train[df_train[ID_COL].duplicated(keep=False)][ID_COL].head(10).tolist()\n",
    "    raise RuntimeError(f\"Train id not unique: {nuniq_tr}/{ntr}. Example dups: {dup}\")\n",
    "if nuniq_ts != nts:\n",
    "    dup = df_test[df_test[ID_COL].duplicated(keep=False)][ID_COL].head(10).tolist()\n",
    "    raise RuntimeError(f\"Test id not unique: {nuniq_ts}/{nts}. Example dups: {dup}\")\n",
    "\n",
    "intersect = np.intersect1d(df_train[ID_COL].values, df_test[ID_COL].values)\n",
    "if len(intersect) > 0:\n",
    "    print(f\"[WARN] Train/Test share {len(intersect)} ids (unexpected). Example:\", intersect[:5])\n",
    "\n",
    "print(\"ID uniqueness: OK\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Time ordering checks\n",
    "# ----------------------------\n",
    "if not pd.api.types.is_integer_dtype(df_train[TIME_COL]) and not pd.api.types.is_numeric_dtype(df_train[TIME_COL]):\n",
    "    raise RuntimeError(f\"{TIME_COL} in train is not numeric.\")\n",
    "if not pd.api.types.is_integer_dtype(df_test[TIME_COL]) and not pd.api.types.is_numeric_dtype(df_test[TIME_COL]):\n",
    "    raise RuntimeError(f\"{TIME_COL} in test is not numeric.\")\n",
    "\n",
    "TRAIN_MAX_TS = int(np.nanmax(df_train[TIME_COL].values))\n",
    "TRAIN_MIN_TS = int(np.nanmin(df_train[TIME_COL].values))\n",
    "TEST_MIN_TS  = int(np.nanmin(df_test[TIME_COL].values))\n",
    "TEST_MAX_TS  = int(np.nanmax(df_test[TIME_COL].values))\n",
    "\n",
    "print(\"Train ts_index range:\", TRAIN_MIN_TS, \"->\", TRAIN_MAX_TS)\n",
    "print(\"Test  ts_index range:\", TEST_MIN_TS,  \"->\", TEST_MAX_TS)\n",
    "\n",
    "# Expect test period after train; allow small overlaps but flag loudly\n",
    "if TEST_MIN_TS <= TRAIN_MAX_TS:\n",
    "    print(\"[WARN] Test min ts_index <= Train max ts_index. Check competition rules / possible overlap.\")\n",
    "else:\n",
    "    print(\"Time ordering (train -> test): OK (test starts after train).\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Leakage rules + feature lists\n",
    "# ----------------------------\n",
    "# DO NOT USE columns as model input features:\n",
    "# - id, target, and weight (even if present in train/test)\n",
    "DO_NOT_USE_COLS = {ID_COL, TARGET_COL, WEIGHT_COL}\n",
    "\n",
    "# Basic categorical feature columns: from STAGE 1 CAT_COLS\n",
    "FEATURE_COLS_CAT = [c for c in CAT_COLS if c not in DO_NOT_USE_COLS and c in df_train.columns]\n",
    "\n",
    "# Numeric candidate features: all numeric columns except forbidden\n",
    "numeric_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\n",
    "FEATURE_COLS_NUM = [c for c in numeric_cols if c not in DO_NOT_USE_COLS and c != TIME_COL]  # exclude ts_index by default\n",
    "\n",
    "# Full feature set used by \"tabular model\" baseline:\n",
    "FEATURE_COLS_ALL = FEATURE_COLS_CAT + FEATURE_COLS_NUM\n",
    "\n",
    "print(\"\\n==================== FEATURE LISTS ====================\")\n",
    "print(\"Categorical features:\", FEATURE_COLS_CAT)\n",
    "print(\"Numeric features (excluding ts_index):\", len(FEATURE_COLS_NUM))\n",
    "print(\"Total features:\", len(FEATURE_COLS_ALL))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Minimal integrity checks (dtypes, NaNs)\n",
    "# ----------------------------\n",
    "# Categorical columns should exist in both train and test\n",
    "missing_cats_test = [c for c in FEATURE_COLS_CAT if c not in df_test.columns]\n",
    "if missing_cats_test:\n",
    "    raise RuntimeError(f\"Categorical cols missing in test: {missing_cats_test}\")\n",
    "\n",
    "# Numeric columns should exist in both train and test for inference\n",
    "missing_num_test = [c for c in FEATURE_COLS_NUM if c not in df_test.columns]\n",
    "if missing_num_test:\n",
    "    # It's possible, but unusual; better fail fast\n",
    "    raise RuntimeError(f\"Numeric feature cols missing in test: {missing_num_test[:10]} ... ({len(missing_num_test)} total)\")\n",
    "\n",
    "# Check target has no NaN (important)\n",
    "y_nan = df_train[TARGET_COL].isna().mean()\n",
    "print(\"\\nTarget NaN rate:\", float(y_nan))\n",
    "if y_nan > 0:\n",
    "    print(\"[WARN] Target has missing values. We'll need to drop or impute target rows later (usually drop).\")\n",
    "\n",
    "# Weight sanity (if exists)\n",
    "if has_weight_train:\n",
    "    w = df_train[WEIGHT_COL].to_numpy(dtype=np.float64)\n",
    "    w_nan = np.isnan(w).mean()\n",
    "    w_neg = np.mean(w < 0)\n",
    "    w_zero = np.mean(w == 0)\n",
    "    print(\"\\n==================== WEIGHT SANITY (TRAIN) ====================\")\n",
    "    print(\"NaN rate:\", float(w_nan), \"| negative rate:\", float(w_neg), \"| zero rate:\", float(w_zero))\n",
    "    if w_neg > 0:\n",
    "        print(\"[WARN] Found negative weights. Usually unexpected; we'll handle carefully later.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Leakage-safe reminders (printed)\n",
    "# ----------------------------\n",
    "print(\"\\n==================== LEAKAGE RULES (REMINDER) ====================\")\n",
    "print(\"- Do NOT use 'weight' as a feature (only as sample_weight).\")\n",
    "print(\"- Any preprocessing (imputer/encoder/scaler) must be fit on TRAIN-FOLD only.\")\n",
    "print(\"- Any time-based features (rolling/expanding) must be computed with shift(1) per group.\")\n",
    "print(\"- Do NOT compute statistics using future rows (ts_index > t) for predicting time t.\")\n",
    "print(\"- Avoid fitting encoders on train+test combined.\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce3471d",
   "metadata": {
    "papermill": {
     "duration": 0.00494,
     "end_time": "2026-01-13T16:35:19.718116",
     "exception": false,
     "start_time": "2026-01-13T16:35:19.713176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Implement Official Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e3d273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T16:35:19.730078Z",
     "iopub.status.busy": "2026-01-13T16:35:19.729757Z",
     "iopub.status.idle": "2026-01-13T16:35:20.124850Z",
     "shell.execute_reply": "2026-01-13T16:35:20.123824Z"
    },
    "papermill": {
     "duration": 0.403894,
     "end_time": "2026-01-13T16:35:20.126930",
     "exception": false,
     "start_time": "2026-01-13T16:35:19.723036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== STAGE 3: OFFICIAL METRIC ====================\n",
      "Using weight column: True\n",
      "Baseline (predict 0)            score = 0.000000\n",
      "Baseline (predict w-mean -0.000024) score = 0.011117\n",
      "Baseline (predict median -0.000577) score = 0.000000\n",
      "\n",
      "Diagnostics:\n",
      "denom sum(w*y^2) = 4.082630e+08\n",
      "ratio(predict0)  = 1.000000  (should be ~1.0 => score~0)\n",
      "\n",
      "Globals exported: weighted_rmse_score, score_arrays, score_df\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 â€” Implement Official Metric (ONE CELL, Kaggle)\n",
    "# Assumes STAGE 1â€“2 already ran and created:\n",
    "#   df_train, df_test, TARGET_COL, ID_COL, TIME_COL, (optional) WEIGHT_COL=\"weight\"\n",
    "# This stage:\n",
    "# - Implements competition metric exactly\n",
    "# - Adds helpers to score arrays / dataframes\n",
    "# - Provides a few baselines sanity checks (zero, weighted-mean)\n",
    "# Outputs/Globals:\n",
    "#   weighted_rmse_score, score_df, score_arrays\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require minimal globals\n",
    "# ----------------------------\n",
    "need = [\"df_train\", \"TARGET_COL\", \"ID_COL\", \"TIME_COL\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1â€“2 dulu.\")\n",
    "\n",
    "WEIGHT_COL = \"weight\"\n",
    "HAS_W = WEIGHT_COL in df_train.columns\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Official metric (as provided by host)\n",
    "# ----------------------------\n",
    "def _clip01(x: float) -> float:\n",
    "    return float(np.minimum(np.maximum(x, 0.0), 1.0))\n",
    "\n",
    "def weighted_rmse_score(y_target, y_pred, w) -> float:\n",
    "    \"\"\"\n",
    "    Competition metric:\n",
    "      denom = sum(w * y^2)\n",
    "      ratio = sum(w * (y - yhat)^2) / denom\n",
    "      score = sqrt( 1 - clip01(ratio) )\n",
    "    \"\"\"\n",
    "    y_target = np.asarray(y_target, dtype=np.float64)\n",
    "    y_pred   = np.asarray(y_pred, dtype=np.float64)\n",
    "    w        = np.asarray(w, dtype=np.float64)\n",
    "\n",
    "    # Robust guards\n",
    "    if y_target.shape != y_pred.shape or y_target.shape != w.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y={y_target.shape}, yhat={y_pred.shape}, w={w.shape}\")\n",
    "\n",
    "    # If denom is 0, the metric is ill-defined; return 0 safely\n",
    "    denom = np.sum(w * (y_target ** 2))\n",
    "    if not np.isfinite(denom) or denom <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    ratio = np.sum(w * ((y_target - y_pred) ** 2)) / denom\n",
    "    clipped = _clip01(ratio)\n",
    "    val = 1.0 - clipped\n",
    "    # Numerical safety\n",
    "    val = max(val, 0.0)\n",
    "    return float(np.sqrt(val))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Convenience wrappers\n",
    "# ----------------------------\n",
    "def score_arrays(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray | None = None) -> float:\n",
    "    if w is None:\n",
    "        w = np.ones_like(y_true, dtype=np.float64)\n",
    "    return weighted_rmse_score(y_true, y_pred, w)\n",
    "\n",
    "def score_df(df: pd.DataFrame, y_col: str, pred_col: str, w_col: str = \"weight\") -> float:\n",
    "    if w_col not in df.columns:\n",
    "        w = np.ones(len(df), dtype=np.float64)\n",
    "    else:\n",
    "        w = df[w_col].to_numpy(dtype=np.float64)\n",
    "    return weighted_rmse_score(df[y_col].to_numpy(dtype=np.float64),\n",
    "                               df[pred_col].to_numpy(dtype=np.float64),\n",
    "                               w)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Sanity check on train (simple baselines)\n",
    "# ----------------------------\n",
    "print(\"==================== STAGE 3: OFFICIAL METRIC ====================\")\n",
    "y = df_train[TARGET_COL].to_numpy(dtype=np.float64)\n",
    "\n",
    "if HAS_W:\n",
    "    w = df_train[WEIGHT_COL].to_numpy(dtype=np.float64)\n",
    "else:\n",
    "    w = np.ones_like(y, dtype=np.float64)\n",
    "\n",
    "# Baseline A: predict 0\n",
    "pred0 = np.zeros_like(y, dtype=np.float64)\n",
    "s0 = weighted_rmse_score(y, pred0, w)\n",
    "\n",
    "# Baseline B: predict weighted mean (best constant under weighted MSE)\n",
    "# Use small epsilon to avoid /0\n",
    "w_sum = float(np.sum(w))\n",
    "c = float(np.sum(w * y) / (w_sum + 1e-18))\n",
    "predc = np.full_like(y, c, dtype=np.float64)\n",
    "sc = weighted_rmse_score(y, predc, w)\n",
    "\n",
    "# Baseline C: predict unweighted median (often robust)\n",
    "m = float(np.median(y))\n",
    "predm = np.full_like(y, m, dtype=np.float64)\n",
    "sm = weighted_rmse_score(y, predm, w)\n",
    "\n",
    "print(f\"Using weight column: {HAS_W}\")\n",
    "print(f\"Baseline (predict 0)            score = {s0:.6f}\")\n",
    "print(f\"Baseline (predict w-mean {c:.6f}) score = {sc:.6f}\")\n",
    "print(f\"Baseline (predict median {m:.6f}) score = {sm:.6f}\")\n",
    "\n",
    "# Some extra diagnostics about denom / ratio scaling\n",
    "denom = float(np.sum(w * (y ** 2)))\n",
    "sse0  = float(np.sum(w * ((y - pred0) ** 2)))\n",
    "ratio0 = sse0 / denom if denom > 0 else np.nan\n",
    "print(\"\\nDiagnostics:\")\n",
    "print(f\"denom sum(w*y^2) = {denom:.6e}\")\n",
    "print(f\"ratio(predict0)  = {ratio0:.6f}  (should be ~1.0 => score~0)\")\n",
    "\n",
    "print(\"\\nGlobals exported: weighted_rmse_score, score_arrays, score_df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c6f64",
   "metadata": {
    "papermill": {
     "duration": 0.0052,
     "end_time": "2026-01-13T16:35:20.137577",
     "exception": false,
     "start_time": "2026-01-13T16:35:20.132377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Time-based Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed982957",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T16:35:20.149808Z",
     "iopub.status.busy": "2026-01-13T16:35:20.149432Z",
     "iopub.status.idle": "2026-01-13T16:35:20.642281Z",
     "shell.execute_reply": "2026-01-13T16:35:20.641297Z"
    },
    "papermill": {
     "duration": 0.501559,
     "end_time": "2026-01-13T16:35:20.644240",
     "exception": false,
     "start_time": "2026-01-13T16:35:20.142681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== STAGE 4: TIME SPLITS ====================\n",
      "Train ts_index: 1 -> 3601 | span: 3601\n",
      "Tail fraction: 0.25 | tail_start_ts: 2701\n",
      "N_FOLDS: 4 | VALID_WINDOW: 200 | GAP: 0\n",
      "\n",
      "Fold boundaries:\n",
      "  fold 0: train <= 2801 | valid (2802, 3001] | window=200\n",
      "  fold 1: train <= 3001 | valid (3002, 3201] | window=200\n",
      "  fold 2: train <= 3201 | valid (3202, 3401] | window=200\n",
      "  fold 3: train <= 3401 | valid (3402, 3601] | window=200\n",
      "\n",
      "Fold row counts (fold=-1 means never validated):\n",
      "fold\n",
      "-1    3981259\n",
      " 0     343037\n",
      " 1     342025\n",
      " 2     335612\n",
      " 3     335481\n",
      "Name: count, dtype: int64\n",
      "Validation sizes: OK\n",
      "\n",
      "Globals exported: df_train['fold'], df_folds, FOLD_CFG, fold_boundaries\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 â€” Time-based Validation Split (Leakage-Safe CV) (ONE CELL, Kaggle)\n",
    "# Assumes STAGE 1â€“3 already ran and created:\n",
    "#   df_train, df_test, ID_COL, TIME_COL, TARGET_COL, CAT_COLS\n",
    "#\n",
    "# This stage:\n",
    "# - Builds walk-forward (blocked) time splits on ts_index\n",
    "# - Optionally makes splits per-horizon (recommended later), but here we create a global fold id\n",
    "# - Exports df_folds (id -> fold) and adds df_train[\"fold\"]\n",
    "#\n",
    "# Outputs/Globals:\n",
    "#   df_folds, df_train (with 'fold'), FOLD_CFG\n",
    "#   fold_boundaries (list of dicts)\n",
    "#\n",
    "# Notes:\n",
    "# - We use last portion of time as validation windows.\n",
    "# - Training for fold k uses all data with ts_index <= train_end\n",
    "#   Validation uses (train_end, valid_end] (strict future)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require\n",
    "# ----------------------------\n",
    "need = [\"df_train\", \"ID_COL\", \"TIME_COL\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1â€“3 dulu.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Config (tune-friendly)\n",
    "# ----------------------------\n",
    "# Number of folds (walk-forward windows)\n",
    "N_FOLDS = 4\n",
    "\n",
    "# Validation window size in ts_index units.\n",
    "# If None, we auto-set based on the last ~20% of time span.\n",
    "VALID_WINDOW = None  # e.g., 150, 200, 300; or None for auto\n",
    "\n",
    "# Gap between train_end and valid_start to reduce leakage via feature smoothing (usually 0 is OK)\n",
    "GAP = 0\n",
    "\n",
    "# Ensure we validate only on the tail period (mimics test)\n",
    "TAIL_FRACTION = 0.25  # last 25% of time used to place validation windows\n",
    "\n",
    "# Minimum validation samples per fold (fail-fast if too small)\n",
    "MIN_VALID_ROWS = 200_000\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Prepare timeline\n",
    "# ----------------------------\n",
    "ts = df_train[TIME_COL].to_numpy(dtype=np.int64)\n",
    "ts_min = int(ts.min())\n",
    "ts_max = int(ts.max())\n",
    "ts_unique = np.unique(ts)\n",
    "ts_unique.sort()\n",
    "\n",
    "span = ts_max - ts_min + 1\n",
    "tail_start_ts = int(ts_min + (1.0 - TAIL_FRACTION) * span)\n",
    "tail_start_ts = max(tail_start_ts, ts_min)\n",
    "\n",
    "# determine VALID_WINDOW\n",
    "if VALID_WINDOW is None:\n",
    "    tail_span = ts_max - tail_start_ts + 1\n",
    "    # split tail into N_FOLDS windows, with a bit of buffer\n",
    "    VALID_WINDOW = max(1, int(np.floor(tail_span / (N_FOLDS + 0.5))))\n",
    "VALID_WINDOW = int(VALID_WINDOW)\n",
    "\n",
    "# Build fold boundaries ending at ts_max\n",
    "# Fold k validates on (train_end, valid_end], where valid_end increases toward ts_max\n",
    "fold_boundaries = []\n",
    "valid_end = ts_max\n",
    "for k in range(N_FOLDS-1, -1, -1):\n",
    "    valid_start = valid_end - VALID_WINDOW + 1\n",
    "    # ensure validation window stays in tail\n",
    "    if valid_start < tail_start_ts:\n",
    "        valid_start = tail_start_ts\n",
    "    train_end = valid_start - 1 - GAP\n",
    "    fold_boundaries.append({\n",
    "        \"fold\": k,\n",
    "        \"train_end\": int(train_end),\n",
    "        \"valid_start\": int(valid_start),\n",
    "        \"valid_end\": int(valid_end),\n",
    "        \"gap\": int(GAP),\n",
    "        \"valid_window\": int(valid_end - valid_start + 1),\n",
    "    })\n",
    "    valid_end = train_end  # next fold ends where this train ended\n",
    "\n",
    "# sort by fold id ascending\n",
    "fold_boundaries = sorted(fold_boundaries, key=lambda d: d[\"fold\"])\n",
    "\n",
    "FOLD_CFG = {\n",
    "    \"N_FOLDS\": N_FOLDS,\n",
    "    \"VALID_WINDOW\": VALID_WINDOW,\n",
    "    \"GAP\": GAP,\n",
    "    \"TAIL_FRACTION\": TAIL_FRACTION,\n",
    "    \"MIN_VALID_ROWS\": MIN_VALID_ROWS,\n",
    "    \"TIME_COL\": TIME_COL,\n",
    "    \"ID_COL\": ID_COL,\n",
    "}\n",
    "\n",
    "print(\"==================== STAGE 4: TIME SPLITS ====================\")\n",
    "print(\"Train ts_index:\", ts_min, \"->\", ts_max, \"| span:\", span)\n",
    "print(\"Tail fraction:\", TAIL_FRACTION, \"| tail_start_ts:\", tail_start_ts)\n",
    "print(\"N_FOLDS:\", N_FOLDS, \"| VALID_WINDOW:\", VALID_WINDOW, \"| GAP:\", GAP)\n",
    "print(\"\\nFold boundaries:\")\n",
    "for b in fold_boundaries:\n",
    "    print(f\"  fold {b['fold']}: train <= {b['train_end']} | valid ({b['valid_start']}, {b['valid_end']}] | window={b['valid_window']}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Assign folds\n",
    "# ----------------------------\n",
    "# Default fold = -1 (train-only, never used for validation)\n",
    "fold_arr = np.full(len(df_train), -1, dtype=np.int16)\n",
    "\n",
    "ts_series = df_train[TIME_COL].to_numpy(dtype=np.int64)\n",
    "\n",
    "for b in fold_boundaries:\n",
    "    k = b[\"fold\"]\n",
    "    vs, ve = b[\"valid_start\"], b[\"valid_end\"]\n",
    "    mask = (ts_series >= vs) & (ts_series <= ve)\n",
    "    fold_arr[mask] = k\n",
    "\n",
    "df_train[\"fold\"] = fold_arr\n",
    "\n",
    "# df_folds mapping (id -> fold)\n",
    "df_folds = df_train[[ID_COL, \"fold\"]].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Diagnostics\n",
    "# ----------------------------\n",
    "vc = df_train[\"fold\"].value_counts(dropna=False).sort_index()\n",
    "print(\"\\nFold row counts (fold=-1 means never validated):\")\n",
    "print(vc)\n",
    "\n",
    "# Ensure each fold has enough validation rows\n",
    "ok = True\n",
    "for b in fold_boundaries:\n",
    "    k = b[\"fold\"]\n",
    "    n_valid = int((df_train[\"fold\"] == k).sum())\n",
    "    if n_valid < MIN_VALID_ROWS:\n",
    "        print(f\"[WARN] fold {k} valid rows too small: {n_valid} < {MIN_VALID_ROWS}\")\n",
    "        ok = False\n",
    "if ok:\n",
    "    print(\"Validation sizes: OK\")\n",
    "\n",
    "# Quick check: validation is strictly in the future of its training end\n",
    "viol = []\n",
    "for b in fold_boundaries:\n",
    "    if not (b[\"train_end\"] < b[\"valid_start\"]):\n",
    "        viol.append(b[\"fold\"])\n",
    "if viol:\n",
    "    raise RuntimeError(f\"Invalid split: folds where train_end >= valid_start: {viol}\")\n",
    "\n",
    "print(\"\\nGlobals exported: df_train['fold'], df_folds, FOLD_CFG, fold_boundaries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3d7c3",
   "metadata": {
    "papermill": {
     "duration": 0.005547,
     "end_time": "2026-01-13T16:35:20.656443",
     "exception": false,
     "start_time": "2026-01-13T16:35:20.650896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Preparation & Weighting Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dff9a61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T16:35:20.669483Z",
     "iopub.status.busy": "2026-01-13T16:35:20.669093Z",
     "iopub.status.idle": "2026-01-13T16:35:25.421534Z",
     "shell.execute_reply": "2026-01-13T16:35:25.420616Z"
    },
    "papermill": {
     "duration": 4.76175,
     "end_time": "2026-01-13T16:35:25.423563",
     "exception": false,
     "start_time": "2026-01-13T16:35:20.661813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== STAGE 5: FEATURE PREP & WEIGHTING ====================\n",
      "USE_TS_AS_FEATURE: False (ts_index excluded)\n",
      "Helper cols excluded: ['fold']\n",
      "Categorical cols: ['code', 'sub_code', 'sub_category', 'horizon']\n",
      "Numeric cols     : 87\n",
      "Total features   : 91\n",
      "Cat idxs for CatBoost: [0, 1, 2, 3]\n",
      "\n",
      "Top missing numeric features (train):\n",
      "feature_at    0.124719\n",
      "feature_by    0.110192\n",
      "feature_ay    0.085420\n",
      "feature_cd    0.074964\n",
      "feature_ce    0.051678\n",
      "feature_cf    0.044289\n",
      "feature_al    0.042233\n",
      "feature_aw    0.038444\n",
      "dtype: float64\n",
      "\n",
      "Top missing numeric features (test):\n",
      "feature_y     0.385765\n",
      "feature_x     0.385765\n",
      "feature_w     0.385765\n",
      "feature_z     0.385765\n",
      "feature_at    0.092342\n",
      "feature_by    0.092043\n",
      "feature_ay    0.057988\n",
      "feature_cd    0.057969\n",
      "dtype: float64\n",
      "\n",
      "Weight stats (train):\n",
      "  min: 0.0 p50: 1699.3843705131449 p99.9: 1321398915.3320074 max: 13912217783333.135\n",
      "  zero_rate: 0.0009332234673945098 neg_rate: 0.0\n",
      "\n",
      "Leakage reminder:\n",
      "- Fit imputer/encoder ONLY on train-fold (ts_index <= train_end).\n",
      "- If building rolling/expanding features: sort by ts_index and use shift(1) per group.\n",
      "- Do NOT use 'weight' as a feature; only as sample_weight.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 â€” Feature Preparation & Weighting Strategy (ONE CELL, Kaggle)\n",
    "# REVISI FULL (FIX: exclude 'fold' + any non-feature helper cols)\n",
    "#\n",
    "# Assumes STAGE 1â€“4 already ran and created:\n",
    "#   df_train, df_test, TARGET_COL, ID_COL, TIME_COL\n",
    "#   df_train[\"fold\"] from STAGE 4\n",
    "#\n",
    "# This stage:\n",
    "# - Finalize feature column lists (cat + num) with an option to include ts_index\n",
    "# - Excludes helper columns like 'fold' from features\n",
    "# - Optimize dtypes for categorical columns (category) to save RAM\n",
    "# - Defines leakage-safe helper functions:\n",
    "#     * make_sample_weight(...) -> uses official weight + optional recency weighting + optional clipping\n",
    "#     * fit_median_imputer(...) / apply_median_imputer(...) (for linear models)\n",
    "# - Prepares CatBoost cat feature indices for later stages\n",
    "#\n",
    "# Outputs/Globals:\n",
    "#   WEIGHT_COL, USE_TS_AS_FEATURE\n",
    "#   FEATURE_COLS_CAT, FEATURE_COLS_NUM, FEATURE_COLS_ALL\n",
    "#   CAT_FEATURE_IDXS\n",
    "#   make_sample_weight, fit_median_imputer, apply_median_imputer\n",
    "#   TRAIN_MAX_TS\n",
    "# ============================================================\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require\n",
    "# ----------------------------\n",
    "need = [\"df_train\", \"df_test\", \"TARGET_COL\", \"ID_COL\", \"TIME_COL\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1â€“4 dulu.\")\n",
    "\n",
    "WEIGHT_COL = \"weight\"\n",
    "if WEIGHT_COL not in df_train.columns:\n",
    "    raise RuntimeError(\"Kolom 'weight' tidak ada di train (harusnya ada di dataset ini).\")\n",
    "\n",
    "# Helper cols that must never be treated as features\n",
    "HELPER_COLS = {\"fold\"}\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Feature list finalization\n",
    "# ----------------------------\n",
    "USE_TS_AS_FEATURE = False  # ubah True untuk eksperimen drift\n",
    "\n",
    "# Base categorical columns (as per competition)\n",
    "BASE_CATS = [\"code\", \"sub_code\", \"sub_category\", \"horizon\"]\n",
    "FEATURE_COLS_CAT = [c for c in BASE_CATS if c in df_train.columns and c not in HELPER_COLS]\n",
    "\n",
    "# Numeric features: all numeric columns excluding forbidden + helpers\n",
    "DO_NOT_USE = {ID_COL, TARGET_COL, WEIGHT_COL} | HELPER_COLS\n",
    "\n",
    "numeric_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\n",
    "FEATURE_COLS_NUM = [c for c in numeric_cols if c not in DO_NOT_USE]\n",
    "\n",
    "# Remove ts_index if not used\n",
    "if not USE_TS_AS_FEATURE and TIME_COL in FEATURE_COLS_NUM:\n",
    "    FEATURE_COLS_NUM.remove(TIME_COL)\n",
    "\n",
    "# Full feature set\n",
    "FEATURE_COLS_ALL = FEATURE_COLS_CAT + FEATURE_COLS_NUM\n",
    "\n",
    "# Ensure all selected features exist in test\n",
    "missing_in_test = [c for c in FEATURE_COLS_ALL if c not in df_test.columns]\n",
    "if missing_in_test:\n",
    "    raise RuntimeError(f\"Fitur berikut hilang di test: {missing_in_test[:20]} (total {len(missing_in_test)})\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Categorical dtype optimization (RAM + CatBoost friendliness)\n",
    "# ----------------------------\n",
    "for c in FEATURE_COLS_CAT:\n",
    "    if str(df_train[c].dtype) != \"category\":\n",
    "        df_train[c] = df_train[c].astype(\"category\")\n",
    "    if str(df_test[c].dtype) != \"category\":\n",
    "        df_test[c] = df_test[c].astype(\"category\")\n",
    "\n",
    "# CatBoost expects cat feature indices in the final X column order\n",
    "CAT_FEATURE_IDXS = list(range(len(FEATURE_COLS_CAT)))  # cat cols placed first\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Weighting strategy helpers\n",
    "# ----------------------------\n",
    "TRAIN_MAX_TS = int(df_train[TIME_COL].max())\n",
    "\n",
    "def make_sample_weight(df: pd.DataFrame,\n",
    "                       use_recency: bool = True,\n",
    "                       tau: float = 600.0,\n",
    "                       clip_w_quantile: float | None = None,\n",
    "                       eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    sample_weight = weight * recency_decay(optional)\n",
    "    - clip_w_quantile: e.g. 0.999 or 0.9995, only if training unstable due to huge weights.\n",
    "    \"\"\"\n",
    "    w = df[WEIGHT_COL].to_numpy(dtype=np.float64)\n",
    "\n",
    "    if clip_w_quantile is not None:\n",
    "        q = float(np.nanquantile(w, clip_w_quantile))\n",
    "        if np.isfinite(q) and q > 0:\n",
    "            w = np.minimum(w, q)\n",
    "\n",
    "    if use_recency:\n",
    "        t = df[TIME_COL].to_numpy(dtype=np.float64)\n",
    "        rec = np.exp(-(TRAIN_MAX_TS - t) / float(tau))\n",
    "        w = w * rec\n",
    "\n",
    "    w = np.where(np.isfinite(w), w, 0.0)\n",
    "    w = np.maximum(w, 0.0)\n",
    "    if float(w.sum()) <= eps:\n",
    "        w = np.ones(len(df), dtype=np.float64)\n",
    "    return w\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Median imputer (fit per train-fold only; leakage-safe)\n",
    "# ----------------------------\n",
    "def fit_median_imputer(df_fit: pd.DataFrame, num_cols: list[str]) -> dict:\n",
    "    med = df_fit[num_cols].median(numeric_only=True)\n",
    "    return {c: float(med[c]) if c in med.index and np.isfinite(med[c]) else 0.0 for c in num_cols}\n",
    "\n",
    "def apply_median_imputer(df_apply: pd.DataFrame, medians: dict, num_cols: list[str]) -> pd.DataFrame:\n",
    "    out = df_apply.copy()\n",
    "    # fill only cols that contain NaN\n",
    "    for c in num_cols:\n",
    "        if c in out.columns and out[c].isna().any():\n",
    "            out[c] = out[c].fillna(medians.get(c, 0.0))\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Prints + quick stats\n",
    "# ----------------------------\n",
    "print(\"==================== STAGE 5: FEATURE PREP & WEIGHTING ====================\")\n",
    "print(\"USE_TS_AS_FEATURE:\", USE_TS_AS_FEATURE, f\"(ts_index {'included' if USE_TS_AS_FEATURE else 'excluded'})\")\n",
    "print(\"Helper cols excluded:\", sorted(list(HELPER_COLS)))\n",
    "print(\"Categorical cols:\", FEATURE_COLS_CAT)\n",
    "print(\"Numeric cols     :\", len(FEATURE_COLS_NUM))\n",
    "print(\"Total features   :\", len(FEATURE_COLS_ALL))\n",
    "print(\"Cat idxs for CatBoost:\", CAT_FEATURE_IDXS)\n",
    "\n",
    "# missingness overview (top 8) for selected numeric features (train vs test)\n",
    "if len(FEATURE_COLS_NUM) > 0:\n",
    "    miss_tr = df_train[FEATURE_COLS_NUM].isna().mean().sort_values(ascending=False).head(8)\n",
    "    miss_te = df_test[FEATURE_COLS_NUM].isna().mean().sort_values(ascending=False).head(8)\n",
    "    print(\"\\nTop missing numeric features (train):\")\n",
    "    print(miss_tr)\n",
    "    print(\"\\nTop missing numeric features (test):\")\n",
    "    print(miss_te)\n",
    "\n",
    "# weight sanity\n",
    "w0 = df_train[WEIGHT_COL].to_numpy(dtype=np.float64)\n",
    "print(\"\\nWeight stats (train):\")\n",
    "print(\"  min:\", float(np.nanmin(w0)),\n",
    "      \"p50:\", float(np.nanpercentile(w0, 50)),\n",
    "      \"p99.9:\", float(np.nanpercentile(w0, 99.9)),\n",
    "      \"max:\", float(np.nanmax(w0)))\n",
    "print(\"  zero_rate:\", float(np.mean(w0 == 0.0)),\n",
    "      \"neg_rate:\", float(np.mean(w0 < 0.0)))\n",
    "\n",
    "print(\"\\nLeakage reminder:\")\n",
    "print(\"- Fit imputer/encoder ONLY on train-fold (ts_index <= train_end).\")\n",
    "print(\"- If building rolling/expanding features: sort by ts_index and use shift(1) per group.\")\n",
    "print(\"- Do NOT use 'weight' as a feature; only as sample_weight.\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b8b88",
   "metadata": {
    "papermill": {
     "duration": 0.00556,
     "end_time": "2026-01-13T16:35:25.434709",
     "exception": false,
     "start_time": "2026-01-13T16:35:25.429149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training, OOF Evaluation, and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a7b4d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T16:35:25.447905Z",
     "iopub.status.busy": "2026-01-13T16:35:25.447535Z",
     "iopub.status.idle": "2026-01-13T17:32:37.481416Z",
     "shell.execute_reply": "2026-01-13T17:32:37.480631Z"
    },
    "papermill": {
     "duration": 3432.042939,
     "end_time": "2026-01-13T17:32:37.483258",
     "exception": false,
     "start_time": "2026-01-13T16:35:25.440319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== STAGE 6: CATBOOST OOF ====================\n",
      "MODE: per_horizon\n",
      "Total features: 89 | cat idx count: 3\n",
      "Cat cols: ['code', 'sub_code', 'sub_category']\n",
      "NOTE: 'horizon' excluded from features (used only for filtering).\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "FOLD 0 | train<= 2801 | valid 2802..3001 | train_rows=3,981,259 valid_rows=343,037\n",
      "0:\tlearn: 0.0005893\ttest: 0.0007221\tbest: 0.0007221 (0)\ttotal: 402ms\tremaining: 10m 2s\n",
      "200:\tlearn: 0.0005874\ttest: 0.0007221\tbest: 0.0007220 (76)\ttotal: 53.9s\tremaining: 5m 48s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.0007220254686\n",
      "bestIteration = 76\n",
      "\n",
      "Shrink model to first 77 iterations.\n",
      "  horizon=1: train=400,000 (cap from 1,039,446) | valid=89,622 | best_it=76 | score=0.016606\n",
      "0:\tlearn: 0.0009301\ttest: 0.0012907\tbest: 0.0012907 (0)\ttotal: 370ms\tremaining: 9m 14s\n",
      "200:\tlearn: 0.0009258\ttest: 0.0012903\tbest: 0.0012902 (160)\ttotal: 54.8s\tremaining: 5m 53s\n",
      "400:\tlearn: 0.0009115\ttest: 0.0012899\tbest: 0.0012898 (394)\ttotal: 1m 51s\tremaining: 5m 5s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.001289788463\n",
      "bestIteration = 411\n",
      "\n",
      "Shrink model to first 412 iterations.\n",
      "  horizon=3: train=400,000 (cap from 1,033,087) | valid=89,121 | best_it=411 | score=0.037059\n",
      "0:\tlearn: 0.0016602\ttest: 0.0023029\tbest: 0.0023029 (0)\ttotal: 354ms\tremaining: 8m 50s\n",
      "200:\tlearn: 0.0016419\ttest: 0.0023023\tbest: 0.0023022 (195)\ttotal: 54.3s\tremaining: 5m 51s\n",
      "400:\tlearn: 0.0015950\ttest: 0.0023002\tbest: 0.0023000 (379)\ttotal: 1m 52s\tremaining: 5m 8s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.002299953952\n",
      "bestIteration = 379\n",
      "\n",
      "Shrink model to first 380 iterations.\n",
      "  horizon=10: train=400,000 (cap from 997,960) | valid=86,081 | best_it=379 | score=0.050873\n",
      "0:\tlearn: 0.0023973\ttest: 0.0032660\tbest: 0.0032660 (0)\ttotal: 397ms\tremaining: 9m 55s\n",
      "200:\tlearn: 0.0023571\ttest: 0.0032649\tbest: 0.0032636 (64)\ttotal: 52.4s\tremaining: 5m 38s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.003263567647\n",
      "bestIteration = 64\n",
      "\n",
      "Shrink model to first 65 iterations.\n",
      "  horizon=25: train=400,000 (cap from 910,766) | valid=78,213 | best_it=64 | score=0.034115\n",
      "FOLD 0 aggregate OOF score: 0.039504 | n=343,037\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "FOLD 1 | train<= 3001 | valid 3002..3201 | train_rows=4,324,296 valid_rows=342,025\n",
      "0:\tlearn: 0.0005365\ttest: 0.0012369\tbest: 0.0012369 (0)\ttotal: 356ms\tremaining: 8m 54s\n",
      "200:\tlearn: 0.0005347\ttest: 0.0012369\tbest: 0.0012369 (157)\ttotal: 56.2s\tremaining: 6m 3s\n",
      "400:\tlearn: 0.0005323\ttest: 0.0012368\tbest: 0.0012367 (327)\ttotal: 1m 52s\tremaining: 5m 8s\n",
      "600:\tlearn: 0.0005278\ttest: 0.0012367\tbest: 0.0012367 (593)\ttotal: 2m 52s\tremaining: 4m 17s\n",
      "800:\tlearn: 0.0005244\ttest: 0.0012366\tbest: 0.0012366 (786)\ttotal: 3m 51s\tremaining: 3m 22s\n",
      "1000:\tlearn: 0.0005207\ttest: 0.0012364\tbest: 0.0012364 (975)\ttotal: 4m 50s\tremaining: 2m 24s\n",
      "1200:\tlearn: 0.0005173\ttest: 0.0012362\tbest: 0.0012362 (1184)\ttotal: 5m 50s\tremaining: 1m 27s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.001236212617\n",
      "bestIteration = 1184\n",
      "\n",
      "Shrink model to first 1185 iterations.\n",
      "  horizon=1: train=400,000 (cap from 1,129,068) | valid=89,691 | best_it=1184 | score=0.033245\n",
      "0:\tlearn: 0.0008827\ttest: 0.0021774\tbest: 0.0021774 (0)\ttotal: 356ms\tremaining: 8m 54s\n",
      "200:\tlearn: 0.0008792\ttest: 0.0021771\tbest: 0.0021771 (138)\ttotal: 53.3s\tremaining: 5m 44s\n",
      "400:\tlearn: 0.0008689\ttest: 0.0021762\tbest: 0.0021761 (395)\ttotal: 1m 48s\tremaining: 4m 57s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.002176103503\n",
      "bestIteration = 414\n",
      "\n",
      "Shrink model to first 415 iterations.\n",
      "  horizon=3: train=400,000 (cap from 1,122,208) | valid=89,026 | best_it=414 | score=0.033618\n",
      "0:\tlearn: 0.0015631\ttest: 0.0037275\tbest: 0.0037275 (0)\ttotal: 348ms\tremaining: 8m 41s\n",
      "200:\tlearn: 0.0015493\ttest: 0.0037250\tbest: 0.0037249 (143)\ttotal: 55.7s\tremaining: 5m 59s\n",
      "400:\tlearn: 0.0015006\ttest: 0.0037193\tbest: 0.0037187 (392)\ttotal: 1m 52s\tremaining: 5m 7s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.003718723657\n",
      "bestIteration = 392\n",
      "\n",
      "Shrink model to first 393 iterations.\n",
      "  horizon=10: train=400,000 (cap from 1,084,041) | valid=85,520 | best_it=392 | score=0.068599\n",
      "0:\tlearn: 0.0022696\ttest: 0.0048759\tbest: 0.0048759 (0)\ttotal: 353ms\tremaining: 8m 49s\n",
      "200:\tlearn: 0.0022210\ttest: 0.0048615\tbest: 0.0048615 (200)\ttotal: 52.9s\tremaining: 5m 41s\n",
      "400:\tlearn: 0.0020964\ttest: 0.0048500\tbest: 0.0048500 (400)\ttotal: 1m 48s\tremaining: 4m 58s\n",
      "600:\tlearn: 0.0020383\ttest: 0.0048492\tbest: 0.0048471 (495)\ttotal: 2m 45s\tremaining: 4m 7s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.004847099816\n",
      "bestIteration = 495\n",
      "\n",
      "Shrink model to first 496 iterations.\n",
      "  horizon=25: train=400,000 (cap from 988,979) | valid=77,788 | best_it=495 | score=0.108680\n",
      "FOLD 1 aggregate OOF score: 0.088077 | n=342,025\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "FOLD 2 | train<= 3201 | valid 3202..3401 | train_rows=4,666,321 valid_rows=335,612\n",
      "0:\tlearn: 0.0005686\ttest: 0.0010249\tbest: 0.0010249 (0)\ttotal: 353ms\tremaining: 8m 48s\n",
      "200:\tlearn: 0.0005669\ttest: 0.0010247\tbest: 0.0010247 (178)\ttotal: 52.1s\tremaining: 5m 36s\n",
      "400:\tlearn: 0.0005624\ttest: 0.0010244\tbest: 0.0010244 (391)\ttotal: 1m 48s\tremaining: 4m 57s\n",
      "600:\tlearn: 0.0005557\ttest: 0.0010242\tbest: 0.0010242 (585)\ttotal: 2m 47s\tremaining: 4m 10s\n",
      "800:\tlearn: 0.0005505\ttest: 0.0010242\tbest: 0.0010242 (773)\ttotal: 3m 45s\tremaining: 3m 17s\n",
      "1000:\tlearn: 0.0005460\ttest: 0.0010242\tbest: 0.0010242 (970)\ttotal: 4m 45s\tremaining: 2m 22s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.001024151986\n",
      "bestIteration = 1038\n",
      "\n",
      "Shrink model to first 1039 iterations.\n",
      "  horizon=1: train=400,000 (cap from 1,218,759) | valid=87,621 | best_it=1038 | score=0.038676\n",
      "0:\tlearn: 0.0009392\ttest: 0.0017261\tbest: 0.0017261 (0)\ttotal: 370ms\tremaining: 9m 13s\n",
      "200:\tlearn: 0.0009352\ttest: 0.0017251\tbest: 0.0017251 (195)\ttotal: 52.8s\tremaining: 5m 41s\n",
      "400:\tlearn: 0.0009139\ttest: 0.0017226\tbest: 0.0017226 (400)\ttotal: 1m 50s\tremaining: 5m 3s\n",
      "600:\tlearn: 0.0008973\ttest: 0.0017221\tbest: 0.0017221 (600)\ttotal: 2m 51s\tremaining: 4m 15s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.001721820552\n",
      "bestIteration = 647\n",
      "\n",
      "Shrink model to first 648 iterations.\n",
      "  horizon=3: train=400,000 (cap from 1,211,234) | valid=87,070 | best_it=647 | score=0.070437\n",
      "0:\tlearn: 0.0016622\ttest: 0.0027261\tbest: 0.0027261 (0)\ttotal: 355ms\tremaining: 8m 52s\n",
      "200:\tlearn: 0.0016468\ttest: 0.0027195\tbest: 0.0027195 (200)\ttotal: 54s\tremaining: 5m 48s\n",
      "400:\tlearn: 0.0015891\ttest: 0.0027103\tbest: 0.0027103 (400)\ttotal: 1m 49s\tremaining: 5m\n",
      "600:\tlearn: 0.0015426\ttest: 0.0027063\tbest: 0.0027063 (600)\ttotal: 2m 47s\tremaining: 4m 9s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.002706308123\n",
      "bestIteration = 615\n",
      "\n",
      "Shrink model to first 616 iterations.\n",
      "  horizon=10: train=400,000 (cap from 1,169,561) | valid=84,039 | best_it=615 | score=0.120673\n",
      "0:\tlearn: 0.0024018\ttest: 0.0041162\tbest: 0.0041162 (0)\ttotal: 349ms\tremaining: 8m 42s\n",
      "200:\tlearn: 0.0023592\ttest: 0.0041044\tbest: 0.0041043 (198)\ttotal: 52.2s\tremaining: 5m 37s\n",
      "400:\tlearn: 0.0022418\ttest: 0.0040999\tbest: 0.0040986 (382)\ttotal: 1m 45s\tremaining: 4m 49s\n",
      "600:\tlearn: 0.0021707\ttest: 0.0040972\tbest: 0.0040970 (513)\ttotal: 2m 42s\tremaining: 4m 3s\n",
      "800:\tlearn: 0.0021152\ttest: 0.0040993\tbest: 0.0040956 (667)\ttotal: 3m 39s\tremaining: 3m 11s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.004095575201\n",
      "bestIteration = 667\n",
      "\n",
      "Shrink model to first 668 iterations.\n",
      "  horizon=25: train=400,000 (cap from 1,066,767) | valid=76,882 | best_it=667 | score=0.100628\n",
      "FOLD 2 aggregate OOF score: 0.101337 | n=335,612\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "FOLD 3 | train<= 3401 | valid 3402..3601 | train_rows=5,001,933 valid_rows=335,481\n",
      "0:\tlearn: 0.0005549\ttest: 0.0013152\tbest: 0.0013152 (0)\ttotal: 330ms\tremaining: 8m 14s\n",
      "200:\tlearn: 0.0005529\ttest: 0.0013147\tbest: 0.0013147 (198)\ttotal: 53.5s\tremaining: 5m 45s\n",
      "400:\tlearn: 0.0005480\ttest: 0.0013144\tbest: 0.0013144 (394)\ttotal: 1m 47s\tremaining: 4m 54s\n",
      "600:\tlearn: 0.0005423\ttest: 0.0013140\tbest: 0.0013140 (598)\ttotal: 2m 44s\tremaining: 4m 6s\n",
      "800:\tlearn: 0.0005374\ttest: 0.0013142\tbest: 0.0013140 (663)\ttotal: 3m 42s\tremaining: 3m 14s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.001314007166\n",
      "bestIteration = 663\n",
      "\n",
      "Shrink model to first 664 iterations.\n",
      "  horizon=1: train=400,000 (cap from 1,306,380) | valid=88,273 | best_it=663 | score=0.042105\n",
      "0:\tlearn: 0.0009170\ttest: 0.0021643\tbest: 0.0021643 (0)\ttotal: 358ms\tremaining: 8m 56s\n",
      "200:\tlearn: 0.0009121\ttest: 0.0021628\tbest: 0.0021628 (198)\ttotal: 51.1s\tremaining: 5m 30s\n",
      "400:\tlearn: 0.0008896\ttest: 0.0021590\tbest: 0.0021589 (394)\ttotal: 1m 45s\tremaining: 4m 49s\n",
      "600:\tlearn: 0.0008733\ttest: 0.0021578\tbest: 0.0021578 (600)\ttotal: 2m 44s\tremaining: 4m 5s\n",
      "800:\tlearn: 0.0008639\ttest: 0.0021571\tbest: 0.0021570 (781)\ttotal: 3m 42s\tremaining: 3m 14s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.002156911597\n",
      "bestIteration = 818\n",
      "\n",
      "Shrink model to first 819 iterations.\n",
      "  horizon=3: train=400,000 (cap from 1,298,304) | valid=87,512 | best_it=818 | score=0.082760\n",
      "0:\tlearn: 0.0016063\ttest: 0.0033402\tbest: 0.0033402 (0)\ttotal: 351ms\tremaining: 8m 45s\n",
      "200:\tlearn: 0.0015892\ttest: 0.0033299\tbest: 0.0033299 (200)\ttotal: 53.3s\tremaining: 5m 44s\n",
      "400:\tlearn: 0.0015448\ttest: 0.0033118\tbest: 0.0033118 (399)\ttotal: 1m 48s\tremaining: 4m 56s\n",
      "600:\tlearn: 0.0015134\ttest: 0.0033049\tbest: 0.0033049 (599)\ttotal: 2m 43s\tremaining: 4m 4s\n",
      "800:\tlearn: 0.0014795\ttest: 0.0033024\tbest: 0.0033024 (800)\ttotal: 3m 40s\tremaining: 3m 12s\n",
      "1000:\tlearn: 0.0014572\ttest: 0.0033029\tbest: 0.0033020 (852)\ttotal: 4m 36s\tremaining: 2m 17s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.00330199195\n",
      "bestIteration = 852\n",
      "\n",
      "Shrink model to first 853 iterations.\n",
      "  horizon=10: train=400,000 (cap from 1,253,600) | valid=83,636 | best_it=852 | score=0.151982\n",
      "0:\tlearn: 0.0023881\ttest: 0.0038262\tbest: 0.0038262 (0)\ttotal: 348ms\tremaining: 8m 41s\n",
      "200:\tlearn: 0.0023460\ttest: 0.0038039\tbest: 0.0038038 (199)\ttotal: 54.6s\tremaining: 5m 52s\n",
      "400:\tlearn: 0.0022538\ttest: 0.0037789\tbest: 0.0037789 (398)\ttotal: 1m 50s\tremaining: 5m 1s\n",
      "600:\tlearn: 0.0022090\ttest: 0.0037719\tbest: 0.0037719 (598)\ttotal: 2m 45s\tremaining: 4m 7s\n",
      "800:\tlearn: 0.0021541\ttest: 0.0037684\tbest: 0.0037673 (756)\ttotal: 3m 42s\tremaining: 3m 14s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.003767250137\n",
      "bestIteration = 756\n",
      "\n",
      "Shrink model to first 757 iterations.\n",
      "  horizon=25: train=400,000 (cap from 1,143,649) | valid=76,060 | best_it=756 | score=0.176059\n",
      "FOLD 3 aggregate OOF score: 0.153881 | n=335,481\n",
      "\n",
      "========================================================================\n",
      "OOF SCORE (ALL FOLDS): 0.102244\n",
      "OOF SCORE BY FOLD: {0: 0.039503773473884854, 1: 0.08807702454266421, 2: 0.10133748917434963, 3: 0.15388109451738147}\n",
      "OOF SCORE BY HORIZON: {1: 0.03394158766736598, 3: 0.058398579166161396, 10: 0.10278442529788862, 25: 0.11428281205666856}\n",
      "Models saved under: /kaggle/working/tsf_stage6_catboost_models\n",
      "Elapsed (sec): 3429\n",
      "Saved report: /kaggle/working/tsf_stage6_catboost_models/stage6_oof_report.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 6 â€” Model Training, OOF Evaluation, and Model Selection (ONE CELL, Kaggle)\n",
    "# REVISI FULL v3 (FIX: CatBoost bootstrap_type Bayesian vs subsample conflict)\n",
    "# - Uses bootstrap_type=\"Bernoulli\" so subsample is allowed\n",
    "# - PER-HORIZON mode default; 'horizon' excluded from features (used only for filtering)\n",
    "# ============================================================\n",
    "\n",
    "import gc, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require\n",
    "# ----------------------------\n",
    "need = [\"df_train\",\"TARGET_COL\",\"TIME_COL\",\"ID_COL\",\"FEATURE_COLS_NUM\",\"fold_boundaries\",\"weighted_rmse_score\",\"make_sample_weight\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1â€“5 dulu.\")\n",
    "\n",
    "WEIGHT_COL = \"weight\"\n",
    "if WEIGHT_COL not in df_train.columns:\n",
    "    raise RuntimeError(\"Kolom 'weight' tidak ada di df_train.\")\n",
    "if \"fold\" not in df_train.columns:\n",
    "    raise RuntimeError(\"Kolom 'fold' belum ada. Jalankan STAGE 4 dulu.\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"CatBoost import gagal: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Mode + Feature list\n",
    "# ----------------------------\n",
    "MODE = \"per_horizon\"  # \"per_horizon\" (recommended) or \"single_model\"\n",
    "\n",
    "HELPER_COLS = {\"fold\"}\n",
    "DROP_ALWAYS = {ID_COL, TARGET_COL, WEIGHT_COL} | HELPER_COLS\n",
    "\n",
    "if MODE == \"per_horizon\":\n",
    "    CAT_COLS = [\"code\", \"sub_code\", \"sub_category\"]\n",
    "    DROP_ALWAYS = DROP_ALWAYS | {\"horizon\"}  # horizon hanya untuk filter\n",
    "else:\n",
    "    CAT_COLS = [\"code\", \"sub_code\", \"sub_category\", \"horizon\"]\n",
    "\n",
    "CAT_COLS = [c for c in CAT_COLS if c in df_train.columns]\n",
    "cat_set = set(CAT_COLS)\n",
    "\n",
    "# Numeric cols from Stage 5, excluding forbidden/helper/cat\n",
    "NUM_COLS = [c for c in FEATURE_COLS_NUM if c not in DROP_ALWAYS and c not in cat_set]\n",
    "\n",
    "FEATURE_COLS_ALL = CAT_COLS + NUM_COLS\n",
    "# dedup\n",
    "seen = set()\n",
    "FEATURE_COLS_ALL = [c for c in FEATURE_COLS_ALL if not (c in seen or seen.add(c))]\n",
    "\n",
    "missing = [c for c in FEATURE_COLS_ALL if c not in df_train.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing features in train: {missing[:20]} (total {len(missing)})\")\n",
    "\n",
    "def get_cat_feature_indices(df: pd.DataFrame, cols: list[str]) -> list[int]:\n",
    "    idxs = []\n",
    "    for i, c in enumerate(cols):\n",
    "        dt = df[c].dtype\n",
    "        if str(dt) == \"category\" or dt == object:\n",
    "            idxs.append(i)\n",
    "    return idxs\n",
    "\n",
    "CAT_FEATURE_IDXS = get_cat_feature_indices(df_train, FEATURE_COLS_ALL)\n",
    "\n",
    "print(\"==================== STAGE 6: CATBOOST OOF ====================\")\n",
    "print(\"MODE:\", MODE)\n",
    "print(\"Total features:\", len(FEATURE_COLS_ALL), \"| cat idx count:\", len(CAT_FEATURE_IDXS))\n",
    "print(\"Cat cols:\", CAT_COLS)\n",
    "if MODE == \"per_horizon\":\n",
    "    print(\"NOTE: 'horizon' excluded from features (used only for filtering).\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Config (CPU-safe)\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "\n",
    "# Weight strategy for TRAIN\n",
    "USE_RECENCY = True\n",
    "TAU = 600.0\n",
    "CLIP_W_Q = None  # kalau instabil, coba 0.999 atau 0.9995\n",
    "\n",
    "# Runtime control\n",
    "TRAIN_SAMPLE_CAP = 400_000   # per (fold,horizon)\n",
    "SAMPLE_WEIGHTED = True\n",
    "\n",
    "# CatBoost params (FIX: Bernoulli supports subsample)\n",
    "CB_PARAMS = dict(\n",
    "    loss_function=\"RMSE\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    iterations=1500,\n",
    "    learning_rate=0.06,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=6.0,\n",
    "    random_strength=1.0,\n",
    "    rsm=0.9,\n",
    "    min_data_in_leaf=300,\n",
    "\n",
    "    bootstrap_type=\"Bernoulli\",  # FIX\n",
    "    subsample=0.8,               # allowed with Bernoulli\n",
    "\n",
    "    task_type=\"CPU\",\n",
    "    thread_count=-1,\n",
    "    random_seed=SEED,\n",
    "    allow_writing_files=False,\n",
    ")\n",
    "EARLY_STOPPING_ROUNDS = 150\n",
    "\n",
    "OUT_DIR = Path(\"/kaggle/working/tsf_stage6_catboost_models\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CB_CFG_USED = dict(\n",
    "    MODE=MODE, SEED=SEED,\n",
    "    USE_RECENCY=USE_RECENCY, TAU=TAU, CLIP_W_Q=CLIP_W_Q,\n",
    "    TRAIN_SAMPLE_CAP=TRAIN_SAMPLE_CAP, SAMPLE_WEIGHTED=SAMPLE_WEIGHTED,\n",
    "    CB_PARAMS=CB_PARAMS, EARLY_STOPPING_ROUNDS=EARLY_STOPPING_ROUNDS,\n",
    "    FEATURE_COLS_ALL=FEATURE_COLS_ALL, CAT_FEATURE_IDXS=CAT_FEATURE_IDXS\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Arrays\n",
    "# ----------------------------\n",
    "n = len(df_train)\n",
    "oof_pred = np.full(n, np.nan, dtype=np.float32)\n",
    "\n",
    "y_all    = df_train[TARGET_COL].to_numpy(dtype=np.float64)\n",
    "w_off    = df_train[WEIGHT_COL].to_numpy(dtype=np.float64)   # official weight for scoring\n",
    "ts_all   = df_train[TIME_COL].to_numpy(dtype=np.int64)\n",
    "fold_all = df_train[\"fold\"].to_numpy(dtype=np.int16)\n",
    "\n",
    "if \"horizon\" in df_train.columns:\n",
    "    h_all = df_train[\"horizon\"].astype(int).to_numpy(dtype=np.int16)\n",
    "    h_vals = np.unique(h_all); h_vals.sort()\n",
    "else:\n",
    "    h_all = None\n",
    "    h_vals = np.array([0], dtype=np.int16)\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Helpers\n",
    "# ----------------------------\n",
    "def sample_cap_indices(idx: np.ndarray, w_aligned: np.ndarray, cap: int | None) -> np.ndarray:\n",
    "    if cap is None or len(idx) <= cap:\n",
    "        return idx\n",
    "    if not SAMPLE_WEIGHTED:\n",
    "        return rng.choice(idx, size=cap, replace=False)\n",
    "    ww = np.asarray(w_aligned, dtype=np.float64)\n",
    "    ww = np.where(np.isfinite(ww), ww, 0.0)\n",
    "    ww = np.maximum(ww, 0.0)\n",
    "    s = ww.sum()\n",
    "    if s <= 0:\n",
    "        return rng.choice(idx, size=cap, replace=False)\n",
    "    p = ww / s\n",
    "    return rng.choice(idx, size=cap, replace=False, p=p)\n",
    "\n",
    "def fold_train_valid_indices(b: dict) -> tuple[np.ndarray, np.ndarray]:\n",
    "    k = int(b[\"fold\"])\n",
    "    train_end = int(b[\"train_end\"])\n",
    "    vs, ve = int(b[\"valid_start\"]), int(b[\"valid_end\"])\n",
    "    tr_idx = np.where(ts_all <= train_end)[0]\n",
    "    va_idx = np.where((ts_all >= vs) & (ts_all <= ve))[0]\n",
    "    # sanity compare with fold markers\n",
    "    va2 = np.where(fold_all == k)[0]\n",
    "    if abs(len(va_idx) - len(va2)) > 1000:\n",
    "        print(f\"[WARN] Fold {k}: time-valid {len(va_idx)} vs fold-mark {len(va2)}\")\n",
    "    return tr_idx, va_idx\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Train OOF\n",
    "# ----------------------------\n",
    "models_cb = {}\n",
    "oof_score_by_fold = {}\n",
    "oof_score_by_horizon = {}\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for b in fold_boundaries:\n",
    "    k = int(b[\"fold\"])\n",
    "    tr_idx_all, va_idx_all = fold_train_valid_indices(b)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*72)\n",
    "    print(f\"FOLD {k} | train<= {b['train_end']} | valid {b['valid_start']}..{b['valid_end']} | \"\n",
    "          f\"train_rows={len(tr_idx_all):,} valid_rows={len(va_idx_all):,}\")\n",
    "\n",
    "    if MODE == \"single_model\":\n",
    "        tr_idx = tr_idx_all\n",
    "        va_idx = va_idx_all\n",
    "\n",
    "        w_tr_full = make_sample_weight(df_train.iloc[tr_idx], use_recency=USE_RECENCY, tau=TAU, clip_w_quantile=CLIP_W_Q)\n",
    "        tr_idx_cap = sample_cap_indices(tr_idx, w_tr_full, TRAIN_SAMPLE_CAP)\n",
    "        w_tr = make_sample_weight(df_train.iloc[tr_idx_cap], use_recency=USE_RECENCY, tau=TAU, clip_w_quantile=CLIP_W_Q)\n",
    "\n",
    "        X_tr = df_train.iloc[tr_idx_cap][FEATURE_COLS_ALL]\n",
    "        y_tr = y_all[tr_idx_cap]\n",
    "        X_va = df_train.iloc[va_idx][FEATURE_COLS_ALL]\n",
    "        y_va = y_all[va_idx]\n",
    "        w_va = w_off[va_idx].astype(np.float64)\n",
    "\n",
    "        train_pool = Pool(X_tr, label=y_tr, weight=w_tr, cat_features=CAT_FEATURE_IDXS)\n",
    "        valid_pool = Pool(X_va, label=y_va, weight=w_va, cat_features=CAT_FEATURE_IDXS)\n",
    "\n",
    "        model = CatBoostRegressor(**CB_PARAMS)\n",
    "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True,\n",
    "                  verbose=200, early_stopping_rounds=EARLY_STOPPING_ROUNDS)\n",
    "\n",
    "        pred_va = model.predict(valid_pool).astype(np.float32)\n",
    "        oof_pred[va_idx] = pred_va\n",
    "\n",
    "        best_it = int(model.get_best_iteration() if model.get_best_iteration() is not None else CB_PARAMS[\"iterations\"])\n",
    "        model_path = OUT_DIR / f\"cb_single_fold{k}_best{best_it}.cbm\"\n",
    "        model.save_model(str(model_path))\n",
    "        models_cb[(\"single\", k)] = str(model_path)\n",
    "\n",
    "        s_fold = weighted_rmse_score(y_va, pred_va.astype(np.float64), w_off[va_idx].astype(np.float64))\n",
    "        oof_score_by_fold[k] = float(s_fold)\n",
    "        print(f\"FOLD {k} single-model score: {s_fold:.6f}\")\n",
    "\n",
    "        del model, train_pool, valid_pool, X_tr, X_va\n",
    "        gc.collect()\n",
    "\n",
    "    else:\n",
    "        for h in h_vals:\n",
    "            h = int(h)\n",
    "            tr_idx = tr_idx_all[h_all[tr_idx_all] == h]\n",
    "            va_idx = va_idx_all[h_all[va_idx_all] == h]\n",
    "            if len(tr_idx) == 0 or len(va_idx) == 0:\n",
    "                print(f\"  horizon={h}: skipped (train={len(tr_idx)}, valid={len(va_idx)})\")\n",
    "                continue\n",
    "\n",
    "            w_tr_full = make_sample_weight(df_train.iloc[tr_idx], use_recency=USE_RECENCY, tau=TAU, clip_w_quantile=CLIP_W_Q)\n",
    "            tr_idx_cap = sample_cap_indices(tr_idx, w_tr_full, TRAIN_SAMPLE_CAP)\n",
    "            w_tr = make_sample_weight(df_train.iloc[tr_idx_cap], use_recency=USE_RECENCY, tau=TAU, clip_w_quantile=CLIP_W_Q)\n",
    "\n",
    "            X_tr = df_train.iloc[tr_idx_cap][FEATURE_COLS_ALL]\n",
    "            y_tr = y_all[tr_idx_cap]\n",
    "\n",
    "            X_va = df_train.iloc[va_idx][FEATURE_COLS_ALL]\n",
    "            y_va = y_all[va_idx]\n",
    "            w_va = w_off[va_idx].astype(np.float64)\n",
    "\n",
    "            train_pool = Pool(X_tr, label=y_tr, weight=w_tr, cat_features=CAT_FEATURE_IDXS)\n",
    "            valid_pool = Pool(X_va, label=y_va, weight=w_va, cat_features=CAT_FEATURE_IDXS)\n",
    "\n",
    "            model = CatBoostRegressor(**CB_PARAMS)\n",
    "            model.fit(train_pool, eval_set=valid_pool, use_best_model=True,\n",
    "                      verbose=200, early_stopping_rounds=EARLY_STOPPING_ROUNDS)\n",
    "\n",
    "            pred_va = model.predict(valid_pool).astype(np.float32)\n",
    "            oof_pred[va_idx] = pred_va\n",
    "\n",
    "            best_it = int(model.get_best_iteration() if model.get_best_iteration() is not None else CB_PARAMS[\"iterations\"])\n",
    "            model_path = OUT_DIR / f\"cb_h{h}_fold{k}_best{best_it}.cbm\"\n",
    "            model.save_model(str(model_path))\n",
    "            models_cb[(h, k)] = str(model_path)\n",
    "\n",
    "            s = weighted_rmse_score(y_va, pred_va.astype(np.float64), w_off[va_idx].astype(np.float64))\n",
    "            print(f\"  horizon={h}: train={len(tr_idx_cap):,} (cap from {len(tr_idx):,}) | valid={len(va_idx):,} | \"\n",
    "                  f\"best_it={best_it} | score={s:.6f}\")\n",
    "\n",
    "            del model, train_pool, valid_pool, X_tr, X_va\n",
    "            gc.collect()\n",
    "\n",
    "        idx_fold = np.where(fold_all == k)[0]\n",
    "        pf = oof_pred[idx_fold].astype(np.float64)\n",
    "        yf = y_all[idx_fold]\n",
    "        wf = w_off[idx_fold]\n",
    "        m = np.isfinite(pf)\n",
    "        s_fold = weighted_rmse_score(yf[m], pf[m], wf[m])\n",
    "        oof_score_by_fold[k] = float(s_fold)\n",
    "        print(f\"FOLD {k} aggregate OOF score: {s_fold:.6f} | n={m.sum():,}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Aggregate OOF + per-horizon\n",
    "# ----------------------------\n",
    "valid_idx_all = np.where(fold_all >= 0)[0]\n",
    "p_all = oof_pred[valid_idx_all].astype(np.float64)\n",
    "y_v   = y_all[valid_idx_all]\n",
    "w_v   = w_off[valid_idx_all]\n",
    "m = np.isfinite(p_all)\n",
    "oof_score_all = float(weighted_rmse_score(y_v[m], p_all[m], w_v[m]))\n",
    "\n",
    "if MODE == \"per_horizon\":\n",
    "    for h in h_vals:\n",
    "        h = int(h)\n",
    "        idx_h = valid_idx_all[h_all[valid_idx_all] == h]\n",
    "        if len(idx_h) == 0:\n",
    "            continue\n",
    "        ph = oof_pred[idx_h].astype(np.float64)\n",
    "        yh = y_all[idx_h]\n",
    "        wh = w_off[idx_h]\n",
    "        mm = np.isfinite(ph)\n",
    "        oof_score_by_horizon[h] = float(weighted_rmse_score(yh[mm], ph[mm], wh[mm]))\n",
    "\n",
    "elapsed = int(time.time() - t0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*72)\n",
    "print(\"OOF SCORE (ALL FOLDS):\", f\"{oof_score_all:.6f}\")\n",
    "print(\"OOF SCORE BY FOLD:\", oof_score_by_fold)\n",
    "if MODE == \"per_horizon\":\n",
    "    print(\"OOF SCORE BY HORIZON:\", oof_score_by_horizon)\n",
    "print(\"Models saved under:\", str(OUT_DIR))\n",
    "print(\"Elapsed (sec):\", elapsed)\n",
    "\n",
    "# Save report\n",
    "report_path = OUT_DIR / \"stage6_oof_report.json\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"oof_score_all\": oof_score_all,\n",
    "            \"oof_score_by_fold\": oof_score_by_fold,\n",
    "            \"oof_score_by_horizon\": oof_score_by_horizon,\n",
    "            \"fold_boundaries\": fold_boundaries,\n",
    "            \"cfg\": CB_CFG_USED,\n",
    "            \"models\": {str(k): v for k, v in models_cb.items()},\n",
    "        },\n",
    "        f,\n",
    "        indent=2\n",
    "    )\n",
    "print(\"Saved report:\", str(report_path))\n",
    "\n",
    "# Export globals\n",
    "globals()[\"oof_pred\"] = oof_pred\n",
    "globals()[\"oof_score_all\"] = oof_score_all\n",
    "globals()[\"oof_score_by_fold\"] = oof_score_by_fold\n",
    "globals()[\"oof_score_by_horizon\"] = oof_score_by_horizon\n",
    "globals()[\"models_cb\"] = models_cb\n",
    "globals()[\"CB_CFG_USED\"] = CB_CFG_USED\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26dbef0",
   "metadata": {
    "papermill": {
     "duration": 0.00981,
     "end_time": "2026-01-13T17:32:37.502842",
     "exception": false,
     "start_time": "2026-01-13T17:32:37.493032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Fit, Test Inference, and Submission Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f4e3c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T17:32:37.525055Z",
     "iopub.status.busy": "2026-01-13T17:32:37.524694Z",
     "iopub.status.idle": "2026-01-13T19:00:13.446028Z",
     "shell.execute_reply": "2026-01-13T19:00:13.445050Z"
    },
    "papermill": {
     "duration": 5255.935079,
     "end_time": "2026-01-13T19:00:13.448015",
     "exception": false,
     "start_time": "2026-01-13T17:32:37.512936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== STAGE 7: FINAL FIT + TEST INFER + SUBMISSION ====================\n",
      "MODE: per_horizon\n",
      "USE_RECENCY: True | TAU: 600.0 | CLIP_W_Q: None\n",
      "Features: 89 | Cat idx: 3\n",
      "0:\tlearn: 0.0010246\ttotal: 1.14s\tremaining: 28m 32s\n",
      "200:\tlearn: 0.0010236\ttotal: 2m 46s\tremaining: 17m 54s\n",
      "400:\tlearn: 0.0010208\ttotal: 5m 32s\tremaining: 15m 12s\n",
      "600:\tlearn: 0.0010184\ttotal: 8m 37s\tremaining: 12m 54s\n",
      "800:\tlearn: 0.0010165\ttotal: 11m 41s\tremaining: 10m 12s\n",
      "1000:\tlearn: 0.0010145\ttotal: 14m 47s\tremaining: 7m 22s\n",
      "1200:\tlearn: 0.0010129\ttotal: 17m 54s\tremaining: 4m 27s\n",
      "1400:\tlearn: 0.0010114\ttotal: 20m 58s\tremaining: 1m 28s\n",
      "1499:\tlearn: 0.0010108\ttotal: 22m 29s\tremaining: 0us\n",
      "horizon=1: train=1,394,653 test=379,617 -> saved cb_final_h1.cbm\n",
      "0:\tlearn: 0.0017059\ttotal: 1s\tremaining: 24m 59s\n",
      "200:\tlearn: 0.0017030\ttotal: 2m 37s\tremaining: 16m 57s\n",
      "400:\tlearn: 0.0016956\ttotal: 5m 26s\tremaining: 14m 53s\n",
      "600:\tlearn: 0.0016901\ttotal: 8m 30s\tremaining: 12m 44s\n",
      "800:\tlearn: 0.0016841\ttotal: 11m 37s\tremaining: 10m 8s\n",
      "1000:\tlearn: 0.0016796\ttotal: 14m 42s\tremaining: 7m 19s\n",
      "1200:\tlearn: 0.0016749\ttotal: 17m 51s\tremaining: 4m 26s\n",
      "1400:\tlearn: 0.0016715\ttotal: 21m 2s\tremaining: 1m 29s\n",
      "1499:\tlearn: 0.0016702\ttotal: 22m 35s\tremaining: 0us\n",
      "horizon=3: train=1,385,816 test=376,558 -> saved cb_final_h3.cbm\n",
      "0:\tlearn: 0.0028575\ttotal: 881ms\tremaining: 21m 59s\n",
      "200:\tlearn: 0.0028449\ttotal: 2m 43s\tremaining: 17m 36s\n",
      "400:\tlearn: 0.0028330\ttotal: 5m 16s\tremaining: 14m 27s\n",
      "600:\tlearn: 0.0028227\ttotal: 8m 9s\tremaining: 12m 12s\n",
      "800:\tlearn: 0.0028137\ttotal: 10m 55s\tremaining: 9m 32s\n",
      "1000:\tlearn: 0.0028079\ttotal: 13m 51s\tremaining: 6m 54s\n",
      "1200:\tlearn: 0.0027998\ttotal: 16m 49s\tremaining: 4m 11s\n",
      "1400:\tlearn: 0.0027918\ttotal: 19m 49s\tremaining: 1m 24s\n",
      "1499:\tlearn: 0.0027869\ttotal: 21m 18s\tremaining: 0us\n",
      "horizon=10: train=1,337,236 test=362,057 -> saved cb_final_h10.cbm\n",
      "0:\tlearn: 0.0038141\ttotal: 813ms\tremaining: 20m 18s\n",
      "200:\tlearn: 0.0037873\ttotal: 2m 24s\tremaining: 15m 32s\n",
      "400:\tlearn: 0.0037602\ttotal: 4m 46s\tremaining: 13m 4s\n",
      "600:\tlearn: 0.0037518\ttotal: 7m 32s\tremaining: 11m 17s\n",
      "800:\tlearn: 0.0037443\ttotal: 10m 16s\tremaining: 8m 58s\n",
      "1000:\tlearn: 0.0037317\ttotal: 13m 3s\tremaining: 6m 30s\n",
      "1200:\tlearn: 0.0037226\ttotal: 15m 51s\tremaining: 3m 56s\n",
      "1400:\tlearn: 0.0037016\ttotal: 18m 37s\tremaining: 1m 18s\n",
      "1499:\tlearn: 0.0036934\ttotal: 19m 57s\tremaining: 0us\n",
      "horizon=25: train=1,219,709 test=328,875 -> saved cb_final_h25.cbm\n",
      "Final fit + inference done. Elapsed (sec): 5250\n",
      "Saved: /kaggle/working/submission.csv | shape: (1447107, 2)\n",
      "                                       id  prediction\n",
      "0   W2MW3G2L__495MGHFJ__PZ9S1Z4V__3__3647   -0.031110\n",
      "1  W2MW3G2L__495MGHFJ__PZ9S1Z4V__10__3647   -0.100017\n",
      "2  W2MW3G2L__495MGHFJ__PZ9S1Z4V__25__3647   -0.175151\n",
      "3   W2MW3G2L__495MGHFJ__PZ9S1Z4V__1__3647   -0.004744\n",
      "4  W2MW3G2L__495MGHFJ__PZ9S1Z4V__10__3648   -0.100052\n",
      "Saved bundle: /kaggle/working/tsf_stage7_bundle/bundle.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 7 â€” Final Fit, Test Inference, and Submission Packaging (ONE CELL, Kaggle)\n",
    "# Baseline: CatBoostRegressor final training per-horizon on FULL TRAIN (all ts_index <= 3601)\n",
    "# Then predict TEST and write /kaggle/working/submission.csv\n",
    "#\n",
    "# Requires globals from STAGE 1â€“6:\n",
    "#   df_train, df_test, TARGET_COL, ID_COL, TIME_COL, WEIGHT_COL=\"weight\"\n",
    "#   FEATURE_COLS_ALL, CAT_FEATURE_IDXS   (from STAGE 6 cell)\n",
    "#   make_sample_weight                  (from STAGE 5)\n",
    "#   CB_CFG_USED                         (from STAGE 6)  (for params)\n",
    "#\n",
    "# Output:\n",
    "#   /kaggle/working/submission.csv\n",
    "#   /kaggle/working/tsf_stage7_bundle/final_models/*.cbm\n",
    "#   /kaggle/working/tsf_stage7_bundle/bundle.json\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Require\n",
    "# ----------------------------\n",
    "need = [\"df_train\",\"df_test\",\"ID_COL\",\"TIME_COL\",\"TARGET_COL\",\"make_sample_weight\"]\n",
    "for k in need:\n",
    "    if k not in globals():\n",
    "        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1â€“6 dulu.\")\n",
    "\n",
    "WEIGHT_COL = \"weight\"\n",
    "if WEIGHT_COL not in df_train.columns:\n",
    "    raise RuntimeError(\"Kolom 'weight' tidak ada di df_train.\")\n",
    "\n",
    "# Feature globals from Stage 6 (or fallback from Stage 5)\n",
    "if \"FEATURE_COLS_ALL\" not in globals() or \"CAT_FEATURE_IDXS\" not in globals():\n",
    "    raise RuntimeError(\"Missing FEATURE_COLS_ALL / CAT_FEATURE_IDXS. Jalankan STAGE 6 (revisi) dulu.\")\n",
    "\n",
    "# CatBoost import\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"CatBoost import gagal: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load training config from Stage 6\n",
    "# ----------------------------\n",
    "if \"CB_CFG_USED\" in globals() and isinstance(CB_CFG_USED, dict) and \"CB_PARAMS\" in CB_CFG_USED:\n",
    "    CB_PARAMS = dict(CB_CFG_USED[\"CB_PARAMS\"])\n",
    "    MODE = CB_CFG_USED.get(\"MODE\", \"per_horizon\")\n",
    "    USE_RECENCY = bool(CB_CFG_USED.get(\"USE_RECENCY\", True))\n",
    "    TAU = float(CB_CFG_USED.get(\"TAU\", 600.0))\n",
    "    CLIP_W_Q = CB_CFG_USED.get(\"CLIP_W_Q\", None)\n",
    "else:\n",
    "    # fallback defaults\n",
    "    MODE = \"per_horizon\"\n",
    "    USE_RECENCY = True\n",
    "    TAU = 600.0\n",
    "    CLIP_W_Q = None\n",
    "    CB_PARAMS = dict(\n",
    "        loss_function=\"RMSE\",\n",
    "        eval_metric=\"RMSE\",\n",
    "        iterations=1500,\n",
    "        learning_rate=0.06,\n",
    "        depth=8,\n",
    "        l2_leaf_reg=6.0,\n",
    "        random_strength=1.0,\n",
    "        rsm=0.9,\n",
    "        min_data_in_leaf=300,\n",
    "        bootstrap_type=\"Bernoulli\",\n",
    "        subsample=0.8,\n",
    "        task_type=\"CPU\",\n",
    "        thread_count=-1,\n",
    "        random_seed=42,\n",
    "        allow_writing_files=False,\n",
    "    )\n",
    "\n",
    "print(\"==================== STAGE 7: FINAL FIT + TEST INFER + SUBMISSION ====================\")\n",
    "print(\"MODE:\", MODE)\n",
    "print(\"USE_RECENCY:\", USE_RECENCY, \"| TAU:\", TAU, \"| CLIP_W_Q:\", CLIP_W_Q)\n",
    "print(\"Features:\", len(FEATURE_COLS_ALL), \"| Cat idx:\", len(CAT_FEATURE_IDXS))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Bundle dirs\n",
    "# ----------------------------\n",
    "BUNDLE_DIR = Path(\"/kaggle/working/tsf_stage7_bundle\")\n",
    "MODEL_DIR = BUNDLE_DIR / \"final_models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Prepare horizon handling\n",
    "# ----------------------------\n",
    "if \"horizon\" not in df_train.columns or \"horizon\" not in df_test.columns:\n",
    "    raise RuntimeError(\"Kolom 'horizon' wajib ada di train & test.\")\n",
    "\n",
    "h_train = df_train[\"horizon\"].astype(int).to_numpy()\n",
    "h_test  = df_test[\"horizon\"].astype(int).to_numpy()\n",
    "\n",
    "h_vals = np.unique(h_train)\n",
    "h_vals.sort()\n",
    "\n",
    "# In per_horizon mode, make sure horizon is NOT a feature (it was excluded in Stage 6)\n",
    "if MODE == \"per_horizon\" and \"horizon\" in FEATURE_COLS_ALL:\n",
    "    raise RuntimeError(\"MODE per_horizon tapi 'horizon' masih ada di FEATURE_COLS_ALL. Jalankan ulang STAGE 6 revisi.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Train final model(s) and predict test\n",
    "# ----------------------------\n",
    "t0 = time.time()\n",
    "\n",
    "# Prediction array for test\n",
    "test_pred = np.zeros(len(df_test), dtype=np.float32)\n",
    "\n",
    "# Training data\n",
    "y = df_train[TARGET_COL].to_numpy(dtype=np.float64)\n",
    "\n",
    "final_models = {}\n",
    "\n",
    "if MODE == \"single_model\":\n",
    "    # Train one model on full train and predict all test\n",
    "    w_tr = make_sample_weight(df_train, use_recency=USE_RECENCY, tau=TAU, clip_w_quantile=CLIP_W_Q)\n",
    "\n",
    "    X_tr = df_train[FEATURE_COLS_ALL]\n",
    "    X_te = df_test[FEATURE_COLS_ALL]\n",
    "\n",
    "    train_pool = Pool(X_tr, label=y, weight=w_tr, cat_features=CAT_FEATURE_IDXS)\n",
    "    test_pool  = Pool(X_te, cat_features=CAT_FEATURE_IDXS)\n",
    "\n",
    "    model = CatBoostRegressor(**CB_PARAMS)\n",
    "    model.fit(train_pool, verbose=200)\n",
    "\n",
    "    pred_te = model.predict(test_pool).astype(np.float32)\n",
    "    test_pred[:] = pred_te\n",
    "\n",
    "    model_path = MODEL_DIR / \"cb_final_single.cbm\"\n",
    "    model.save_model(str(model_path))\n",
    "    final_models[\"single\"] = str(model_path)\n",
    "\n",
    "    del model, train_pool, test_pool, X_tr, X_te\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Per-horizon: train one model for each horizon value and predict test rows for that horizon\n",
    "    for h in h_vals:\n",
    "        h = int(h)\n",
    "        tr_idx = np.where(h_train == h)[0]\n",
    "        te_idx = np.where(h_test == h)[0]\n",
    "\n",
    "        if len(tr_idx) == 0:\n",
    "            print(f\"[WARN] horizon={h}: no train rows, skipped\")\n",
    "            continue\n",
    "        if len(te_idx) == 0:\n",
    "            print(f\"[WARN] horizon={h}: no test rows, skipped\")\n",
    "            continue\n",
    "\n",
    "        df_tr_h = df_train.iloc[tr_idx]\n",
    "        w_tr = make_sample_weight(df_tr_h, use_recency=USE_RECENCY, tau=TAU, clip_w_quantile=CLIP_W_Q)\n",
    "\n",
    "        X_tr = df_tr_h[FEATURE_COLS_ALL]\n",
    "        y_tr = y[tr_idx]\n",
    "\n",
    "        X_te = df_test.iloc[te_idx][FEATURE_COLS_ALL]\n",
    "\n",
    "        train_pool = Pool(X_tr, label=y_tr, weight=w_tr, cat_features=CAT_FEATURE_IDXS)\n",
    "        test_pool  = Pool(X_te, cat_features=CAT_FEATURE_IDXS)\n",
    "\n",
    "        model = CatBoostRegressor(**CB_PARAMS)\n",
    "        model.fit(train_pool, verbose=200)\n",
    "\n",
    "        pred_te = model.predict(test_pool).astype(np.float32)\n",
    "        test_pred[te_idx] = pred_te\n",
    "\n",
    "        model_path = MODEL_DIR / f\"cb_final_h{h}.cbm\"\n",
    "        model.save_model(str(model_path))\n",
    "        final_models[str(h)] = str(model_path)\n",
    "\n",
    "        print(f\"horizon={h}: train={len(tr_idx):,} test={len(te_idx):,} -> saved {model_path.name}\")\n",
    "\n",
    "        del model, train_pool, test_pool, X_tr, X_te\n",
    "        gc.collect()\n",
    "\n",
    "elapsed = int(time.time() - t0)\n",
    "print(\"Final fit + inference done. Elapsed (sec):\", elapsed)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Build submission.csv\n",
    "# ----------------------------\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": df_test[ID_COL].astype(str).values,\n",
    "    \"prediction\": test_pred.astype(np.float64)  # keep float64 for CSV\n",
    "})\n",
    "\n",
    "# safety: unique ids\n",
    "if sub[\"id\"].nunique() != len(sub):\n",
    "    raise RuntimeError(\"Submission id tidak unik (unexpected).\")\n",
    "\n",
    "SUB_PATH = Path(\"/kaggle/working/submission.csv\")\n",
    "sub.to_csv(SUB_PATH, index=False)\n",
    "print(\"Saved:\", str(SUB_PATH), \"| shape:\", sub.shape)\n",
    "print(sub.head(5))\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Package bundle (models + config + feature list)\n",
    "# ----------------------------\n",
    "bundle = {\n",
    "    \"created_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "    \"mode\": MODE,\n",
    "    \"feature_cols\": FEATURE_COLS_ALL,\n",
    "    \"cat_feature_idxs\": CAT_FEATURE_IDXS,\n",
    "    \"cat_cols_intended\": [c for c in [\"code\",\"sub_code\",\"sub_category\",\"horizon\"] if c in df_train.columns],\n",
    "    \"cb_params\": CB_PARAMS,\n",
    "    \"weighting\": {\"use_recency\": USE_RECENCY, \"tau\": TAU, \"clip_w_q\": CLIP_W_Q},\n",
    "    \"final_models\": final_models,\n",
    "    \"train_ts_index_max\": int(df_train[TIME_COL].max()),\n",
    "    \"test_ts_index_min\": int(df_test[TIME_COL].min()),\n",
    "    \"test_ts_index_max\": int(df_test[TIME_COL].max()),\n",
    "    \"n_train\": int(len(df_train)),\n",
    "    \"n_test\": int(len(df_test)),\n",
    "}\n",
    "BUNDLE_JSON = BUNDLE_DIR / \"bundle.json\"\n",
    "BUNDLE_JSON.write_text(json.dumps(bundle, indent=2))\n",
    "print(\"Saved bundle:\", str(BUNDLE_JSON))\n",
    "\n",
    "# Export globals\n",
    "globals()[\"test_pred\"] = test_pred\n",
    "globals()[\"SUB_PATH\"] = str(SUB_PATH)\n",
    "globals()[\"BUNDLE_DIR\"] = str(BUNDLE_DIR)\n",
    "\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15271735,
     "sourceId": 105581,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31239,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8741.129257,
   "end_time": "2026-01-13T19:00:16.079223",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-13T16:34:34.949966",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
