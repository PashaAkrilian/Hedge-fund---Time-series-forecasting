{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Data & Initial Inspection","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 1 â€” Load Data & Initial Inspection (ONE CELL, Kaggle)\n# Paths (given):\n#   /kaggle/input/ts-forecasting/train.parquet\n#   /kaggle/input/ts-forecasting/test.parquet\n# Output globals:\n#   df_train, df_test, TARGET_COL, ID_COL, WEIGHT_COL, TIME_COL, CAT_COLS, FEAT_COLS, NUM_COLS\n# ============================================================\n\nimport os, gc\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\npd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.width\", 200)\n\nTRAIN_PATH = Path(\"/kaggle/input/ts-forecasting/train.parquet\")\nTEST_PATH  = Path(\"/kaggle/input/ts-forecasting/test.parquet\")\n\nfor p in [TRAIN_PATH, TEST_PATH]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing file: {p}\")\n\nprint(\"Loading parquet...\")\ndf_train = pd.read_parquet(TRAIN_PATH)\ndf_test  = pd.read_parquet(TEST_PATH)\n\nprint(\"\\n==================== BASIC SHAPES ====================\")\nprint(\"train:\", df_train.shape)\nprint(\"test :\", df_test.shape)\n\nprint(\"\\n==================== COLUMNS ====================\")\nprint(\"train cols:\", len(df_train.columns))\nprint(\"test  cols:\", len(df_test.columns))\n\n# ---- Standard column names (from competition description)\nID_COL     = \"id\"\nWEIGHT_COL = \"weight\"\nTIME_COL   = \"ts_index\"\nBASE_CATS  = [\"code\", \"sub_code\", \"sub_category\", \"horizon\"]\n\n# ---- Detect target column (must exist in train, not in test)\ntrain_only_cols = [c for c in df_train.columns if c not in df_test.columns]\n# remove any obviously non-target extras if present\ntrain_only_cols = [c for c in train_only_cols if c not in [ID_COL, WEIGHT_COL, TIME_COL] + BASE_CATS]\n\n# Prefer common target names if present\npreferred_names = [\"target\", \"y\", \"label\", \"value\", \"prediction_target\"]\nTARGET_COL = None\nfor name in preferred_names:\n    if name in df_train.columns and name not in df_test.columns:\n        TARGET_COL = name\n        break\n\nif TARGET_COL is None:\n    # If exactly one train-only col remains -> pick it\n    if len(train_only_cols) == 1:\n        TARGET_COL = train_only_cols[0]\n    else:\n        # Fallback: pick numeric column(s) absent in test\n        cand = []\n        for c in [c for c in df_train.columns if c not in df_test.columns]:\n            if pd.api.types.is_numeric_dtype(df_train[c]):\n                cand.append(c)\n        # remove known non-target just in case\n        cand = [c for c in cand if c not in [WEIGHT_COL, TIME_COL]]\n        if len(cand) == 1:\n            TARGET_COL = cand[0]\n        elif len(cand) > 1:\n            # pick the one with highest variance (usually the true target)\n            vars_ = {c: float(np.nanvar(df_train[c].to_numpy(dtype=np.float64))) for c in cand}\n            TARGET_COL = sorted(vars_.items(), key=lambda kv: kv[1], reverse=True)[0][0]\n            print(\"\\n[WARN] Multiple numeric train-only cols found; picked by variance:\", TARGET_COL)\n        else:\n            # last resort: if any train-only cols exist, pick the first\n            if len([c for c in df_train.columns if c not in df_test.columns]) > 0:\n                TARGET_COL = [c for c in df_train.columns if c not in df_test.columns][0]\n                print(\"\\n[WARN] Could not confidently detect target; picked first train-only col:\", TARGET_COL)\n            else:\n                raise RuntimeError(\"Could not detect target column (no train-only columns).\")\n\nprint(\"\\n==================== KEY COLS ====================\")\nprint(\"ID_COL     :\", ID_COL, \"| exists:\", ID_COL in df_train.columns and ID_COL in df_test.columns)\nprint(\"TIME_COL   :\", TIME_COL, \"| exists:\", TIME_COL in df_train.columns and TIME_COL in df_test.columns)\nprint(\"WEIGHT_COL :\", WEIGHT_COL, \"| exists:\", WEIGHT_COL in df_train.columns and WEIGHT_COL in df_test.columns)\nprint(\"TARGET_COL :\", TARGET_COL, \"| exists in train:\", TARGET_COL in df_train.columns, \"| exists in test:\", TARGET_COL in df_test.columns)\n\n# ---- Determine categorical columns that exist\nCAT_COLS = [c for c in BASE_CATS if c in df_train.columns]\n# Also include any object/category cols (excluding id)\nfor c in df_train.columns:\n    if c == ID_COL or c == TARGET_COL:\n        continue\n    if pd.api.types.is_object_dtype(df_train[c]) or str(df_train[c].dtype).startswith(\"category\"):\n        if c not in CAT_COLS:\n            CAT_COLS.append(c)\n\n# ---- Determine feature columns (exclude id, target, weight; keep everything else)\nEXCLUDE = set([ID_COL, TARGET_COL, WEIGHT_COL])\nFEAT_COLS = [c for c in df_train.columns if c not in EXCLUDE]\n\n# ---- Determine numeric feature columns\nNUM_COLS = [c for c in FEAT_COLS if pd.api.types.is_numeric_dtype(df_train[c]) and c != WEIGHT_COL]\n\nprint(\"\\n==================== QUICK CHECKS ====================\")\n# id uniqueness\nif ID_COL in df_train.columns:\n    print(\"train id unique:\", df_train[ID_COL].nunique(), \"/\", len(df_train))\nif ID_COL in df_test.columns:\n    print(\"test  id unique:\", df_test[ID_COL].nunique(), \"/\", len(df_test))\n\n# ts_index ranges\nif TIME_COL in df_train.columns and TIME_COL in df_test.columns:\n    print(\"train ts_index range:\", int(df_train[TIME_COL].min()), \"->\", int(df_train[TIME_COL].max()))\n    print(\"test  ts_index range:\", int(df_test[TIME_COL].min()),  \"->\", int(df_test[TIME_COL].max()))\n\n# horizon distribution (small peek)\nif \"horizon\" in df_train.columns:\n    print(\"\\ntrain horizon value counts (top):\")\n    print(df_train[\"horizon\"].value_counts(dropna=False).head(10))\nif \"horizon\" in df_test.columns:\n    print(\"\\ntest horizon value counts (top):\")\n    print(df_test[\"horizon\"].value_counts(dropna=False).head(10))\n\n# missingness summary (top 15 columns)\nprint(\"\\n==================== MISSING VALUES (TOP) ====================\")\nmiss_train = df_train.isna().mean().sort_values(ascending=False)\nmiss_test  = df_test.isna().mean().sort_values(ascending=False)\nprint(\"train missing rate top 15:\")\nprint(miss_train.head(15))\nprint(\"\\ntest missing rate top 15:\")\nprint(miss_test.head(15))\n\n# target stats\nif TARGET_COL in df_train.columns and pd.api.types.is_numeric_dtype(df_train[TARGET_COL]):\n    y = df_train[TARGET_COL].to_numpy(dtype=np.float64)\n    print(\"\\n==================== TARGET STATS ====================\")\n    print(\"count:\", np.isfinite(y).sum(), \" / \", len(y))\n    print(\"mean :\", float(np.nanmean(y)))\n    print(\"std  :\", float(np.nanstd(y)))\n    print(\"min  :\", float(np.nanmin(y)))\n    print(\"p1   :\", float(np.nanpercentile(y, 1)))\n    print(\"p50  :\", float(np.nanpercentile(y, 50)))\n    print(\"p99  :\", float(np.nanpercentile(y, 99)))\n    print(\"max  :\", float(np.nanmax(y)))\n\n# weight stats (reminder: do NOT use as feature)\nif WEIGHT_COL in df_train.columns and pd.api.types.is_numeric_dtype(df_train[WEIGHT_COL]):\n    w = df_train[WEIGHT_COL].to_numpy(dtype=np.float64)\n    print(\"\\n==================== WEIGHT STATS (NOT A FEATURE) ====================\")\n    print(\"mean :\", float(np.nanmean(w)))\n    print(\"min  :\", float(np.nanmin(w)))\n    print(\"p50  :\", float(np.nanpercentile(w, 50)))\n    print(\"p99  :\", float(np.nanpercentile(w, 99)))\n    print(\"max  :\", float(np.nanmax(w)))\n\nprint(\"\\n==================== FEATURE SET SUMMARY ====================\")\nprint(\"CAT_COLS :\", CAT_COLS)\nprint(\"NUM_COLS :\", len(NUM_COLS), \"(numeric features excluding weight/target/id)\")\nprint(\"FEAT_COLS:\", len(FEAT_COLS), \"(all usable columns excluding target and weight; id excluded)\")\n\nprint(\"\\n==================== HEAD (train) ====================\")\ndisplay(df_train.head(3))\nprint(\"\\n==================== HEAD (test) ====================\")\ndisplay(df_test.head(3))\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T15:58:00.554112Z","iopub.execute_input":"2026-01-13T15:58:00.554541Z","iopub.status.idle":"2026-01-13T15:58:27.300054Z","shell.execute_reply.started":"2026-01-13T15:58:00.554510Z","shell.execute_reply":"2026-01-13T15:58:27.298634Z"}},"outputs":[{"name":"stdout","text":"Loading parquet...\n\n==================== BASIC SHAPES ====================\ntrain: (5337414, 94)\ntest : (1447107, 92)\n\n==================== COLUMNS ====================\ntrain cols: 94\ntest  cols: 92\n\n==================== KEY COLS ====================\nID_COL     : id | exists: True\nTIME_COL   : ts_index | exists: True\nWEIGHT_COL : weight | exists: False\nTARGET_COL : y_target | exists in train: True | exists in test: False\n\n==================== QUICK CHECKS ====================\ntrain id unique: 5337414 / 5337414\ntest  id unique: 1447107 / 1447107\ntrain ts_index range: 1 -> 3601\ntest  ts_index range: 3602 -> 4376\n\ntrain horizon value counts (top):\nhorizon\n1     1394653\n3     1385816\n10    1337236\n25    1219709\nName: count, dtype: int64\n\ntest horizon value counts (top):\nhorizon\n1     379617\n3     376558\n10    362057\n25    328875\nName: count, dtype: int64\n\n==================== MISSING VALUES (TOP) ====================\ntrain missing rate top 15:\nfeature_at    0.124719\nfeature_by    0.110192\nfeature_ay    0.085420\nfeature_cd    0.074964\nfeature_ce    0.051678\nfeature_cf    0.044289\nfeature_al    0.042233\nfeature_aw    0.038444\nfeature_bz    0.028426\nfeature_bi    0.027622\nfeature_i     0.011059\nfeature_k     0.011059\nfeature_h     0.010954\nfeature_j     0.010954\nfeature_cg    0.007428\ndtype: float64\n\ntest missing rate top 15:\nfeature_x     0.385765\nfeature_w     0.385765\nfeature_z     0.385765\nfeature_y     0.385765\nfeature_at    0.092342\nfeature_by    0.092043\nfeature_ay    0.057988\nfeature_cd    0.057969\nfeature_aw    0.038026\nfeature_bz    0.037755\nfeature_ce    0.035255\nfeature_cf    0.035255\nfeature_bi    0.033526\nfeature_al    0.032591\nfeature_av    0.001190\ndtype: float64\n\n==================== TARGET STATS ====================\ncount: 5337414  /  5337414\nmean : -0.6659048367929424\nstd  : 32.52763850220829\nmin  : -2201.8815779044935\np1   : -82.79721584066921\np50  : -0.0005774817524177132\np99  : 62.923418606328426\nmax  : 2314.4111524982895\n\n==================== WEIGHT STATS (NOT A FEATURE) ====================\nmean : 16427879.64529177\nmin  : 0.0\np50  : 1699.3843705131449\np99  : 303840772.74105984\nmax  : 13912217783333.135\n\n==================== FEATURE SET SUMMARY ====================\nCAT_COLS : ['code', 'sub_code', 'sub_category', 'horizon']\nNUM_COLS : 88 (numeric features excluding weight/target/id)\nFEAT_COLS: 91 (all usable columns excluding target and weight; id excluded)\n\n==================== HEAD (train) ====================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                     id      code  sub_code sub_category  horizon  ts_index  feature_a  feature_b  feature_c  feature_d  feature_e  feature_f  feature_g  feature_h  feature_i  \\\n0  W2MW3G2L__J0G2B0KU__PZ9S1Z4V__25__89  W2MW3G2L  J0G2B0KU     PZ9S1Z4V       25        89         29  16.364093   7.464023   5.966933   1.622184  10.261360   4.914369   0.000467   0.023686   \n1   W2MW3G2L__J0G2B0KU__PZ9S1Z4V__1__89  W2MW3G2L  J0G2B0KU     PZ9S1Z4V        1        89         53   2.858806   5.050617  15.906651  10.879453   3.072151   4.091032   0.000467   0.023686   \n2   W2MW3G2L__J0G2B0KU__PZ9S1Z4V__3__89  W2MW3G2L  J0G2B0KU     PZ9S1Z4V        3        89         51   9.585452   1.076268   9.004147  16.740490  15.166901  11.427983   0.000467   0.023686   \n\n   feature_j  feature_k  feature_l  feature_m  feature_n  feature_o  feature_p  feature_q  feature_r  feature_s  feature_t  feature_u  feature_v  feature_w  feature_x  feature_y   feature_z  \\\n0   0.006409   0.000187   0.744244   2.001013   -0.01687   0.009892   0.013162   0.021502   0.901966   0.402125   0.038566   0.177947   0.091141 -84.968733  -1.765306  10.109641  145.320404   \n1   0.006409   0.000187   0.744244   2.001013   -0.01687   0.009892   0.013162   0.021502   0.901966   0.402125   0.038566   0.177947   0.091141 -84.968733  -1.765306  10.109641  145.320404   \n2   0.006409   0.000187   0.744244   2.001013   -0.01687   0.009892   0.013162   0.021502   0.901966   0.402125   0.038566   0.177947   0.091141 -84.968733  -1.765306  10.109641  145.320404   \n\n   feature_aa  feature_ab  feature_ac  feature_ad  feature_ae  feature_af  feature_ag  feature_ah  feature_ai  feature_aj  feature_ak  feature_al  feature_am  feature_an  feature_ao  feature_ap  \\\n0     0.08958    0.868698    0.080088    0.101631    0.026555    0.092776       0.004    1.298973    7.321646    3.628258    0.453027   -0.080212    0.192181    0.510727   17.136629    0.267856   \n1     0.08958    0.868698    0.080088    0.101631    0.026555    0.092776       0.004    1.298973    7.321646    3.628258    0.453027    0.001480    0.192181    0.510727   17.136629    0.267856   \n2     0.08958    0.868698    0.080088    0.101631    0.026555    0.092776       0.004    1.298973    7.321646    3.628258    0.453027   -0.045494    0.192181    0.510727   17.136629    0.267856   \n\n   feature_aq  feature_ar  feature_as  feature_at  feature_au  feature_av  feature_aw  feature_ax  feature_ay  feature_az  feature_ba  feature_bb  feature_bc    feature_bd    feature_be  feature_bf  \\\n0    7.745722    4.037853     4.85679         NaN    5.188995   79.423474  244.471191   13.848771         NaN     0.01707    0.709292    21.80395    0.120968  26999.430482  34126.269444  791.709562   \n1    7.745722    4.037853     4.85679         NaN    5.188995   79.423474  244.471191   13.848771         NaN     0.01707    0.709292    21.80395    0.120968  26999.430482  34126.269444  791.709562   \n2    7.745722    4.037853     4.85679         NaN    5.188995   79.423474  244.471191   13.848771         NaN     0.01707    0.709292    21.80395    0.120968  26999.430482  34126.269444  791.709562   \n\n   feature_bg   feature_bh  feature_bi  feature_bj  feature_bk  feature_bl  feature_bm  feature_bn  feature_bo  feature_bp  feature_bq  feature_br  feature_bs  feature_bt  feature_bu  feature_bv  \\\n0     0.15467  9499.742248    1.266071  429.318704  2540.88981    0.008927    1.122459   23.815924     0.54985    0.067941    0.076033     0.02759    -0.47269   -0.202944   -3.769914    0.104535   \n1     0.15467  9499.742248    1.266071  429.318704  2540.88981    0.008927    1.122459   23.815924     0.54985    0.067941    0.076033     0.02759    -0.47269   -0.202944   -3.769914    0.104535   \n2     0.15467  9499.742248    1.266071  429.318704  2540.88981    0.008927    1.122459   23.815924     0.54985    0.067941    0.076033     0.02759    -0.47269   -0.202944   -3.769914    0.104535   \n\n   feature_bw  feature_bx  feature_by  feature_bz  feature_ca  feature_cb  feature_cc  feature_cd  feature_ce  feature_cf  feature_cg  feature_ch  y_target      weight  \n0    3.040304    4.499546         NaN   -0.058543   -0.001686   -0.105328   -0.005045         NaN   -0.133697    2.849819    0.112068           1 -0.551324   40.982572  \n1    3.040304    4.499546         NaN   -0.058543   -0.001686   -0.105328   -0.005045         NaN   -0.133697    2.849819    0.112068           1 -0.315583  150.075406  \n2    3.040304    4.499546         NaN   -0.058543   -0.001686   -0.105328   -0.005045         NaN   -0.133697    2.849819    0.112068           1 -0.362894  115.953552  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>code</th>\n      <th>sub_code</th>\n      <th>sub_category</th>\n      <th>horizon</th>\n      <th>ts_index</th>\n      <th>feature_a</th>\n      <th>feature_b</th>\n      <th>feature_c</th>\n      <th>feature_d</th>\n      <th>feature_e</th>\n      <th>feature_f</th>\n      <th>feature_g</th>\n      <th>feature_h</th>\n      <th>feature_i</th>\n      <th>feature_j</th>\n      <th>feature_k</th>\n      <th>feature_l</th>\n      <th>feature_m</th>\n      <th>feature_n</th>\n      <th>feature_o</th>\n      <th>feature_p</th>\n      <th>feature_q</th>\n      <th>feature_r</th>\n      <th>feature_s</th>\n      <th>feature_t</th>\n      <th>feature_u</th>\n      <th>feature_v</th>\n      <th>feature_w</th>\n      <th>feature_x</th>\n      <th>feature_y</th>\n      <th>feature_z</th>\n      <th>feature_aa</th>\n      <th>feature_ab</th>\n      <th>feature_ac</th>\n      <th>feature_ad</th>\n      <th>feature_ae</th>\n      <th>feature_af</th>\n      <th>feature_ag</th>\n      <th>feature_ah</th>\n      <th>feature_ai</th>\n      <th>feature_aj</th>\n      <th>feature_ak</th>\n      <th>feature_al</th>\n      <th>feature_am</th>\n      <th>feature_an</th>\n      <th>feature_ao</th>\n      <th>feature_ap</th>\n      <th>feature_aq</th>\n      <th>feature_ar</th>\n      <th>feature_as</th>\n      <th>feature_at</th>\n      <th>feature_au</th>\n      <th>feature_av</th>\n      <th>feature_aw</th>\n      <th>feature_ax</th>\n      <th>feature_ay</th>\n      <th>feature_az</th>\n      <th>feature_ba</th>\n      <th>feature_bb</th>\n      <th>feature_bc</th>\n      <th>feature_bd</th>\n      <th>feature_be</th>\n      <th>feature_bf</th>\n      <th>feature_bg</th>\n      <th>feature_bh</th>\n      <th>feature_bi</th>\n      <th>feature_bj</th>\n      <th>feature_bk</th>\n      <th>feature_bl</th>\n      <th>feature_bm</th>\n      <th>feature_bn</th>\n      <th>feature_bo</th>\n      <th>feature_bp</th>\n      <th>feature_bq</th>\n      <th>feature_br</th>\n      <th>feature_bs</th>\n      <th>feature_bt</th>\n      <th>feature_bu</th>\n      <th>feature_bv</th>\n      <th>feature_bw</th>\n      <th>feature_bx</th>\n      <th>feature_by</th>\n      <th>feature_bz</th>\n      <th>feature_ca</th>\n      <th>feature_cb</th>\n      <th>feature_cc</th>\n      <th>feature_cd</th>\n      <th>feature_ce</th>\n      <th>feature_cf</th>\n      <th>feature_cg</th>\n      <th>feature_ch</th>\n      <th>y_target</th>\n      <th>weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>W2MW3G2L__J0G2B0KU__PZ9S1Z4V__25__89</td>\n      <td>W2MW3G2L</td>\n      <td>J0G2B0KU</td>\n      <td>PZ9S1Z4V</td>\n      <td>25</td>\n      <td>89</td>\n      <td>29</td>\n      <td>16.364093</td>\n      <td>7.464023</td>\n      <td>5.966933</td>\n      <td>1.622184</td>\n      <td>10.261360</td>\n      <td>4.914369</td>\n      <td>0.000467</td>\n      <td>0.023686</td>\n      <td>0.006409</td>\n      <td>0.000187</td>\n      <td>0.744244</td>\n      <td>2.001013</td>\n      <td>-0.01687</td>\n      <td>0.009892</td>\n      <td>0.013162</td>\n      <td>0.021502</td>\n      <td>0.901966</td>\n      <td>0.402125</td>\n      <td>0.038566</td>\n      <td>0.177947</td>\n      <td>0.091141</td>\n      <td>-84.968733</td>\n      <td>-1.765306</td>\n      <td>10.109641</td>\n      <td>145.320404</td>\n      <td>0.08958</td>\n      <td>0.868698</td>\n      <td>0.080088</td>\n      <td>0.101631</td>\n      <td>0.026555</td>\n      <td>0.092776</td>\n      <td>0.004</td>\n      <td>1.298973</td>\n      <td>7.321646</td>\n      <td>3.628258</td>\n      <td>0.453027</td>\n      <td>-0.080212</td>\n      <td>0.192181</td>\n      <td>0.510727</td>\n      <td>17.136629</td>\n      <td>0.267856</td>\n      <td>7.745722</td>\n      <td>4.037853</td>\n      <td>4.85679</td>\n      <td>NaN</td>\n      <td>5.188995</td>\n      <td>79.423474</td>\n      <td>244.471191</td>\n      <td>13.848771</td>\n      <td>NaN</td>\n      <td>0.01707</td>\n      <td>0.709292</td>\n      <td>21.80395</td>\n      <td>0.120968</td>\n      <td>26999.430482</td>\n      <td>34126.269444</td>\n      <td>791.709562</td>\n      <td>0.15467</td>\n      <td>9499.742248</td>\n      <td>1.266071</td>\n      <td>429.318704</td>\n      <td>2540.88981</td>\n      <td>0.008927</td>\n      <td>1.122459</td>\n      <td>23.815924</td>\n      <td>0.54985</td>\n      <td>0.067941</td>\n      <td>0.076033</td>\n      <td>0.02759</td>\n      <td>-0.47269</td>\n      <td>-0.202944</td>\n      <td>-3.769914</td>\n      <td>0.104535</td>\n      <td>3.040304</td>\n      <td>4.499546</td>\n      <td>NaN</td>\n      <td>-0.058543</td>\n      <td>-0.001686</td>\n      <td>-0.105328</td>\n      <td>-0.005045</td>\n      <td>NaN</td>\n      <td>-0.133697</td>\n      <td>2.849819</td>\n      <td>0.112068</td>\n      <td>1</td>\n      <td>-0.551324</td>\n      <td>40.982572</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>W2MW3G2L__J0G2B0KU__PZ9S1Z4V__1__89</td>\n      <td>W2MW3G2L</td>\n      <td>J0G2B0KU</td>\n      <td>PZ9S1Z4V</td>\n      <td>1</td>\n      <td>89</td>\n      <td>53</td>\n      <td>2.858806</td>\n      <td>5.050617</td>\n      <td>15.906651</td>\n      <td>10.879453</td>\n      <td>3.072151</td>\n      <td>4.091032</td>\n      <td>0.000467</td>\n      <td>0.023686</td>\n      <td>0.006409</td>\n      <td>0.000187</td>\n      <td>0.744244</td>\n      <td>2.001013</td>\n      <td>-0.01687</td>\n      <td>0.009892</td>\n      <td>0.013162</td>\n      <td>0.021502</td>\n      <td>0.901966</td>\n      <td>0.402125</td>\n      <td>0.038566</td>\n      <td>0.177947</td>\n      <td>0.091141</td>\n      <td>-84.968733</td>\n      <td>-1.765306</td>\n      <td>10.109641</td>\n      <td>145.320404</td>\n      <td>0.08958</td>\n      <td>0.868698</td>\n      <td>0.080088</td>\n      <td>0.101631</td>\n      <td>0.026555</td>\n      <td>0.092776</td>\n      <td>0.004</td>\n      <td>1.298973</td>\n      <td>7.321646</td>\n      <td>3.628258</td>\n      <td>0.453027</td>\n      <td>0.001480</td>\n      <td>0.192181</td>\n      <td>0.510727</td>\n      <td>17.136629</td>\n      <td>0.267856</td>\n      <td>7.745722</td>\n      <td>4.037853</td>\n      <td>4.85679</td>\n      <td>NaN</td>\n      <td>5.188995</td>\n      <td>79.423474</td>\n      <td>244.471191</td>\n      <td>13.848771</td>\n      <td>NaN</td>\n      <td>0.01707</td>\n      <td>0.709292</td>\n      <td>21.80395</td>\n      <td>0.120968</td>\n      <td>26999.430482</td>\n      <td>34126.269444</td>\n      <td>791.709562</td>\n      <td>0.15467</td>\n      <td>9499.742248</td>\n      <td>1.266071</td>\n      <td>429.318704</td>\n      <td>2540.88981</td>\n      <td>0.008927</td>\n      <td>1.122459</td>\n      <td>23.815924</td>\n      <td>0.54985</td>\n      <td>0.067941</td>\n      <td>0.076033</td>\n      <td>0.02759</td>\n      <td>-0.47269</td>\n      <td>-0.202944</td>\n      <td>-3.769914</td>\n      <td>0.104535</td>\n      <td>3.040304</td>\n      <td>4.499546</td>\n      <td>NaN</td>\n      <td>-0.058543</td>\n      <td>-0.001686</td>\n      <td>-0.105328</td>\n      <td>-0.005045</td>\n      <td>NaN</td>\n      <td>-0.133697</td>\n      <td>2.849819</td>\n      <td>0.112068</td>\n      <td>1</td>\n      <td>-0.315583</td>\n      <td>150.075406</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>W2MW3G2L__J0G2B0KU__PZ9S1Z4V__3__89</td>\n      <td>W2MW3G2L</td>\n      <td>J0G2B0KU</td>\n      <td>PZ9S1Z4V</td>\n      <td>3</td>\n      <td>89</td>\n      <td>51</td>\n      <td>9.585452</td>\n      <td>1.076268</td>\n      <td>9.004147</td>\n      <td>16.740490</td>\n      <td>15.166901</td>\n      <td>11.427983</td>\n      <td>0.000467</td>\n      <td>0.023686</td>\n      <td>0.006409</td>\n      <td>0.000187</td>\n      <td>0.744244</td>\n      <td>2.001013</td>\n      <td>-0.01687</td>\n      <td>0.009892</td>\n      <td>0.013162</td>\n      <td>0.021502</td>\n      <td>0.901966</td>\n      <td>0.402125</td>\n      <td>0.038566</td>\n      <td>0.177947</td>\n      <td>0.091141</td>\n      <td>-84.968733</td>\n      <td>-1.765306</td>\n      <td>10.109641</td>\n      <td>145.320404</td>\n      <td>0.08958</td>\n      <td>0.868698</td>\n      <td>0.080088</td>\n      <td>0.101631</td>\n      <td>0.026555</td>\n      <td>0.092776</td>\n      <td>0.004</td>\n      <td>1.298973</td>\n      <td>7.321646</td>\n      <td>3.628258</td>\n      <td>0.453027</td>\n      <td>-0.045494</td>\n      <td>0.192181</td>\n      <td>0.510727</td>\n      <td>17.136629</td>\n      <td>0.267856</td>\n      <td>7.745722</td>\n      <td>4.037853</td>\n      <td>4.85679</td>\n      <td>NaN</td>\n      <td>5.188995</td>\n      <td>79.423474</td>\n      <td>244.471191</td>\n      <td>13.848771</td>\n      <td>NaN</td>\n      <td>0.01707</td>\n      <td>0.709292</td>\n      <td>21.80395</td>\n      <td>0.120968</td>\n      <td>26999.430482</td>\n      <td>34126.269444</td>\n      <td>791.709562</td>\n      <td>0.15467</td>\n      <td>9499.742248</td>\n      <td>1.266071</td>\n      <td>429.318704</td>\n      <td>2540.88981</td>\n      <td>0.008927</td>\n      <td>1.122459</td>\n      <td>23.815924</td>\n      <td>0.54985</td>\n      <td>0.067941</td>\n      <td>0.076033</td>\n      <td>0.02759</td>\n      <td>-0.47269</td>\n      <td>-0.202944</td>\n      <td>-3.769914</td>\n      <td>0.104535</td>\n      <td>3.040304</td>\n      <td>4.499546</td>\n      <td>NaN</td>\n      <td>-0.058543</td>\n      <td>-0.001686</td>\n      <td>-0.105328</td>\n      <td>-0.005045</td>\n      <td>NaN</td>\n      <td>-0.133697</td>\n      <td>2.849819</td>\n      <td>0.112068</td>\n      <td>1</td>\n      <td>-0.362894</td>\n      <td>115.953552</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n==================== HEAD (test) ====================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                       id      code  sub_code sub_category  horizon  ts_index  feature_a  feature_b  feature_c  feature_d  feature_e  feature_f  feature_g  feature_h  feature_i  \\\n0   W2MW3G2L__495MGHFJ__PZ9S1Z4V__3__3647  W2MW3G2L  495MGHFJ     PZ9S1Z4V        3      3647         95  10.365266   3.209321   8.109339   9.043471  10.123041  15.722121   0.000243   0.021819   \n1  W2MW3G2L__495MGHFJ__PZ9S1Z4V__10__3647  W2MW3G2L  495MGHFJ     PZ9S1Z4V       10      3647         88   2.571477  15.234848  16.505699   0.230426  10.145378  10.159641   0.000243   0.021819   \n2  W2MW3G2L__495MGHFJ__PZ9S1Z4V__25__3647  W2MW3G2L  495MGHFJ     PZ9S1Z4V       25      3647         71   5.524709   6.931663   8.939537   0.668187  16.578701   3.150690   0.000243   0.021819   \n\n   feature_j  feature_k  feature_l  feature_m  feature_n  feature_o  feature_p  feature_q  feature_r  feature_s  feature_t  feature_u  feature_v  feature_w  feature_x  feature_y   feature_z  \\\n0    0.00142   0.000073   0.572125   1.265875   1.341192   0.005564   0.011987   0.035243   0.833918   1.791284   0.020539   0.218876    0.08066  -50.98124  -4.854592  -8.087713  119.237254   \n1    0.00142   0.000073   0.572125   1.265875   1.341192   0.005564   0.011987   0.035243   0.833918   1.791284   0.020539   0.218876    0.08066  -50.98124  -4.854592  -8.087713  119.237254   \n2    0.00142   0.000073   0.572125   1.265875   1.341192   0.005564   0.011987   0.035243   0.833918   1.791284   0.020539   0.218876    0.08066  -50.98124  -4.854592  -8.087713  119.237254   \n\n   feature_aa  feature_ab  feature_ac  feature_ad  feature_ae  feature_af  feature_ag  feature_ah  feature_ai  feature_aj  feature_ak  feature_al  feature_am  feature_an  feature_ao  feature_ap  \\\n0    0.040442    0.635006    0.105355    0.075415     0.03444     0.09455    0.006728    1.986904    4.411098    3.050746    0.484755    0.020247    0.186578    0.528456   15.395411    0.219483   \n1    0.040442    0.635006    0.105355    0.075415     0.03444     0.09455    0.006728    1.986904    4.411098    3.050746    0.484755    0.052623    0.186578    0.528456   15.395411    0.219483   \n2    0.040442    0.635006    0.105355    0.075415     0.03444     0.09455    0.006728    1.986904    4.411098    3.050746    0.484755    0.041667    0.186578    0.528456   15.395411    0.219483   \n\n   feature_aq  feature_ar  feature_as  feature_at  feature_au   feature_av  feature_aw  feature_ax  feature_ay  feature_az  feature_ba  feature_bb  feature_bc    feature_bd    feature_be  \\\n0     4.83955    2.420422    2.652015         0.0    4.151196  1012.649294  425.853042  197.344987  209.253182    0.016366    0.552138  108.859861    2.369993  66589.814887  34282.221003   \n1     4.83955    2.420422    2.652015         0.0    4.151196  1012.649294  425.853042  197.344987  209.253182    0.016366    0.552138  108.859861    2.369993  66589.814887  34282.221003   \n2     4.83955    2.420422    2.652015         0.0    4.151196  1012.649294  425.853042  197.344987  209.253182    0.016366    0.552138  108.859861    2.369993  66589.814887  34282.221003   \n\n    feature_bf  feature_bg    feature_bh  feature_bi  feature_bj  feature_bk  feature_bl  feature_bm  feature_bn  feature_bo  feature_bp  feature_bq  feature_br  feature_bs  feature_bt  feature_bu  \\\n0  1316.738008     0.04801  11660.961097    0.116372   11.122246  716.158132    0.008559    1.772256   38.452077    0.872948     0.06611    0.078856    0.030888   -0.480743   -0.197747   -3.659776   \n1  1316.738008     0.04801  11660.961097    0.116372   11.122246  716.158132    0.008559    1.772256   38.452077    0.872948     0.06611    0.078856    0.030888   -0.480743   -0.197747   -3.659776   \n2  1316.738008     0.04801  11660.961097    0.116372   11.122246  716.158132    0.008559    1.772256   38.452077    0.872948     0.06611    0.078856    0.030888   -0.480743   -0.197747   -3.659776   \n\n   feature_bv  feature_bw  feature_bx  feature_by  feature_bz  feature_ca  feature_cb  feature_cc  feature_cd  feature_ce  feature_cf  feature_cg  feature_ch  \n0    0.100295    3.131395    4.554259   -0.000832   -0.032241    -0.00083   -0.058961   -0.002774    -0.00148    -0.25646    1.665532    0.071324           2  \n1    0.100295    3.131395    4.554259   -0.000832   -0.032241    -0.00083   -0.058961   -0.002774    -0.00148    -0.25646    1.665532    0.071324           2  \n2    0.100295    3.131395    4.554259   -0.000832   -0.032241    -0.00083   -0.058961   -0.002774    -0.00148    -0.25646    1.665532    0.071324           2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>code</th>\n      <th>sub_code</th>\n      <th>sub_category</th>\n      <th>horizon</th>\n      <th>ts_index</th>\n      <th>feature_a</th>\n      <th>feature_b</th>\n      <th>feature_c</th>\n      <th>feature_d</th>\n      <th>feature_e</th>\n      <th>feature_f</th>\n      <th>feature_g</th>\n      <th>feature_h</th>\n      <th>feature_i</th>\n      <th>feature_j</th>\n      <th>feature_k</th>\n      <th>feature_l</th>\n      <th>feature_m</th>\n      <th>feature_n</th>\n      <th>feature_o</th>\n      <th>feature_p</th>\n      <th>feature_q</th>\n      <th>feature_r</th>\n      <th>feature_s</th>\n      <th>feature_t</th>\n      <th>feature_u</th>\n      <th>feature_v</th>\n      <th>feature_w</th>\n      <th>feature_x</th>\n      <th>feature_y</th>\n      <th>feature_z</th>\n      <th>feature_aa</th>\n      <th>feature_ab</th>\n      <th>feature_ac</th>\n      <th>feature_ad</th>\n      <th>feature_ae</th>\n      <th>feature_af</th>\n      <th>feature_ag</th>\n      <th>feature_ah</th>\n      <th>feature_ai</th>\n      <th>feature_aj</th>\n      <th>feature_ak</th>\n      <th>feature_al</th>\n      <th>feature_am</th>\n      <th>feature_an</th>\n      <th>feature_ao</th>\n      <th>feature_ap</th>\n      <th>feature_aq</th>\n      <th>feature_ar</th>\n      <th>feature_as</th>\n      <th>feature_at</th>\n      <th>feature_au</th>\n      <th>feature_av</th>\n      <th>feature_aw</th>\n      <th>feature_ax</th>\n      <th>feature_ay</th>\n      <th>feature_az</th>\n      <th>feature_ba</th>\n      <th>feature_bb</th>\n      <th>feature_bc</th>\n      <th>feature_bd</th>\n      <th>feature_be</th>\n      <th>feature_bf</th>\n      <th>feature_bg</th>\n      <th>feature_bh</th>\n      <th>feature_bi</th>\n      <th>feature_bj</th>\n      <th>feature_bk</th>\n      <th>feature_bl</th>\n      <th>feature_bm</th>\n      <th>feature_bn</th>\n      <th>feature_bo</th>\n      <th>feature_bp</th>\n      <th>feature_bq</th>\n      <th>feature_br</th>\n      <th>feature_bs</th>\n      <th>feature_bt</th>\n      <th>feature_bu</th>\n      <th>feature_bv</th>\n      <th>feature_bw</th>\n      <th>feature_bx</th>\n      <th>feature_by</th>\n      <th>feature_bz</th>\n      <th>feature_ca</th>\n      <th>feature_cb</th>\n      <th>feature_cc</th>\n      <th>feature_cd</th>\n      <th>feature_ce</th>\n      <th>feature_cf</th>\n      <th>feature_cg</th>\n      <th>feature_ch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>W2MW3G2L__495MGHFJ__PZ9S1Z4V__3__3647</td>\n      <td>W2MW3G2L</td>\n      <td>495MGHFJ</td>\n      <td>PZ9S1Z4V</td>\n      <td>3</td>\n      <td>3647</td>\n      <td>95</td>\n      <td>10.365266</td>\n      <td>3.209321</td>\n      <td>8.109339</td>\n      <td>9.043471</td>\n      <td>10.123041</td>\n      <td>15.722121</td>\n      <td>0.000243</td>\n      <td>0.021819</td>\n      <td>0.00142</td>\n      <td>0.000073</td>\n      <td>0.572125</td>\n      <td>1.265875</td>\n      <td>1.341192</td>\n      <td>0.005564</td>\n      <td>0.011987</td>\n      <td>0.035243</td>\n      <td>0.833918</td>\n      <td>1.791284</td>\n      <td>0.020539</td>\n      <td>0.218876</td>\n      <td>0.08066</td>\n      <td>-50.98124</td>\n      <td>-4.854592</td>\n      <td>-8.087713</td>\n      <td>119.237254</td>\n      <td>0.040442</td>\n      <td>0.635006</td>\n      <td>0.105355</td>\n      <td>0.075415</td>\n      <td>0.03444</td>\n      <td>0.09455</td>\n      <td>0.006728</td>\n      <td>1.986904</td>\n      <td>4.411098</td>\n      <td>3.050746</td>\n      <td>0.484755</td>\n      <td>0.020247</td>\n      <td>0.186578</td>\n      <td>0.528456</td>\n      <td>15.395411</td>\n      <td>0.219483</td>\n      <td>4.83955</td>\n      <td>2.420422</td>\n      <td>2.652015</td>\n      <td>0.0</td>\n      <td>4.151196</td>\n      <td>1012.649294</td>\n      <td>425.853042</td>\n      <td>197.344987</td>\n      <td>209.253182</td>\n      <td>0.016366</td>\n      <td>0.552138</td>\n      <td>108.859861</td>\n      <td>2.369993</td>\n      <td>66589.814887</td>\n      <td>34282.221003</td>\n      <td>1316.738008</td>\n      <td>0.04801</td>\n      <td>11660.961097</td>\n      <td>0.116372</td>\n      <td>11.122246</td>\n      <td>716.158132</td>\n      <td>0.008559</td>\n      <td>1.772256</td>\n      <td>38.452077</td>\n      <td>0.872948</td>\n      <td>0.06611</td>\n      <td>0.078856</td>\n      <td>0.030888</td>\n      <td>-0.480743</td>\n      <td>-0.197747</td>\n      <td>-3.659776</td>\n      <td>0.100295</td>\n      <td>3.131395</td>\n      <td>4.554259</td>\n      <td>-0.000832</td>\n      <td>-0.032241</td>\n      <td>-0.00083</td>\n      <td>-0.058961</td>\n      <td>-0.002774</td>\n      <td>-0.00148</td>\n      <td>-0.25646</td>\n      <td>1.665532</td>\n      <td>0.071324</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>W2MW3G2L__495MGHFJ__PZ9S1Z4V__10__3647</td>\n      <td>W2MW3G2L</td>\n      <td>495MGHFJ</td>\n      <td>PZ9S1Z4V</td>\n      <td>10</td>\n      <td>3647</td>\n      <td>88</td>\n      <td>2.571477</td>\n      <td>15.234848</td>\n      <td>16.505699</td>\n      <td>0.230426</td>\n      <td>10.145378</td>\n      <td>10.159641</td>\n      <td>0.000243</td>\n      <td>0.021819</td>\n      <td>0.00142</td>\n      <td>0.000073</td>\n      <td>0.572125</td>\n      <td>1.265875</td>\n      <td>1.341192</td>\n      <td>0.005564</td>\n      <td>0.011987</td>\n      <td>0.035243</td>\n      <td>0.833918</td>\n      <td>1.791284</td>\n      <td>0.020539</td>\n      <td>0.218876</td>\n      <td>0.08066</td>\n      <td>-50.98124</td>\n      <td>-4.854592</td>\n      <td>-8.087713</td>\n      <td>119.237254</td>\n      <td>0.040442</td>\n      <td>0.635006</td>\n      <td>0.105355</td>\n      <td>0.075415</td>\n      <td>0.03444</td>\n      <td>0.09455</td>\n      <td>0.006728</td>\n      <td>1.986904</td>\n      <td>4.411098</td>\n      <td>3.050746</td>\n      <td>0.484755</td>\n      <td>0.052623</td>\n      <td>0.186578</td>\n      <td>0.528456</td>\n      <td>15.395411</td>\n      <td>0.219483</td>\n      <td>4.83955</td>\n      <td>2.420422</td>\n      <td>2.652015</td>\n      <td>0.0</td>\n      <td>4.151196</td>\n      <td>1012.649294</td>\n      <td>425.853042</td>\n      <td>197.344987</td>\n      <td>209.253182</td>\n      <td>0.016366</td>\n      <td>0.552138</td>\n      <td>108.859861</td>\n      <td>2.369993</td>\n      <td>66589.814887</td>\n      <td>34282.221003</td>\n      <td>1316.738008</td>\n      <td>0.04801</td>\n      <td>11660.961097</td>\n      <td>0.116372</td>\n      <td>11.122246</td>\n      <td>716.158132</td>\n      <td>0.008559</td>\n      <td>1.772256</td>\n      <td>38.452077</td>\n      <td>0.872948</td>\n      <td>0.06611</td>\n      <td>0.078856</td>\n      <td>0.030888</td>\n      <td>-0.480743</td>\n      <td>-0.197747</td>\n      <td>-3.659776</td>\n      <td>0.100295</td>\n      <td>3.131395</td>\n      <td>4.554259</td>\n      <td>-0.000832</td>\n      <td>-0.032241</td>\n      <td>-0.00083</td>\n      <td>-0.058961</td>\n      <td>-0.002774</td>\n      <td>-0.00148</td>\n      <td>-0.25646</td>\n      <td>1.665532</td>\n      <td>0.071324</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>W2MW3G2L__495MGHFJ__PZ9S1Z4V__25__3647</td>\n      <td>W2MW3G2L</td>\n      <td>495MGHFJ</td>\n      <td>PZ9S1Z4V</td>\n      <td>25</td>\n      <td>3647</td>\n      <td>71</td>\n      <td>5.524709</td>\n      <td>6.931663</td>\n      <td>8.939537</td>\n      <td>0.668187</td>\n      <td>16.578701</td>\n      <td>3.150690</td>\n      <td>0.000243</td>\n      <td>0.021819</td>\n      <td>0.00142</td>\n      <td>0.000073</td>\n      <td>0.572125</td>\n      <td>1.265875</td>\n      <td>1.341192</td>\n      <td>0.005564</td>\n      <td>0.011987</td>\n      <td>0.035243</td>\n      <td>0.833918</td>\n      <td>1.791284</td>\n      <td>0.020539</td>\n      <td>0.218876</td>\n      <td>0.08066</td>\n      <td>-50.98124</td>\n      <td>-4.854592</td>\n      <td>-8.087713</td>\n      <td>119.237254</td>\n      <td>0.040442</td>\n      <td>0.635006</td>\n      <td>0.105355</td>\n      <td>0.075415</td>\n      <td>0.03444</td>\n      <td>0.09455</td>\n      <td>0.006728</td>\n      <td>1.986904</td>\n      <td>4.411098</td>\n      <td>3.050746</td>\n      <td>0.484755</td>\n      <td>0.041667</td>\n      <td>0.186578</td>\n      <td>0.528456</td>\n      <td>15.395411</td>\n      <td>0.219483</td>\n      <td>4.83955</td>\n      <td>2.420422</td>\n      <td>2.652015</td>\n      <td>0.0</td>\n      <td>4.151196</td>\n      <td>1012.649294</td>\n      <td>425.853042</td>\n      <td>197.344987</td>\n      <td>209.253182</td>\n      <td>0.016366</td>\n      <td>0.552138</td>\n      <td>108.859861</td>\n      <td>2.369993</td>\n      <td>66589.814887</td>\n      <td>34282.221003</td>\n      <td>1316.738008</td>\n      <td>0.04801</td>\n      <td>11660.961097</td>\n      <td>0.116372</td>\n      <td>11.122246</td>\n      <td>716.158132</td>\n      <td>0.008559</td>\n      <td>1.772256</td>\n      <td>38.452077</td>\n      <td>0.872948</td>\n      <td>0.06611</td>\n      <td>0.078856</td>\n      <td>0.030888</td>\n      <td>-0.480743</td>\n      <td>-0.197747</td>\n      <td>-3.659776</td>\n      <td>0.100295</td>\n      <td>3.131395</td>\n      <td>4.554259</td>\n      <td>-0.000832</td>\n      <td>-0.032241</td>\n      <td>-0.00083</td>\n      <td>-0.058961</td>\n      <td>-0.002774</td>\n      <td>-0.00148</td>\n      <td>-0.25646</td>\n      <td>1.665532</td>\n      <td>0.071324</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Sanity Checks & Leakage Rules Setup","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 2 â€” Sanity Checks & Leakage Rules Setup (ONE CELL, Kaggle)\n# Assumes STAGE 1 already ran and created:\n#   df_train, df_test, TARGET_COL, ID_COL, TIME_COL, CAT_COLS, FEAT_COLS\n# This stage:\n# - Validates schema and uniqueness\n# - Confirms time ordering (train < test)\n# - Sets leakage-safe column lists\n# - Defines \"DO NOT USE\" columns and lightweight guards\n# Outputs/Globals:\n#   DO_NOT_USE_COLS, FEATURE_COLS_NUM, FEATURE_COLS_CAT, FEATURE_COLS_ALL\n#   TRAIN_MAX_TS, TEST_MIN_TS, TEST_MAX_TS\n# ============================================================\n\nimport gc, re\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require STAGE 1 globals\n# ----------------------------\nneed = [\"df_train\",\"df_test\",\"TARGET_COL\",\"ID_COL\",\"TIME_COL\",\"CAT_COLS\",\"FEAT_COLS\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1 dulu.\")\n\nassert isinstance(df_train, pd.DataFrame) and isinstance(df_test, pd.DataFrame)\n\n# ----------------------------\n# 1) Core column existence\n# ----------------------------\nmust_in_train = [ID_COL, TIME_COL, TARGET_COL]\nmust_in_test  = [ID_COL, TIME_COL]\nfor c in must_in_train:\n    if c not in df_train.columns:\n        raise RuntimeError(f\"Train missing required column: {c}\")\nfor c in must_in_test:\n    if c not in df_test.columns:\n        raise RuntimeError(f\"Test missing required column: {c}\")\n\n# Optional: weight may exist only in train\nWEIGHT_COL = \"weight\"\nhas_weight_train = WEIGHT_COL in df_train.columns\nhas_weight_test  = WEIGHT_COL in df_test.columns\n\nprint(\"==================== STAGE 2: SANITY ====================\")\nprint(\"Has weight in train:\", has_weight_train, \"| in test:\", has_weight_test)\nif has_weight_test:\n    print(\"[WARN] weight also exists in test. We'll still exclude it as a feature.\")\nprint(\"Target col:\", TARGET_COL)\n\n# ----------------------------\n# 2) ID uniqueness checks\n# ----------------------------\nntr = len(df_train)\nnts = len(df_test)\n\nnuniq_tr = df_train[ID_COL].nunique(dropna=False)\nnuniq_ts = df_test[ID_COL].nunique(dropna=False)\n\nif nuniq_tr != ntr:\n    dup = df_train[df_train[ID_COL].duplicated(keep=False)][ID_COL].head(10).tolist()\n    raise RuntimeError(f\"Train id not unique: {nuniq_tr}/{ntr}. Example dups: {dup}\")\nif nuniq_ts != nts:\n    dup = df_test[df_test[ID_COL].duplicated(keep=False)][ID_COL].head(10).tolist()\n    raise RuntimeError(f\"Test id not unique: {nuniq_ts}/{nts}. Example dups: {dup}\")\n\nintersect = np.intersect1d(df_train[ID_COL].values, df_test[ID_COL].values)\nif len(intersect) > 0:\n    print(f\"[WARN] Train/Test share {len(intersect)} ids (unexpected). Example:\", intersect[:5])\n\nprint(\"ID uniqueness: OK\")\n\n# ----------------------------\n# 3) Time ordering checks\n# ----------------------------\nif not pd.api.types.is_integer_dtype(df_train[TIME_COL]) and not pd.api.types.is_numeric_dtype(df_train[TIME_COL]):\n    raise RuntimeError(f\"{TIME_COL} in train is not numeric.\")\nif not pd.api.types.is_integer_dtype(df_test[TIME_COL]) and not pd.api.types.is_numeric_dtype(df_test[TIME_COL]):\n    raise RuntimeError(f\"{TIME_COL} in test is not numeric.\")\n\nTRAIN_MAX_TS = int(np.nanmax(df_train[TIME_COL].values))\nTRAIN_MIN_TS = int(np.nanmin(df_train[TIME_COL].values))\nTEST_MIN_TS  = int(np.nanmin(df_test[TIME_COL].values))\nTEST_MAX_TS  = int(np.nanmax(df_test[TIME_COL].values))\n\nprint(\"Train ts_index range:\", TRAIN_MIN_TS, \"->\", TRAIN_MAX_TS)\nprint(\"Test  ts_index range:\", TEST_MIN_TS,  \"->\", TEST_MAX_TS)\n\n# Expect test period after train; allow small overlaps but flag loudly\nif TEST_MIN_TS <= TRAIN_MAX_TS:\n    print(\"[WARN] Test min ts_index <= Train max ts_index. Check competition rules / possible overlap.\")\nelse:\n    print(\"Time ordering (train -> test): OK (test starts after train).\")\n\n# ----------------------------\n# 4) Leakage rules + feature lists\n# ----------------------------\n# DO NOT USE columns as model input features:\n# - id, target, and weight (even if present in train/test)\nDO_NOT_USE_COLS = {ID_COL, TARGET_COL, WEIGHT_COL}\n\n# Basic categorical feature columns: from STAGE 1 CAT_COLS\nFEATURE_COLS_CAT = [c for c in CAT_COLS if c not in DO_NOT_USE_COLS and c in df_train.columns]\n\n# Numeric candidate features: all numeric columns except forbidden\nnumeric_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\nFEATURE_COLS_NUM = [c for c in numeric_cols if c not in DO_NOT_USE_COLS and c != TIME_COL]  # exclude ts_index by default\n\n# Full feature set used by \"tabular model\" baseline:\nFEATURE_COLS_ALL = FEATURE_COLS_CAT + FEATURE_COLS_NUM\n\nprint(\"\\n==================== FEATURE LISTS ====================\")\nprint(\"Categorical features:\", FEATURE_COLS_CAT)\nprint(\"Numeric features (excluding ts_index):\", len(FEATURE_COLS_NUM))\nprint(\"Total features:\", len(FEATURE_COLS_ALL))\n\n# ----------------------------\n# 5) Minimal integrity checks (dtypes, NaNs)\n# ----------------------------\n# Categorical columns should exist in both train and test\nmissing_cats_test = [c for c in FEATURE_COLS_CAT if c not in df_test.columns]\nif missing_cats_test:\n    raise RuntimeError(f\"Categorical cols missing in test: {missing_cats_test}\")\n\n# Numeric columns should exist in both train and test for inference\nmissing_num_test = [c for c in FEATURE_COLS_NUM if c not in df_test.columns]\nif missing_num_test:\n    # It's possible, but unusual; better fail fast\n    raise RuntimeError(f\"Numeric feature cols missing in test: {missing_num_test[:10]} ... ({len(missing_num_test)} total)\")\n\n# Check target has no NaN (important)\ny_nan = df_train[TARGET_COL].isna().mean()\nprint(\"\\nTarget NaN rate:\", float(y_nan))\nif y_nan > 0:\n    print(\"[WARN] Target has missing values. We'll need to drop or impute target rows later (usually drop).\")\n\n# Weight sanity (if exists)\nif has_weight_train:\n    w = df_train[WEIGHT_COL].to_numpy(dtype=np.float64)\n    w_nan = np.isnan(w).mean()\n    w_neg = np.mean(w < 0)\n    w_zero = np.mean(w == 0)\n    print(\"\\n==================== WEIGHT SANITY (TRAIN) ====================\")\n    print(\"NaN rate:\", float(w_nan), \"| negative rate:\", float(w_neg), \"| zero rate:\", float(w_zero))\n    if w_neg > 0:\n        print(\"[WARN] Found negative weights. Usually unexpected; we'll handle carefully later.\")\n\n# ----------------------------\n# 6) Leakage-safe reminders (printed)\n# ----------------------------\nprint(\"\\n==================== LEAKAGE RULES (REMINDER) ====================\")\nprint(\"- Do NOT use 'weight' as a feature (only as sample_weight).\")\nprint(\"- Any preprocessing (imputer/encoder/scaler) must be fit on TRAIN-FOLD only.\")\nprint(\"- Any time-based features (rolling/expanding) must be computed with shift(1) per group.\")\nprint(\"- Do NOT compute statistics using future rows (ts_index > t) for predicting time t.\")\nprint(\"- Avoid fitting encoders on train+test combined.\")\n\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T16:02:39.630213Z","iopub.execute_input":"2026-01-13T16:02:39.630588Z","iopub.status.idle":"2026-01-13T16:03:08.214203Z","shell.execute_reply.started":"2026-01-13T16:02:39.630564Z","shell.execute_reply":"2026-01-13T16:03:08.212199Z"}},"outputs":[{"name":"stdout","text":"==================== STAGE 2: SANITY ====================\nHas weight in train: True | in test: False\nTarget col: y_target\nID uniqueness: OK\nTrain ts_index range: 1 -> 3601\nTest  ts_index range: 3602 -> 4376\nTime ordering (train -> test): OK (test starts after train).\n\n==================== FEATURE LISTS ====================\nCategorical features: ['code', 'sub_code', 'sub_category', 'horizon']\nNumeric features (excluding ts_index): 87\nTotal features: 91\n\nTarget NaN rate: 0.0\n\n==================== WEIGHT SANITY (TRAIN) ====================\nNaN rate: 0.0 | negative rate: 0.0 | zero rate: 0.0009332234673945098\n\n==================== LEAKAGE RULES (REMINDER) ====================\n- Do NOT use 'weight' as a feature (only as sample_weight).\n- Any preprocessing (imputer/encoder/scaler) must be fit on TRAIN-FOLD only.\n- Any time-based features (rolling/expanding) must be computed with shift(1) per group.\n- Do NOT compute statistics using future rows (ts_index > t) for predicting time t.\n- Avoid fitting encoders on train+test combined.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Implement Official Metric","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 3 â€” Implement Official Metric (ONE CELL, Kaggle)\n# Assumes STAGE 1â€“2 already ran and created:\n#   df_train, df_test, TARGET_COL, ID_COL, TIME_COL, (optional) WEIGHT_COL=\"weight\"\n# This stage:\n# - Implements competition metric exactly\n# - Adds helpers to score arrays / dataframes\n# - Provides a few baselines sanity checks (zero, weighted-mean)\n# Outputs/Globals:\n#   weighted_rmse_score, score_df, score_arrays\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require minimal globals\n# ----------------------------\nneed = [\"df_train\", \"TARGET_COL\", \"ID_COL\", \"TIME_COL\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1â€“2 dulu.\")\n\nWEIGHT_COL = \"weight\"\nHAS_W = WEIGHT_COL in df_train.columns\n\n# ----------------------------\n# 1) Official metric (as provided by host)\n# ----------------------------\ndef _clip01(x: float) -> float:\n    return float(np.minimum(np.maximum(x, 0.0), 1.0))\n\ndef weighted_rmse_score(y_target, y_pred, w) -> float:\n    \"\"\"\n    Competition metric:\n      denom = sum(w * y^2)\n      ratio = sum(w * (y - yhat)^2) / denom\n      score = sqrt( 1 - clip01(ratio) )\n    \"\"\"\n    y_target = np.asarray(y_target, dtype=np.float64)\n    y_pred   = np.asarray(y_pred, dtype=np.float64)\n    w        = np.asarray(w, dtype=np.float64)\n\n    # Robust guards\n    if y_target.shape != y_pred.shape or y_target.shape != w.shape:\n        raise ValueError(f\"Shape mismatch: y={y_target.shape}, yhat={y_pred.shape}, w={w.shape}\")\n\n    # If denom is 0, the metric is ill-defined; return 0 safely\n    denom = np.sum(w * (y_target ** 2))\n    if not np.isfinite(denom) or denom <= 0:\n        return 0.0\n\n    ratio = np.sum(w * ((y_target - y_pred) ** 2)) / denom\n    clipped = _clip01(ratio)\n    val = 1.0 - clipped\n    # Numerical safety\n    val = max(val, 0.0)\n    return float(np.sqrt(val))\n\n# ----------------------------\n# 2) Convenience wrappers\n# ----------------------------\ndef score_arrays(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray | None = None) -> float:\n    if w is None:\n        w = np.ones_like(y_true, dtype=np.float64)\n    return weighted_rmse_score(y_true, y_pred, w)\n\ndef score_df(df: pd.DataFrame, y_col: str, pred_col: str, w_col: str = \"weight\") -> float:\n    if w_col not in df.columns:\n        w = np.ones(len(df), dtype=np.float64)\n    else:\n        w = df[w_col].to_numpy(dtype=np.float64)\n    return weighted_rmse_score(df[y_col].to_numpy(dtype=np.float64),\n                               df[pred_col].to_numpy(dtype=np.float64),\n                               w)\n\n# ----------------------------\n# 3) Sanity check on train (simple baselines)\n# ----------------------------\nprint(\"==================== STAGE 3: OFFICIAL METRIC ====================\")\ny = df_train[TARGET_COL].to_numpy(dtype=np.float64)\n\nif HAS_W:\n    w = df_train[WEIGHT_COL].to_numpy(dtype=np.float64)\nelse:\n    w = np.ones_like(y, dtype=np.float64)\n\n# Baseline A: predict 0\npred0 = np.zeros_like(y, dtype=np.float64)\ns0 = weighted_rmse_score(y, pred0, w)\n\n# Baseline B: predict weighted mean (best constant under weighted MSE)\n# Use small epsilon to avoid /0\nw_sum = float(np.sum(w))\nc = float(np.sum(w * y) / (w_sum + 1e-18))\npredc = np.full_like(y, c, dtype=np.float64)\nsc = weighted_rmse_score(y, predc, w)\n\n# Baseline C: predict unweighted median (often robust)\nm = float(np.median(y))\npredm = np.full_like(y, m, dtype=np.float64)\nsm = weighted_rmse_score(y, predm, w)\n\nprint(f\"Using weight column: {HAS_W}\")\nprint(f\"Baseline (predict 0)            score = {s0:.6f}\")\nprint(f\"Baseline (predict w-mean {c:.6f}) score = {sc:.6f}\")\nprint(f\"Baseline (predict median {m:.6f}) score = {sm:.6f}\")\n\n# Some extra diagnostics about denom / ratio scaling\ndenom = float(np.sum(w * (y ** 2)))\nsse0  = float(np.sum(w * ((y - pred0) ** 2)))\nratio0 = sse0 / denom if denom > 0 else np.nan\nprint(\"\\nDiagnostics:\")\nprint(f\"denom sum(w*y^2) = {denom:.6e}\")\nprint(f\"ratio(predict0)  = {ratio0:.6f}  (should be ~1.0 => score~0)\")\n\nprint(\"\\nGlobals exported: weighted_rmse_score, score_arrays, score_df\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T16:05:21.063193Z","iopub.execute_input":"2026-01-13T16:05:21.064021Z","iopub.status.idle":"2026-01-13T16:05:21.485510Z","shell.execute_reply.started":"2026-01-13T16:05:21.063964Z","shell.execute_reply":"2026-01-13T16:05:21.484163Z"}},"outputs":[{"name":"stdout","text":"==================== STAGE 3: OFFICIAL METRIC ====================\nUsing weight column: True\nBaseline (predict 0)            score = 0.000000\nBaseline (predict w-mean -0.000024) score = 0.011117\nBaseline (predict median -0.000577) score = 0.000000\n\nDiagnostics:\ndenom sum(w*y^2) = 4.082630e+08\nratio(predict0)  = 1.000000  (should be ~1.0 => score~0)\n\nGlobals exported: weighted_rmse_score, score_arrays, score_df\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Time-based Validation Split","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 4 â€” Time-based Validation Split (Leakage-Safe CV) (ONE CELL, Kaggle)\n# Assumes STAGE 1â€“3 already ran and created:\n#   df_train, df_test, ID_COL, TIME_COL, TARGET_COL, CAT_COLS\n#\n# This stage:\n# - Builds walk-forward (blocked) time splits on ts_index\n# - Optionally makes splits per-horizon (recommended later), but here we create a global fold id\n# - Exports df_folds (id -> fold) and adds df_train[\"fold\"]\n#\n# Outputs/Globals:\n#   df_folds, df_train (with 'fold'), FOLD_CFG\n#   fold_boundaries (list of dicts)\n#\n# Notes:\n# - We use last portion of time as validation windows.\n# - Training for fold k uses all data with ts_index <= train_end\n#   Validation uses (train_end, valid_end] (strict future)\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require\n# ----------------------------\nneed = [\"df_train\", \"ID_COL\", \"TIME_COL\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1â€“3 dulu.\")\n\n# ----------------------------\n# 1) Config (tune-friendly)\n# ----------------------------\n# Number of folds (walk-forward windows)\nN_FOLDS = 4\n\n# Validation window size in ts_index units.\n# If None, we auto-set based on the last ~20% of time span.\nVALID_WINDOW = None  # e.g., 150, 200, 300; or None for auto\n\n# Gap between train_end and valid_start to reduce leakage via feature smoothing (usually 0 is OK)\nGAP = 0\n\n# Ensure we validate only on the tail period (mimics test)\nTAIL_FRACTION = 0.25  # last 25% of time used to place validation windows\n\n# Minimum validation samples per fold (fail-fast if too small)\nMIN_VALID_ROWS = 200_000\n\n# ----------------------------\n# 2) Prepare timeline\n# ----------------------------\nts = df_train[TIME_COL].to_numpy(dtype=np.int64)\nts_min = int(ts.min())\nts_max = int(ts.max())\nts_unique = np.unique(ts)\nts_unique.sort()\n\nspan = ts_max - ts_min + 1\ntail_start_ts = int(ts_min + (1.0 - TAIL_FRACTION) * span)\ntail_start_ts = max(tail_start_ts, ts_min)\n\n# determine VALID_WINDOW\nif VALID_WINDOW is None:\n    tail_span = ts_max - tail_start_ts + 1\n    # split tail into N_FOLDS windows, with a bit of buffer\n    VALID_WINDOW = max(1, int(np.floor(tail_span / (N_FOLDS + 0.5))))\nVALID_WINDOW = int(VALID_WINDOW)\n\n# Build fold boundaries ending at ts_max\n# Fold k validates on (train_end, valid_end], where valid_end increases toward ts_max\nfold_boundaries = []\nvalid_end = ts_max\nfor k in range(N_FOLDS-1, -1, -1):\n    valid_start = valid_end - VALID_WINDOW + 1\n    # ensure validation window stays in tail\n    if valid_start < tail_start_ts:\n        valid_start = tail_start_ts\n    train_end = valid_start - 1 - GAP\n    fold_boundaries.append({\n        \"fold\": k,\n        \"train_end\": int(train_end),\n        \"valid_start\": int(valid_start),\n        \"valid_end\": int(valid_end),\n        \"gap\": int(GAP),\n        \"valid_window\": int(valid_end - valid_start + 1),\n    })\n    valid_end = train_end  # next fold ends where this train ended\n\n# sort by fold id ascending\nfold_boundaries = sorted(fold_boundaries, key=lambda d: d[\"fold\"])\n\nFOLD_CFG = {\n    \"N_FOLDS\": N_FOLDS,\n    \"VALID_WINDOW\": VALID_WINDOW,\n    \"GAP\": GAP,\n    \"TAIL_FRACTION\": TAIL_FRACTION,\n    \"MIN_VALID_ROWS\": MIN_VALID_ROWS,\n    \"TIME_COL\": TIME_COL,\n    \"ID_COL\": ID_COL,\n}\n\nprint(\"==================== STAGE 4: TIME SPLITS ====================\")\nprint(\"Train ts_index:\", ts_min, \"->\", ts_max, \"| span:\", span)\nprint(\"Tail fraction:\", TAIL_FRACTION, \"| tail_start_ts:\", tail_start_ts)\nprint(\"N_FOLDS:\", N_FOLDS, \"| VALID_WINDOW:\", VALID_WINDOW, \"| GAP:\", GAP)\nprint(\"\\nFold boundaries:\")\nfor b in fold_boundaries:\n    print(f\"  fold {b['fold']}: train <= {b['train_end']} | valid ({b['valid_start']}, {b['valid_end']}] | window={b['valid_window']}\")\n\n# ----------------------------\n# 3) Assign folds\n# ----------------------------\n# Default fold = -1 (train-only, never used for validation)\nfold_arr = np.full(len(df_train), -1, dtype=np.int16)\n\nts_series = df_train[TIME_COL].to_numpy(dtype=np.int64)\n\nfor b in fold_boundaries:\n    k = b[\"fold\"]\n    vs, ve = b[\"valid_start\"], b[\"valid_end\"]\n    mask = (ts_series >= vs) & (ts_series <= ve)\n    fold_arr[mask] = k\n\ndf_train[\"fold\"] = fold_arr\n\n# df_folds mapping (id -> fold)\ndf_folds = df_train[[ID_COL, \"fold\"]].copy()\n\n# ----------------------------\n# 4) Diagnostics\n# ----------------------------\nvc = df_train[\"fold\"].value_counts(dropna=False).sort_index()\nprint(\"\\nFold row counts (fold=-1 means never validated):\")\nprint(vc)\n\n# Ensure each fold has enough validation rows\nok = True\nfor b in fold_boundaries:\n    k = b[\"fold\"]\n    n_valid = int((df_train[\"fold\"] == k).sum())\n    if n_valid < MIN_VALID_ROWS:\n        print(f\"[WARN] fold {k} valid rows too small: {n_valid} < {MIN_VALID_ROWS}\")\n        ok = False\nif ok:\n    print(\"Validation sizes: OK\")\n\n# Quick check: validation is strictly in the future of its training end\nviol = []\nfor b in fold_boundaries:\n    if not (b[\"train_end\"] < b[\"valid_start\"]):\n        viol.append(b[\"fold\"])\nif viol:\n    raise RuntimeError(f\"Invalid split: folds where train_end >= valid_start: {viol}\")\n\nprint(\"\\nGlobals exported: df_train['fold'], df_folds, FOLD_CFG, fold_boundaries\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T16:08:25.083018Z","iopub.execute_input":"2026-01-13T16:08:25.083444Z","iopub.status.idle":"2026-01-13T16:08:25.745382Z","shell.execute_reply.started":"2026-01-13T16:08:25.083399Z","shell.execute_reply":"2026-01-13T16:08:25.744266Z"}},"outputs":[{"name":"stdout","text":"==================== STAGE 4: TIME SPLITS ====================\nTrain ts_index: 1 -> 3601 | span: 3601\nTail fraction: 0.25 | tail_start_ts: 2701\nN_FOLDS: 4 | VALID_WINDOW: 200 | GAP: 0\n\nFold boundaries:\n  fold 0: train <= 2801 | valid (2802, 3001] | window=200\n  fold 1: train <= 3001 | valid (3002, 3201] | window=200\n  fold 2: train <= 3201 | valid (3202, 3401] | window=200\n  fold 3: train <= 3401 | valid (3402, 3601] | window=200\n\nFold row counts (fold=-1 means never validated):\nfold\n-1    3981259\n 0     343037\n 1     342025\n 2     335612\n 3     335481\nName: count, dtype: int64\nValidation sizes: OK\n\nGlobals exported: df_train['fold'], df_folds, FOLD_CFG, fold_boundaries\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Feature Preparation & Weighting Strategy","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 5 â€” Feature Preparation & Weighting Strategy (ONE CELL, Kaggle)\n# REVISI FULL (FIX: exclude 'fold' + any non-feature helper cols)\n#\n# Assumes STAGE 1â€“4 already ran and created:\n#   df_train, df_test, TARGET_COL, ID_COL, TIME_COL\n#   df_train[\"fold\"] from STAGE 4\n#\n# This stage:\n# - Finalize feature column lists (cat + num) with an option to include ts_index\n# - Excludes helper columns like 'fold' from features\n# - Optimize dtypes for categorical columns (category) to save RAM\n# - Defines leakage-safe helper functions:\n#     * make_sample_weight(...) -> uses official weight + optional recency weighting + optional clipping\n#     * fit_median_imputer(...) / apply_median_imputer(...) (for linear models)\n# - Prepares CatBoost cat feature indices for later stages\n#\n# Outputs/Globals:\n#   WEIGHT_COL, USE_TS_AS_FEATURE\n#   FEATURE_COLS_CAT, FEATURE_COLS_NUM, FEATURE_COLS_ALL\n#   CAT_FEATURE_IDXS\n#   make_sample_weight, fit_median_imputer, apply_median_imputer\n#   TRAIN_MAX_TS\n# ============================================================\n\nimport gc\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require\n# ----------------------------\nneed = [\"df_train\", \"df_test\", \"TARGET_COL\", \"ID_COL\", \"TIME_COL\"]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1â€“4 dulu.\")\n\nWEIGHT_COL = \"weight\"\nif WEIGHT_COL not in df_train.columns:\n    raise RuntimeError(\"Kolom 'weight' tidak ada di train (harusnya ada di dataset ini).\")\n\n# Helper cols that must never be treated as features\nHELPER_COLS = {\"fold\"}\n\n# ----------------------------\n# 1) Feature list finalization\n# ----------------------------\nUSE_TS_AS_FEATURE = False  # ubah True untuk eksperimen drift\n\n# Base categorical columns (as per competition)\nBASE_CATS = [\"code\", \"sub_code\", \"sub_category\", \"horizon\"]\nFEATURE_COLS_CAT = [c for c in BASE_CATS if c in df_train.columns and c not in HELPER_COLS]\n\n# Numeric features: all numeric columns excluding forbidden + helpers\nDO_NOT_USE = {ID_COL, TARGET_COL, WEIGHT_COL} | HELPER_COLS\n\nnumeric_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\nFEATURE_COLS_NUM = [c for c in numeric_cols if c not in DO_NOT_USE]\n\n# Remove ts_index if not used\nif not USE_TS_AS_FEATURE and TIME_COL in FEATURE_COLS_NUM:\n    FEATURE_COLS_NUM.remove(TIME_COL)\n\n# Full feature set\nFEATURE_COLS_ALL = FEATURE_COLS_CAT + FEATURE_COLS_NUM\n\n# Ensure all selected features exist in test\nmissing_in_test = [c for c in FEATURE_COLS_ALL if c not in df_test.columns]\nif missing_in_test:\n    raise RuntimeError(f\"Fitur berikut hilang di test: {missing_in_test[:20]} (total {len(missing_in_test)})\")\n\n# ----------------------------\n# 2) Categorical dtype optimization (RAM + CatBoost friendliness)\n# ----------------------------\nfor c in FEATURE_COLS_CAT:\n    if str(df_train[c].dtype) != \"category\":\n        df_train[c] = df_train[c].astype(\"category\")\n    if str(df_test[c].dtype) != \"category\":\n        df_test[c] = df_test[c].astype(\"category\")\n\n# CatBoost expects cat feature indices in the final X column order\nCAT_FEATURE_IDXS = list(range(len(FEATURE_COLS_CAT)))  # cat cols placed first\n\n# ----------------------------\n# 3) Weighting strategy helpers\n# ----------------------------\nTRAIN_MAX_TS = int(df_train[TIME_COL].max())\n\ndef make_sample_weight(df: pd.DataFrame,\n                       use_recency: bool = True,\n                       tau: float = 600.0,\n                       clip_w_quantile: float | None = None,\n                       eps: float = 1e-12) -> np.ndarray:\n    \"\"\"\n    sample_weight = weight * recency_decay(optional)\n    - clip_w_quantile: e.g. 0.999 or 0.9995, only if training unstable due to huge weights.\n    \"\"\"\n    w = df[WEIGHT_COL].to_numpy(dtype=np.float64)\n\n    if clip_w_quantile is not None:\n        q = float(np.nanquantile(w, clip_w_quantile))\n        if np.isfinite(q) and q > 0:\n            w = np.minimum(w, q)\n\n    if use_recency:\n        t = df[TIME_COL].to_numpy(dtype=np.float64)\n        rec = np.exp(-(TRAIN_MAX_TS - t) / float(tau))\n        w = w * rec\n\n    w = np.where(np.isfinite(w), w, 0.0)\n    w = np.maximum(w, 0.0)\n    if float(w.sum()) <= eps:\n        w = np.ones(len(df), dtype=np.float64)\n    return w\n\n# ----------------------------\n# 4) Median imputer (fit per train-fold only; leakage-safe)\n# ----------------------------\ndef fit_median_imputer(df_fit: pd.DataFrame, num_cols: list[str]) -> dict:\n    med = df_fit[num_cols].median(numeric_only=True)\n    return {c: float(med[c]) if c in med.index and np.isfinite(med[c]) else 0.0 for c in num_cols}\n\ndef apply_median_imputer(df_apply: pd.DataFrame, medians: dict, num_cols: list[str]) -> pd.DataFrame:\n    out = df_apply.copy()\n    # fill only cols that contain NaN\n    for c in num_cols:\n        if c in out.columns and out[c].isna().any():\n            out[c] = out[c].fillna(medians.get(c, 0.0))\n    return out\n\n# ----------------------------\n# 5) Prints + quick stats\n# ----------------------------\nprint(\"==================== STAGE 5: FEATURE PREP & WEIGHTING ====================\")\nprint(\"USE_TS_AS_FEATURE:\", USE_TS_AS_FEATURE, f\"(ts_index {'included' if USE_TS_AS_FEATURE else 'excluded'})\")\nprint(\"Helper cols excluded:\", sorted(list(HELPER_COLS)))\nprint(\"Categorical cols:\", FEATURE_COLS_CAT)\nprint(\"Numeric cols     :\", len(FEATURE_COLS_NUM))\nprint(\"Total features   :\", len(FEATURE_COLS_ALL))\nprint(\"Cat idxs for CatBoost:\", CAT_FEATURE_IDXS)\n\n# missingness overview (top 8) for selected numeric features (train vs test)\nif len(FEATURE_COLS_NUM) > 0:\n    miss_tr = df_train[FEATURE_COLS_NUM].isna().mean().sort_values(ascending=False).head(8)\n    miss_te = df_test[FEATURE_COLS_NUM].isna().mean().sort_values(ascending=False).head(8)\n    print(\"\\nTop missing numeric features (train):\")\n    print(miss_tr)\n    print(\"\\nTop missing numeric features (test):\")\n    print(miss_te)\n\n# weight sanity\nw0 = df_train[WEIGHT_COL].to_numpy(dtype=np.float64)\nprint(\"\\nWeight stats (train):\")\nprint(\"  min:\", float(np.nanmin(w0)),\n      \"p50:\", float(np.nanpercentile(w0, 50)),\n      \"p99.9:\", float(np.nanpercentile(w0, 99.9)),\n      \"max:\", float(np.nanmax(w0)))\nprint(\"  zero_rate:\", float(np.mean(w0 == 0.0)),\n      \"neg_rate:\", float(np.mean(w0 < 0.0)))\n\nprint(\"\\nLeakage reminder:\")\nprint(\"- Fit imputer/encoder ONLY on train-fold (ts_index <= train_end).\")\nprint(\"- If building rolling/expanding features: sort by ts_index and use shift(1) per group.\")\nprint(\"- Do NOT use 'weight' as a feature; only as sample_weight.\")\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T16:14:28.441706Z","iopub.execute_input":"2026-01-13T16:14:28.442265Z","iopub.status.idle":"2026-01-13T16:14:35.207952Z","shell.execute_reply.started":"2026-01-13T16:14:28.442234Z","shell.execute_reply":"2026-01-13T16:14:35.206993Z"}},"outputs":[{"name":"stdout","text":"==================== STAGE 5: FEATURE PREP & WEIGHTING ====================\nUSE_TS_AS_FEATURE: False (ts_index excluded)\nHelper cols excluded: ['fold']\nCategorical cols: ['code', 'sub_code', 'sub_category', 'horizon']\nNumeric cols     : 87\nTotal features   : 91\nCat idxs for CatBoost: [0, 1, 2, 3]\n\nTop missing numeric features (train):\nfeature_at    0.124719\nfeature_by    0.110192\nfeature_ay    0.085420\nfeature_cd    0.074964\nfeature_ce    0.051678\nfeature_cf    0.044289\nfeature_al    0.042233\nfeature_aw    0.038444\ndtype: float64\n\nTop missing numeric features (test):\nfeature_w     0.385765\nfeature_y     0.385765\nfeature_z     0.385765\nfeature_x     0.385765\nfeature_at    0.092342\nfeature_by    0.092043\nfeature_ay    0.057988\nfeature_cd    0.057969\ndtype: float64\n\nWeight stats (train):\n  min: 0.0 p50: 1699.3843705131449 p99.9: 1321398915.3320074 max: 13912217783333.135\n  zero_rate: 0.0009332234673945098 neg_rate: 0.0\n\nLeakage reminder:\n- Fit imputer/encoder ONLY on train-fold (ts_index <= train_end).\n- If building rolling/expanding features: sort by ts_index and use shift(1) per group.\n- Do NOT use 'weight' as a feature; only as sample_weight.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Model Training, OOF Evaluation, and Model Selection","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 6 â€” Model Training, OOF Evaluation, and Model Selection (ONE CELL, Kaggle)\n# REVISI FULL (FIX: duplicate 'horizon' feature name + FIX: sampling-weight bug)\n#\n# Baseline: CatBoostRegressor + walk-forward OOF using fold_boundaries\n# Default: PER-HORIZON models (recommended). Horizon feature is EXCLUDED (constant within each model).\n#\n# Requires globals from STAGE 1â€“5:\n#   df_train, TARGET_COL, TIME_COL, ID_COL\n#   FEATURE_COLS_CAT, FEATURE_COLS_NUM (from Stage 5)\n#   fold_boundaries (Stage 4)\n#   weighted_rmse_score (Stage 3)\n#   make_sample_weight (Stage 5)\n#\n# Outputs/Globals:\n#   oof_pred (float32, len(df_train), NaN for fold=-1)\n#   oof_score_all, oof_score_by_fold, oof_score_by_horizon\n#   models_cb (dict): (horizon, fold) -> model_path\n#   CB_CFG_USED (dict)\n# ============================================================\n\nimport os, gc, json, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 0) Require\n# ----------------------------\nneed = [\n    \"df_train\",\"TARGET_COL\",\"TIME_COL\",\"ID_COL\",\n    \"FEATURE_COLS_CAT\",\"FEATURE_COLS_NUM\",\n    \"fold_boundaries\",\"weighted_rmse_score\",\"make_sample_weight\"\n]\nfor k in need:\n    if k not in globals():\n        raise RuntimeError(f\"Missing global '{k}'. Jalankan STAGE 1â€“5 dulu.\")\n\nWEIGHT_COL = \"weight\"\nif WEIGHT_COL not in df_train.columns:\n    raise RuntimeError(\"Kolom 'weight' tidak ada di df_train (harusnya ada).\")\n\ntry:\n    from catboost import CatBoostRegressor, Pool\nexcept Exception as e:\n    raise RuntimeError(f\"CatBoost import gagal: {e}\")\n\n# ----------------------------\n# 1) Mode + Feature list (FIX duplicates)\n# ----------------------------\nMODE = \"per_horizon\"   # \"per_horizon\" (default, recommended) or \"single_model\"\n\n# For per-horizon models, DO NOT include 'horizon' as a feature (it becomes constant and also caused duplicates).\nCAT_BASE = [\"code\", \"sub_code\", \"sub_category\"]\nCAT_SINGLE = [\"code\", \"sub_code\", \"sub_category\", \"horizon\"]\n\nif MODE == \"per_horizon\":\n    FEATURE_COLS_CAT_ = [c for c in CAT_BASE if c in df_train.columns]\nelse:\n    FEATURE_COLS_CAT_ = [c for c in CAT_SINGLE if c in df_train.columns]\n\n# Numeric: from Stage 5 FEATURE_COLS_NUM, but remove any cat cols & helper cols\nHELPER_COLS = {\"fold\"}\ncat_set = set(FEATURE_COLS_CAT_) | HELPER_COLS\nFEATURE_COLS_NUM_ = [c for c in FEATURE_COLS_NUM if c not in cat_set and c not in [ID_COL, TARGET_COL, WEIGHT_COL]]\n\n# Build final feature order: cat first then num\nFEATURE_COLS_ALL_ = FEATURE_COLS_CAT_ + FEATURE_COLS_NUM_\n\n# Deduplicate safety (should already be unique now)\nseen = set()\ndedup = []\nfor c in FEATURE_COLS_ALL_:\n    if c not in seen:\n        dedup.append(c); seen.add(c)\nFEATURE_COLS_ALL_ = dedup\n\n# Validate existence in train\nmissing_train = [c for c in FEATURE_COLS_ALL_ if c not in df_train.columns]\nif missing_train:\n    raise RuntimeError(f\"Feature missing in train: {missing_train[:20]} (total {len(missing_train)})\")\n\n# CatBoost cat indices (since cats are first)\nCAT_FEATURE_IDXS_ = list(range(len(FEATURE_COLS_CAT_)))\n\nprint(\"==================== STAGE 6: CATBOOST OOF ====================\")\nprint(\"MODE:\", MODE)\nprint(\"Cat cols:\", FEATURE_COLS_CAT_)\nprint(\"Num cols:\", len(FEATURE_COLS_NUM_))\nprint(\"Total features:\", len(FEATURE_COLS_ALL_))\nprint(\"Cat idxs:\", CAT_FEATURE_IDXS_)\n\n# ----------------------------\n# 2) Config (CPU-safe baseline)\n# ----------------------------\nSEED = 42\n\n# Weight strategy (training)\nUSE_RECENCY = True\nTAU = 600.0\nCLIP_W_Q = None   # try 0.999 if instabil due to extreme weights\n\n# Runtime control: cap training rows per (fold, horizon)\n# (dataset kamu besar; 16 model kalau per_horizon x 4 fold)\nTRAIN_SAMPLE_CAP = 600_000   # turunkan kalau OOM / lambat (mis. 300k)\nSAMPLE_WEIGHTED = True       # sampling proportional to training weights\n\n# CatBoost params\nCB_PARAMS = dict(\n    loss_function=\"RMSE\",\n    eval_metric=\"RMSE\",\n    iterations=2000,\n    learning_rate=0.05,\n    depth=8,\n    l2_leaf_reg=6.0,\n    random_strength=1.0,\n    bagging_temperature=0.5,\n    subsample=0.8,\n    rsm=0.9,\n    min_data_in_leaf=300,\n    bootstrap_type=\"Bayesian\",\n    task_type=\"CPU\",\n    thread_count=-1,\n    random_seed=SEED,\n    allow_writing_files=False,\n)\nEARLY_STOPPING_ROUNDS = 150\n\nOUT_DIR = Path(\"/kaggle/working/tsf_stage6_catboost_models\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nCB_CFG_USED = dict(\n    MODE=MODE,\n    SEED=SEED,\n    USE_RECENCY=USE_RECENCY,\n    TAU=TAU,\n    CLIP_W_Q=CLIP_W_Q,\n    TRAIN_SAMPLE_CAP=TRAIN_SAMPLE_CAP,\n    SAMPLE_WEIGHTED=SAMPLE_WEIGHTED,\n    CB_PARAMS=CB_PARAMS,\n    EARLY_STOPPING_ROUNDS=EARLY_STOPPING_ROUNDS,\n    FEATURE_COLS_ALL=FEATURE_COLS_ALL_,\n    FEATURE_COLS_CAT=FEATURE_COLS_CAT_,\n)\n\n# ----------------------------\n# 3) Arrays for fast slicing\n# ----------------------------\nn = len(df_train)\noof_pred = np.full(n, np.nan, dtype=np.float32)\n\ny_all  = df_train[TARGET_COL].to_numpy(dtype=np.float64)\nw_off  = df_train[WEIGHT_COL].to_numpy(dtype=np.float64)  # OFFICIAL weight for scoring/eval pool\nts_all = df_train[TIME_COL].to_numpy(dtype=np.int64)\nfold_all = df_train[\"fold\"].to_numpy(dtype=np.int16) if \"fold\" in df_train.columns else np.full(n, -1, dtype=np.int16)\n\n# horizon as int array for fast compare (works even if category)\nh_all = df_train[\"horizon\"].astype(int).to_numpy() if \"horizon\" in df_train.columns else None\nh_vals = np.unique(h_all) if h_all is not None else np.array([0], dtype=int)\nh_vals.sort()\n\nrng = np.random.default_rng(SEED)\n\n# ----------------------------\n# 4) Helpers (FIX sampling weights alignment)\n# ----------------------------\ndef sample_cap_indices(idx: np.ndarray, w_aligned: np.ndarray, cap: int | None) -> np.ndarray:\n    \"\"\"\n    idx: indices into df_train\n    w_aligned: weights aligned 1-1 with idx (same length, not full-length)\n    \"\"\"\n    if cap is None or len(idx) <= cap:\n        return idx\n    if not SAMPLE_WEIGHTED:\n        return rng.choice(idx, size=cap, replace=False)\n    ww = np.asarray(w_aligned, dtype=np.float64)\n    ww = np.where(np.isfinite(ww), ww, 0.0)\n    ww = np.maximum(ww, 0.0)\n    s = ww.sum()\n    if s <= 0:\n        return rng.choice(idx, size=cap, replace=False)\n    p = ww / s\n    return rng.choice(idx, size=cap, replace=False, p=p)\n\ndef fold_train_valid_indices(b: dict) -> tuple[np.ndarray, np.ndarray]:\n    k = int(b[\"fold\"])\n    train_end = int(b[\"train_end\"])\n    vs, ve = int(b[\"valid_start\"]), int(b[\"valid_end\"])\n    tr_idx = np.where(ts_all <= train_end)[0]\n    va_idx = np.where((ts_all >= vs) & (ts_all <= ve))[0]\n    # prefer fold markers if present (should match)\n    if (fold_all >= 0).any():\n        va_idx2 = np.where(fold_all == k)[0]\n        if abs(len(va_idx) - len(va_idx2)) > 1000:\n            print(f\"[WARN] Fold {k}: time valid rows {len(va_idx)} vs fold-mark {len(va_idx2)}\")\n    return tr_idx, va_idx\n\n# ----------------------------\n# 5) Train OOF\n# ----------------------------\nmodels_cb = {}  # (horizon, fold) -> model_path\noof_score_by_fold = {}\noof_score_by_horizon = {}\n\nt0 = time.time()\n\nfor b in fold_boundaries:\n    k = int(b[\"fold\"])\n    tr_idx_all, va_idx_all = fold_train_valid_indices(b)\n\n    print(\"\\n\" + \"-\"*70)\n    print(f\"FOLD {k} | train<= {b['train_end']} | valid {b['valid_start']}..{b['valid_end']} | \"\n          f\"train_rows={len(tr_idx_all):,} valid_rows={len(va_idx_all):,}\")\n\n    if MODE == \"single_model\":\n        # One model per fold, horizon included as categorical feature\n        tr_idx = tr_idx_all\n        va_idx = va_idx_all\n\n        w_tr_full = make_sample_weight(df_train.iloc[tr_idx], use_recency=USE_RECENCY, tau=TAU, clip_w_quantile=CLIP_W_Q)\n        tr_idx_cap = sample_cap_indices(tr_idx, w_tr_full, TRAIN_SAMPLE_CAP)\n        w_tr = make_sample_weight(df_train.iloc[tr_idx_cap], use_recency=USE_RECENCY, tau=TAU, clip_w_quantile=CLIP_W_Q)\n\n        X_tr = df_train.iloc[tr_idx_cap][FEATURE_COLS_ALL_]\n        y_tr = y_all[tr_idx_cap]\n        X_va = df_train.iloc[va_idx][FEATURE_COLS_ALL_]\n        y_va = y_all[va_idx]\n        w_va = w_off[va_idx].astype(np.float64)\n\n        train_pool = Pool(X_tr, label=y_tr, weight=w_tr, cat_features=CAT_FEATURE_IDXS_)\n        valid_pool = Pool(X_va, label=y_va, weight=w_va, cat_features=CAT_FEATURE_IDXS_)\n\n        model = CatBoostRegressor(**CB_PARAMS)\n        model.fit(train_pool, eval_set=valid_pool, use_best_model=True,\n                  verbose=200, early_stopping_rounds=EARLY_STOPPING_ROUNDS)\n\n        pred_va = model.predict(valid_pool).astype(np.float32)\n        oof_pred[va_idx] = pred_va\n\n        best_it = int(model.get_best_iteration() if model.get_best_iteration() is not None else CB_PARAMS[\"iterations\"])\n        model_path = OUT_DIR / f\"cb_single_fold{k}_best{best_it}.cbm\"\n        model.save_model(str(model_path))\n        models_cb[(\"single\", k)] = str(model_path)\n\n        s_fold = weighted_rmse_score(y_va, pred_va.astype(np.float64), w_off[va_idx].astype(np.float64))\n        oof_score_by_fold[k] = float(s_fold)\n        print(f\"FOLD {k} single-model score: {s_fold:.6f}\")\n\n        del model, train_pool, valid_pool, X_tr, X_va\n        gc.collect()\n\n    else:\n        # Per-horizon models (horizon excluded from features)\n        for h in h_vals:\n            h = int(h)\n            tr_idx = tr_idx_all[h_all[tr_idx_all] == h]\n            va_idx = va_idx_all[h_all[va_idx_all] == h]\n\n            if len(tr_idx) == 0 or len(va_idx) == 0:\n                print(f\"  horizon={h}: skipped (train={len(tr_idx)}, valid={len(va_idx)})\")\n                continue\n\n            w_tr_full = make_sample_weight(df_train.iloc[tr_idx], use_recency=USE_RECENCY, tau=TAU, clip_w_quantile=CLIP_W_Q)\n            tr_idx_cap = sample_cap_indices(tr_idx, w_tr_full, TRAIN_SAMPLE_CAP)\n            w_tr = make_sample_weight(df_train.iloc[tr_idx_cap], use_recency=USE_RECENCY, tau=TAU, clip_w_quantile=CLIP_W_Q)\n\n            X_tr = df_train.iloc[tr_idx_cap][FEATURE_COLS_ALL_]\n            y_tr = y_all[tr_idx_cap]\n\n            X_va = df_train.iloc[va_idx][FEATURE_COLS_ALL_]\n            y_va = y_all[va_idx]\n            w_va = w_off[va_idx].astype(np.float64)\n\n            train_pool = Pool(X_tr, label=y_tr, weight=w_tr, cat_features=CAT_FEATURE_IDXS_)\n            valid_pool = Pool(X_va, label=y_va, weight=w_va, cat_features=CAT_FEATURE_IDXS_)\n\n            model = CatBoostRegressor(**CB_PARAMS)\n            model.fit(train_pool, eval_set=valid_pool, use_best_model=True,\n                      verbose=200, early_stopping_rounds=EARLY_STOPPING_ROUNDS)\n\n            pred_va = model.predict(valid_pool).astype(np.float32)\n            oof_pred[va_idx] = pred_va\n\n            best_it = int(model.get_best_iteration() if model.get_best_iteration() is not None else CB_PARAMS[\"iterations\"])\n            model_path = OUT_DIR / f\"cb_h{h}_fold{k}_best{best_it}.cbm\"\n            model.save_model(str(model_path))\n            models_cb[(h, k)] = str(model_path)\n\n            s = weighted_rmse_score(y_va, pred_va.astype(np.float64), w_off[va_idx].astype(np.float64))\n            print(f\"  horizon={h}: train={len(tr_idx_cap):,} (cap from {len(tr_idx):,}) | valid={len(va_idx):,} | \"\n                  f\"best_it={best_it} | score={s:.6f}\")\n\n            del model, train_pool, valid_pool, X_tr, X_va\n            gc.collect()\n\n        # Fold aggregate score (official) across all horizons\n        idx_fold = np.where(fold_all == k)[0]\n        pred_fold = oof_pred[idx_fold].astype(np.float64)\n        y_fold = y_all[idx_fold]\n        w_fold = w_off[idx_fold]\n        m = np.isfinite(pred_fold)\n        s_fold = weighted_rmse_score(y_fold[m], pred_fold[m], w_fold[m])\n        oof_score_by_fold[k] = float(s_fold)\n        print(f\"FOLD {k} aggregate OOF score: {s_fold:.6f} | n={m.sum():,}\")\n\n# ----------------------------\n# 6) Aggregate OOF (all folds)\n# ----------------------------\nvalid_idx_all = np.where(fold_all >= 0)[0]\npred_all = oof_pred[valid_idx_all].astype(np.float64)\ny_valid = y_all[valid_idx_all]\nw_valid = w_off[valid_idx_all]\nm = np.isfinite(pred_all)\noof_score_all = float(weighted_rmse_score(y_valid[m], pred_all[m], w_valid[m]))\n\n# Per-horizon OOF\nif MODE == \"per_horizon\":\n    for h in h_vals:\n        h = int(h)\n        idx_h = valid_idx_all[h_all[valid_idx_all] == h]\n        if len(idx_h) == 0:\n            continue\n        ph = oof_pred[idx_h].astype(np.float64)\n        yh = y_all[idx_h]\n        wh = w_off[idx_h]\n        mm = np.isfinite(ph)\n        oof_score_by_horizon[h] = float(weighted_rmse_score(yh[mm], ph[mm], wh[mm]))\n\nelapsed = int(time.time() - t0)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"OOF SCORE (ALL FOLDS):\", f\"{oof_score_all:.6f}\")\nprint(\"OOF SCORE BY FOLD:\", oof_score_by_fold)\nif MODE == \"per_horizon\":\n    print(\"OOF SCORE BY HORIZON:\", oof_score_by_horizon)\nprint(\"Models saved under:\", str(OUT_DIR))\nprint(\"Elapsed (sec):\", elapsed)\n\n# Save report\nreport_path = OUT_DIR / \"stage6_oof_report.json\"\nwith open(report_path, \"w\") as f:\n    json.dump(\n        {\n            \"oof_score_all\": oof_score_all,\n            \"oof_score_by_fold\": oof_score_by_fold,\n            \"oof_score_by_horizon\": oof_score_by_horizon,\n            \"fold_boundaries\": fold_boundaries,\n            \"cfg\": CB_CFG_USED,\n            \"models\": {str(k): v for k, v in models_cb.items()},\n        },\n        f,\n        indent=2\n    )\nprint(\"Saved report:\", str(report_path))\n\n# Export globals\nglobals()[\"oof_pred\"] = oof_pred\nglobals()[\"oof_score_all\"] = oof_score_all\nglobals()[\"oof_score_by_fold\"] = oof_score_by_fold\nglobals()[\"oof_score_by_horizon\"] = oof_score_by_horizon\nglobals()[\"models_cb\"] = models_cb\nglobals()[\"CB_CFG_USED\"] = CB_CFG_USED\n\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T16:24:41.670118Z","iopub.execute_input":"2026-01-13T16:24:41.670599Z","iopub.status.idle":"2026-01-13T16:24:46.296171Z","shell.execute_reply.started":"2026-01-13T16:24:41.670565Z","shell.execute_reply":"2026-01-13T16:24:46.293696Z"}},"outputs":[{"name":"stdout","text":"==================== STAGE 6: CATBOOST OOF ====================\nMODE: per_horizon\nCat cols: ['code', 'sub_code', 'sub_category']\nNum cols: 87\nTotal features: 90\nCat idxs: [0, 1, 2]\n\n----------------------------------------------------------------------\nFOLD 0 | train<= 2801 | valid 2802..3001 | train_rows=3,981,259 valid_rows=343,037\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1786241623.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mw_va\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_off\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mva_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mtrain_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCAT_FEATURE_IDXS_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mvalid_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCAT_FEATURE_IDXS_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, graph, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr, data_can_be_none)\u001b[0m\n\u001b[1;32m    853\u001b[0m                         )\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                     self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n\u001b[0m\u001b[1;32m    856\u001b[0m                                group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n\u001b[1;32m    857\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata_can_be_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeature_tags\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m             \u001b[0mfeature_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_transform_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m         self._init_pool(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n\u001b[0m\u001b[1;32m   1492\u001b[0m                         group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_features_order_layout_pool\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._set_features_order_data_pd_data_frame\u001b[0;34m()\u001b[0m\n","\u001b[0;31mCatBoostError\u001b[0m: features data: pandas.DataFrame column 'horizon' has dtype 'category' but is not in  cat_features list"],"ename":"CatBoostError","evalue":"features data: pandas.DataFrame column 'horizon' has dtype 'category' but is not in  cat_features list","output_type":"error"}],"execution_count":9},{"cell_type":"markdown","source":"# Final Fit, Test Inference, and Submission Packaging","metadata":{}}]}